{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pybullet build time: Oct 14 2022 01:09:34\n"
     ]
    }
   ],
   "source": [
    "'''Complete Information DQN (CIdqn)\n",
    "'''\n",
    "\n",
    "import glob\n",
    "import imp\n",
    "import math\n",
    "import gc\n",
    "import os\n",
    "from sre_constants import SUCCESS\n",
    "import time\n",
    "import datetime\n",
    "import pybullet as p\n",
    "import cv2\n",
    "import numpy as np\n",
    "from graphviz import Digraph\n",
    "import argparse\n",
    "import random\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from time import sleep\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "from Config.constants import (\n",
    "    GRIPPER_PUSH_RADIUS,\n",
    "    PIXEL_SIZE,\n",
    "    PUSH_DISTANCE,\n",
    "    WORKSPACE_LIMITS,\n",
    "    TARGET_LOWER,\n",
    "    TARGET_UPPER,\n",
    "    orange_lower,\n",
    "    orange_upper,\n",
    "    BG_THRESHOLD,\n",
    "    MIN_GRASP_THRESHOLDS\n",
    ")\n",
    "\n",
    "from Environments.environment_sim import Environment\n",
    "import Environments.utils as env_utils\n",
    "from V1_destination_prediction.Test_cases.tc1 import TestCase1\n",
    "\n",
    "from create_env import get_push_start, get_max_extent_of_target_from_bottom\n",
    "\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "from V2_next_best_action.models.dqn_v2 import pushDQN2\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pushDQN2(\n",
       "  (layer1): Linear(in_features=6, out_features=128, bias=True)\n",
       "  (layer2): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (layer3): Linear(in_features=128, out_features=16, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_observations = 6 # 3 for initial state, 3 for goal state\n",
    "n_actions = 16 # 16 push + 1 grasp\n",
    "\n",
    "policy_net = pushDQN2(n_observations, n_actions, use_cuda=True).to(device)\n",
    "\n",
    "checkpoint = torch.load('/home/vishal/Volume_E/Active/Undergrad_research/ICRA22/codebases/TEMP/Mid-Level-Planner/V2_next_best_action/models/model_checkpoints/model9/5350.pt')\n",
    "policy_net.load_state_dict(checkpoint)\n",
    "policy_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    '''Select the next best action \n",
    "    state: tensor(shape=(6))\n",
    "    '''\n",
    "    with torch.no_grad():\n",
    "        return policy_net(state).max(1)[1].view(1, 1)\n",
    "\n",
    "def get_reward(prev_state, current_state):\n",
    "    '''\n",
    "    prev_state: (x1, y1, theta1, x2, y2, theta2)\n",
    "    current_state: (x3, y3, theta3, _, _, _)\n",
    "    '''\n",
    "    reward = np.exp(-1 * np.sqrt(np.linalg.norm(current_state[0:3] - prev_state[3:6])))\n",
    "    return reward\n",
    "\n",
    "def get_reward2(prev_state, current_state):\n",
    "    '''\n",
    "    prev_state: (x1, y1, theta1, x2, y2, theta2)\n",
    "    current_state: (x3, y3, theta3, _, _, _)\n",
    "    '''\n",
    "\n",
    "    print(\"Rewarding ----------------------------\")\n",
    "    pos_diff = np.linalg.norm(prev_state[0:2] - prev_state[3:5]) - np.linalg.norm(current_state[0:2] - prev_state[3:5])\n",
    "    orn_diff = np.linalg.norm(prev_state[2:3] - prev_state[5:6]) - np.linalg.norm(current_state[2:3] - prev_state[5:6])\n",
    "    reward = pos_diff + 0.1*orn_diff  #np.linalg.norm(prev_state[0:3] - prev_state[3:6]) - np.linalg.norm(current_state[0:3] - prev_state[3:6]) # prev distance - current distance\n",
    "    print(f\"Position Diff: {pos_diff}\\tOrn Diff: {0.1*orn_diff}\\nReward Aggregate: {reward}\")\n",
    "    return reward\n",
    "\n",
    "def get_reward4(prev_state, current_state):\n",
    "    '''\n",
    "    prev_state: (x1, y1, theta1, x2, y2, theta2)\n",
    "    current_state: (x3, y3, theta3, _, _, _)\n",
    "    '''\n",
    "\n",
    "    # print(\"Rewarding ----------------------------\")\n",
    "    pos_diff = np.linalg.norm(prev_state[0:2] - prev_state[3:5]) - np.linalg.norm(current_state[0:2] - prev_state[3:5])\n",
    "    orn_diff = np.linalg.norm(prev_state[2:3] - prev_state[5:6]) - np.linalg.norm(current_state[2:3] - prev_state[5:6])\n",
    "    reward = pos_diff + 0.1*orn_diff  #np.linalg.norm(prev_state[0:3] - prev_state[3:6]) - np.linalg.norm(current_state[0:3] - prev_state[3:6]) # prev distance - current distance\n",
    "    # print(f\"Position Diff: {pos_diff}\\tOrn Diff: {0.1*orn_diff}\\nReward Aggregate: {reward}\")\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Environments.utils import sample_goal, get_pose_distance\n",
    "# def sample_goal(target_pos, target_orn):\n",
    "#     target_euler = p.getEulerFromQuaternion(target_orn)\n",
    "#     target_yaw = target_euler[2]\n",
    "\n",
    "#     goal_pos = target_pos + np.random.uniform(low=-0.15, high=0.15, size=(3,))\n",
    "#     goal_pos[2] = target_pos[2]\n",
    "#     goal_yaw = target_yaw + np.random.uniform(low=0, high=2*np.pi)\n",
    "#     goal_euler = np.array(target_euler)\n",
    "#     goal_euler[2] = goal_yaw\n",
    "\n",
    "#     goal_orn = p.getQuaternionFromEuler(goal_euler)\n",
    "\n",
    "#     return goal_pos, goal_orn\n",
    "\n",
    "# def get_pose_distance(target_pos, target_orn, marker_pos, marker_orn):\n",
    "#     target_euler = p.getEulerFromQuaternion(target_orn)\n",
    "#     target_yaw = target_euler[2]\n",
    "\n",
    "#     marker_euler = p.getEulerFromQuaternion(marker_orn)\n",
    "#     marker_yaw = marker_euler[2]\n",
    "\n",
    "#     t_pose = np.array([target_pos[0], target_pos[1], target_yaw], dtype=float)\n",
    "#     m_pose = np.array([marker_pos[0], marker_pos[1], marker_yaw], dtype=float)\n",
    "\n",
    "#     return np.linalg.norm(t_pose - m_pose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startThreads creating 1 threads.\n",
      "starting thread 0\n",
      "started thread 0 \n",
      "argc=2\n",
      "argv[0] = --unused\n",
      "argv[1] = --start_demo_name=Physics Server\n",
      "ExampleBrowserThreadFunc started\n",
      "X11 functions dynamically loaded using dlopen/dlsym OK!\n",
      "X11 functions dynamically loaded using dlopen/dlsym OK!\n",
      "Creating context\n",
      "Created GL 3.3 context\n",
      "Direct GLX rendering context obtained\n",
      "Making context current\n",
      "GL_VENDOR=NVIDIA Corporation\n",
      "GL_RENDERER=NVIDIA GeForce GTX 1050 Ti/PCIe/SSE2\n",
      "GL_VERSION=3.3.0 NVIDIA 510.85.02\n",
      "GL_SHADING_LANGUAGE_VERSION=3.30 NVIDIA via Cg compiler\n",
      "pthread_getconcurrency()=0\n",
      "Version = 3.3.0 NVIDIA 510.85.02\n",
      "Vendor = NVIDIA Corporation\n",
      "Renderer = NVIDIA GeForce GTX 1050 Ti/PCIe/SSE2\n",
      "b3Printf: Selected demo: Physics Server\n",
      "startThreads creating 1 threads.\n",
      "starting thread 0\n",
      "started thread 0 \n",
      "MotionThreadFunc thread started\n",
      "Timestep: 0\tTarget: [ 0.34799836  0.12984701 -0.34883955]\tGoal: [0.38709285 0.13171186 1.56367126]------------------\n",
      "Pose distance: 1.912911250954479\n",
      "Action: 1.5707963267948966\n",
      "Rewarding ----------------------------\n",
      "Position Diff: -0.010540822658004545\tOrn Diff: 0.022066322900060276\n",
      "Reward Aggregate: 0.01152550024205573\n",
      "Reward: 0.01152550024205573\n",
      "Action: 0\n",
      "Rewarding ----------------------------\n",
      "Position Diff: 0.0038357150440490526\tOrn Diff: 0.048744538865184976\n",
      "Reward Aggregate: 0.05258025390923403\n",
      "Reward: 0.05258025390923403\n",
      "Action: 5.105088062083414\n",
      "Rewarding ----------------------------\n",
      "Position Diff: 0.010827365577542283\tOrn Diff: -0.022006877274417525\n",
      "Reward Aggregate: -0.011179511696875241\n",
      "Reward: -0.011179511696875241\n",
      "Action: 5.105088062083414\n",
      "Rewarding ----------------------------\n",
      "Position Diff: 0.02427524517089364\tOrn Diff: 0.030126101759516844\n",
      "Reward Aggregate: 0.05440134693041049\n",
      "Reward: 0.05440134693041049\n",
      "Action: 5.105088062083414\n",
      "Rewarding ----------------------------\n",
      "Position Diff: -0.012208782835415585\tOrn Diff: -0.028126694030693723\n",
      "Reward Aggregate: -0.040335476866109304\n",
      "Reward: -0.040335476866109304\n",
      "Action: 5.105088062083414\n",
      "Rewarding ----------------------------\n",
      "Position Diff: -0.008321719100771459\tOrn Diff: 0.026222512332042626\n",
      "Reward Aggregate: 0.017900793231271167\n",
      "Reward: 0.017900793231271167\n",
      "Action: 5.105088062083414\n",
      "Rewarding ----------------------------\n",
      "Position Diff: -0.028064538794786417\tOrn Diff: -0.008184687493098131\n",
      "Reward Aggregate: -0.036249226287884545\n",
      "Reward: -0.036249226287884545\n",
      "Action: 5.105088062083414\n",
      "Rewarding ----------------------------\n",
      "Position Diff: -0.025275433828452668\tOrn Diff: 0.00931223297826309\n",
      "Reward Aggregate: -0.015963200850189575\n",
      "Reward: -0.015963200850189575\n",
      "Action: 5.105088062083414\n",
      "Rewarding ----------------------------\n",
      "Position Diff: -0.02639251231795682\tOrn Diff: -0.009791257530139675\n",
      "Reward Aggregate: -0.03618376984809649\n",
      "Reward: -0.03618376984809649\n",
      "Action: 1.9634954084936207\n",
      "Rewarding ----------------------------\n",
      "Position Diff: 0.024450988089759984\tOrn Diff: 0.016154081504057373\n",
      "Reward Aggregate: 0.04060506959381736\n",
      "Reward: 0.04060506959381736\n",
      "Action: 5.105088062083414\n",
      "Rewarding ----------------------------\n",
      "Position Diff: -0.02836535271349934\tOrn Diff: -0.015556948810096217\n",
      "Reward Aggregate: -0.04392230152359555\n",
      "Reward: -0.04392230152359555\n",
      "Action: 1.9634954084936207\n",
      "Rewarding ----------------------------\n",
      "Position Diff: 0.025579845713613172\tOrn Diff: 0.014879293410948891\n",
      "Reward Aggregate: 0.04045913912456206\n",
      "Reward: 0.04045913912456206\n",
      "Action: 5.105088062083414\n",
      "Rewarding ----------------------------\n",
      "Position Diff: -0.02236661674041085\tOrn Diff: -0.023656339062098256\n",
      "Reward Aggregate: -0.046022955802509105\n",
      "Reward: -0.046022955802509105\n",
      "Action: 1.5707963267948966\n",
      "Rewarding ----------------------------\n",
      "Position Diff: 0.014183491594109532\tOrn Diff: -0.02674836770166338\n",
      "Reward Aggregate: -0.012564876107553847\n",
      "Reward: -0.012564876107553847\n",
      "Action: 1.5707963267948966\n",
      "Rewarding ----------------------------\n",
      "Position Diff: 0.02685997702576036\tOrn Diff: 0.005547625635214337\n",
      "Reward Aggregate: 0.0324076026609747\n",
      "Reward: 0.0324076026609747\n",
      "Action: 1.5707963267948966\n",
      "Rewarding ----------------------------\n",
      "Position Diff: 0.020820805722076154\tOrn Diff: -0.011856757453127266\n",
      "Reward Aggregate: 0.008964048268948888\n",
      "Reward: 0.008964048268948888\n",
      "Action: 1.5707963267948966\n",
      "Rewarding ----------------------------\n",
      "Position Diff: 0.02295880751793017\tOrn Diff: 0.013122149696530827\n",
      "Reward Aggregate: 0.036080957214460994\n",
      "Reward: 0.036080957214460994\n",
      "Action: 1.5707963267948966\n",
      "Rewarding ----------------------------\n",
      "Position Diff: 0.004344939026176824\tOrn Diff: -0.015024389372915238\n",
      "Reward Aggregate: -0.010679450346738414\n",
      "Reward: -0.010679450346738414\n",
      "Action: 1.5707963267948966\n",
      "Rewarding ----------------------------\n",
      "Position Diff: -0.006060182357024348\tOrn Diff: 0.016033350348443245\n",
      "Reward Aggregate: 0.009973167991418896\n",
      "Reward: 0.009973167991418896\n",
      "Action: 5.105088062083414\n",
      "Rewarding ----------------------------\n",
      "Position Diff: -0.007240349246489371\tOrn Diff: 0.018365299575767425\n",
      "Reward Aggregate: 0.011124950329278054\n",
      "Reward: 0.011124950329278054\n",
      "Action: 3.141592653589793\n",
      "Rewarding ----------------------------\n",
      "Position Diff: 0.008923932857299352\tOrn Diff: -0.031334386405961555\n",
      "Reward Aggregate: -0.022410453548662203\n",
      "Reward: -0.022410453548662203\n",
      "Action: 1.5707963267948966\n",
      "Rewarding ----------------------------\n",
      "Position Diff: -0.008455748813097179\tOrn Diff: 0.024949590425212678\n",
      "Reward Aggregate: 0.0164938416121155\n",
      "Reward: 0.0164938416121155\n",
      "Action: 3.141592653589793\n",
      "Rewarding ----------------------------\n",
      "Position Diff: -0.010215841247183235\tOrn Diff: -0.026875237499226823\n",
      "Reward Aggregate: -0.03709107874641006\n",
      "Reward: -0.03709107874641006\n",
      "Action: 1.5707963267948966\n",
      "Rewarding ----------------------------\n",
      "Position Diff: -0.09171383858949603\tOrn Diff: 0.010448441035046253\n",
      "Reward Aggregate: -0.08126539755444978\n",
      "Reward: -0.08126539755444978\n",
      "Action: 4.71238898038469\n",
      "Force is 854.4005188263206, exceed the max force 300\n",
      "Rewarding ----------------------------\n",
      "Position Diff: -2.3341236059626347e-05\tOrn Diff: -5.786351089513975e-05\n",
      "Reward Aggregate: -8.12047469547661e-05\n",
      "Reward: -8.12047469547661e-05\n",
      "Action: 4.71238898038469\n",
      "Force is 839.9152461486804, exceed the max force 300\n",
      "Rewarding ----------------------------\n",
      "Position Diff: -2.3312186461926787e-05\tOrn Diff: -5.779453742544849e-05\n",
      "Reward Aggregate: -8.110672388737529e-05\n",
      "Reward: -8.110672388737529e-05\n",
      "Action: 4.71238898038469\n",
      "Force is 863.1042159717709, exceed the max force 300\n",
      "Rewarding ----------------------------\n",
      "Position Diff: -2.3320988998321024e-05\tOrn Diff: -5.781007260070581e-05\n",
      "Reward Aggregate: -8.113106159902684e-05\n",
      "Reward: -8.113106159902684e-05\n",
      "Action: 4.71238898038469\n",
      "Force is 836.9134436358522, exceed the max force 300\n",
      "Rewarding ----------------------------\n",
      "Position Diff: -2.333767279200094e-05\tOrn Diff: -5.782627253905215e-05\n",
      "Reward Aggregate: -8.116394533105308e-05\n",
      "Reward: -8.116394533105308e-05\n",
      "Action: 4.71238898038469\n",
      "Force is 834.798744906141, exceed the max force 300\n",
      "Rewarding ----------------------------\n",
      "Position Diff: -2.3332266516712385e-05\tOrn Diff: -5.783081777506194e-05\n",
      "Reward Aggregate: -8.116308429177433e-05\n",
      "Reward: -8.116308429177433e-05\n",
      "Action: 4.71238898038469\n",
      "Force is 899.7612760932883, exceed the max force 300\n",
      "Rewarding ----------------------------\n",
      "Position Diff: -2.3334403044028562e-05\tOrn Diff: -5.784715048555711e-05\n",
      "Reward Aggregate: -8.118155352958567e-05\n",
      "Reward: -8.118155352958567e-05\n",
      "Action: 4.71238898038469\n",
      "Force is 830.5451643543234, exceed the max force 300\n",
      "Rewarding ----------------------------\n",
      "Position Diff: -2.3343912488232865e-05\tOrn Diff: -5.7851027999755725e-05\n",
      "Reward Aggregate: -8.119494048798859e-05\n",
      "Reward: -8.119494048798859e-05\n",
      "Action: 4.71238898038469\n",
      "Force is 830.5427467594537, exceed the max force 300\n",
      "Rewarding ----------------------------\n",
      "Position Diff: -2.3375525689428578e-05\tOrn Diff: -5.786589024396882e-05\n",
      "Reward Aggregate: -8.12414159333974e-05\n",
      "Reward: -8.12414159333974e-05\n",
      "Action: 4.71238898038469\n",
      "Force is 899.759461545275, exceed the max force 300\n",
      "Rewarding ----------------------------\n",
      "Position Diff: -2.3369467255646148e-05\tOrn Diff: -5.787941325203772e-05\n",
      "Reward Aggregate: -8.124888050768387e-05\n",
      "Reward: -8.124888050768387e-05\n",
      "Action: 4.71238898038469\n",
      "Force is 840.5678076559706, exceed the max force 300\n",
      "Rewarding ----------------------------\n",
      "Position Diff: -2.337026950338661e-05\tOrn Diff: -5.789119289445477e-05\n",
      "Reward Aggregate: -8.126146239784138e-05\n",
      "Reward: -8.126146239784138e-05\n",
      "Action: 4.71238898038469\n",
      "Force is 910.2350724905946, exceed the max force 300\n",
      "Rewarding ----------------------------\n",
      "Position Diff: -2.337776084934684e-05\tOrn Diff: -5.7900823962908636e-05\n",
      "Reward Aggregate: -8.127858481225548e-05\n",
      "Reward: -8.127858481225548e-05\n",
      "Action: 4.71238898038469\n",
      "Force is 898.9577998094911, exceed the max force 300\n",
      "Rewarding ----------------------------\n",
      "Position Diff: -2.340667046527023e-05\tOrn Diff: -5.790790018380676e-05\n",
      "Reward Aggregate: -8.131457064907698e-05\n",
      "Reward: -8.131457064907698e-05\n",
      "Action: 4.71238898038469\n",
      "Force is 898.9488998637753, exceed the max force 300\n",
      "Rewarding ----------------------------\n",
      "Position Diff: -2.3397221310422767e-05\tOrn Diff: -5.791201423219761e-05\n",
      "Reward Aggregate: -8.130923554262038e-05\n",
      "Reward: -8.130923554262038e-05\n",
      "Action: 4.71238898038469\n",
      "Force is 898.942344405784, exceed the max force 300\n",
      "Rewarding ----------------------------\n",
      "Position Diff: -2.3393944080518647e-05\tOrn Diff: -5.792467867453688e-05\n",
      "Reward Aggregate: -8.131862275505553e-05\n",
      "Reward: -8.131862275505553e-05\n",
      "Action: 4.71238898038469\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "push_directions = [0, np.pi/8, np.pi/4, 3*np.pi/8, \n",
    "                    np.pi/2, 5*np.pi/8, 3*np.pi/4, 7*np.pi/8, \n",
    "                    np.pi, 9*np.pi/8, 5*np.pi/4, 11*np.pi/8,  \n",
    "                    3*np.pi/2, 13*np.pi/8, 7*np.pi/4, 15*np.pi/8] # 16 standard directions\n",
    "\n",
    "max_timesteps = 250\n",
    "max_per_episode_timesteps = 50\n",
    "timestep = 0\n",
    "\n",
    "\n",
    "env = Environment(gui=True)\n",
    "threshold_d = 0.01 # Threshold pose distance\n",
    "\n",
    "while timestep < max_timesteps:\n",
    "    env.reset()\n",
    "\n",
    "    testcase1 = TestCase1(env)\n",
    "    body_ids, success = testcase1.sample_test_case(bottom_obj='random')\n",
    "\n",
    "    target_pos, target_orn = p.getBasePositionAndOrientation(body_ids[1])\n",
    "    marker_pos, marker_orn = None, None\n",
    "    goal_suc = False\n",
    "    while not goal_suc:\n",
    "        marker_pos, marker_orn = sample_goal(target_pos, target_orn)\n",
    "        marker_obj, goal_suc = testcase1.add_marker_obj(marker_pos, marker_orn, half_extents=testcase1.current_target_size/2)\n",
    "    body_ids.append(marker_obj)\n",
    "\n",
    "    target_euler = p.getEulerFromQuaternion(target_orn)\n",
    "    marker_euler = p.getEulerFromQuaternion(marker_orn)\n",
    "    cur_target_st = np.array([target_pos[0], target_pos[1], target_euler[2]], dtype=np.float64)\n",
    "    cur_target_goal = np.array([marker_pos[0], marker_pos[1], marker_euler[2]], dtype=np.float64)\n",
    "    cur_state = np.hstack((cur_target_st, cur_target_goal))\n",
    "    state = {\n",
    "        'cur_state': torch.tensor(cur_state, dtype=torch.float, device=device).unsqueeze(0),\n",
    "    }\n",
    "\n",
    "    print(f\"Timestep: {timestep}\\tTarget: {cur_target_st}\\tGoal: {cur_target_goal}------------------\")\n",
    "    pose_d = get_pose_distance(target_pos, target_orn, marker_pos, marker_orn)\n",
    "    print(f\"Pose distance: {pose_d}\")\n",
    "\n",
    "    per_episode_timesteps = 0\n",
    "    while (pose_d > threshold_d) and per_episode_timesteps < max_per_episode_timesteps:\n",
    "        action = select_action(state['cur_state'])\n",
    "        print(f\"Action: {push_directions[action.item()]}\")\n",
    "        color_image, depth_image, _ = env_utils.get_true_heightmap(env)\n",
    "\n",
    "        new_target_pos, new_target_orn = None, None\n",
    "        new_state=None\n",
    "        if action.item() in range(0, 16):\n",
    "            temp = cv2.cvtColor(color_image, cv2.COLOR_RGB2HSV)\n",
    "            target_mask = cv2.inRange(temp, TARGET_LOWER, TARGET_UPPER)\n",
    "            push_dir = push_directions[action.item()] # Sample push directions\n",
    "            push_start, push_end = get_push_start(push_dir, target_mask, body_ids[1])\n",
    "            env.push(push_start, push_end)\n",
    "\n",
    "            # collect reward\n",
    "            new_target_pos, new_target_orn = p.getBasePositionAndOrientation(body_ids[1])\n",
    "            target_euler = p.getEulerFromQuaternion(new_target_orn)\n",
    "\n",
    "            new_target_st = np.array([new_target_pos[0], new_target_pos[1], target_euler[2]], dtype=np.float)\n",
    "            # new_target_goal = new_target_st + np.random.uniform(low=[-5, -5, -2*np.pi], high=[5, 5, 2*np.pi], size=(3,))\n",
    "            new_state = np.hstack((new_target_st, cur_target_goal))\n",
    "            reward = get_reward2(current_state=new_state, prev_state=state['cur_state'].squeeze().cpu().numpy())\n",
    "            print(f\"Reward: {reward}\")\n",
    "        else:\n",
    "            print(\"Invalid action type!!!!!!!\")\n",
    "            exit()\n",
    "        \n",
    "        pose_d = get_pose_distance(new_target_pos, new_target_orn, marker_pos, marker_orn)\n",
    "\n",
    "        next_state = {\n",
    "            'cur_state': torch.tensor(new_state, dtype=torch.float, device=device).unsqueeze(0)\n",
    "        }\n",
    "        state=next_state\n",
    "        timestep+=1\n",
    "        per_episode_timesteps += 1\n",
    "\n",
    "        if timestep >= max_timesteps:\n",
    "            print(\"The end!\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vft2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8654fed7540277ddf6b9b5ab1dc4e73f02ee108c349421d9d519d216622133b6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
