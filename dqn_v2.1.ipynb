{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN \n",
    "- To learn to push using a manipulator \n",
    "- Author: Vishal Reddy Mandadi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import imp\n",
    "import math\n",
    "import gc\n",
    "import os\n",
    "from sre_constants import SUCCESS\n",
    "import time\n",
    "import datetime\n",
    "import pybullet as p\n",
    "import cv2\n",
    "import numpy as np\n",
    "from graphviz import Digraph\n",
    "import argparse\n",
    "import random\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from time import sleep\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from Config.constants import (\n",
    "    GRIPPER_PUSH_RADIUS,\n",
    "    PIXEL_SIZE,\n",
    "    PUSH_DISTANCE,\n",
    "    WORKSPACE_LIMITS,\n",
    "    TARGET_LOWER,\n",
    "    TARGET_UPPER,\n",
    "    orange_lower,\n",
    "    orange_upper,\n",
    "    BG_THRESHOLD,\n",
    "    MIN_GRASP_THRESHOLDS\n",
    ")\n",
    "\n",
    "from Environments.environment_sim import Environment\n",
    "import Environments.utils as env_utils\n",
    "from V1_destination_prediction.Test_cases.tc1 import TestCase1\n",
    "\n",
    "from create_env import get_push_start, get_max_extent_of_target_from_bottom\n",
    "\n",
    "\n",
    "from collections import namedtuple, deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Memory Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state_rgb', 'state_height', 'action', 'next_state_rgb', 'next_state_height', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity) -> None:\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \n",
    "        '''Save a transition'''\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Model \n",
    "- Inspired from vpg's model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import OrderedDict\n",
    "\n",
    "\n",
    "class pushDQN(nn.Module):\n",
    "    def __init__(self, use_cuda) -> None:\n",
    "        super(pushDQN, self).__init__()\n",
    "        self.use_cuda = use_cuda\n",
    "\n",
    "        # Initialize push network trunk with DenseNet pre-trained on ImageNet\n",
    "        self.push_color_trunk = torchvision.models.densenet.densenet121(pretrained=True) # (weights='DenseNet121_Weights.DEFAULT') # (pretrained=True) # 7*7*1024 # These pre-trained models will also be trained\n",
    "        self.push_height_trunk = torchvision.models.densenet.densenet121(pretrained=True) #(weights='DenseNet121_Weights.DEFAULT') # (pretrained=True) # 7*7*1024 (given input==3*224*224)These pre-trained models will also be trained\n",
    "\n",
    "\n",
    "        # Additional Layers for the model\n",
    "        self.pushnet = nn.Sequential(OrderedDict([\n",
    "            ('push-norm0', nn.BatchNorm2d(2048)), # here 2048 is the number of channels for the image\n",
    "            ('push-relu0', nn.ReLU(inplace=True)),\n",
    "            # ('push-pool0', nn.MaxPool2d(kernel_size=3, stride=2, padding=0)),\n",
    "            ('push-conv1', nn.Conv2d(2048, 64, kernel_size=5, stride=1, padding='same')),\n",
    "            ('push-norm1', nn.BatchNorm2d(64)), # here 2048 is the number of channels for the image\n",
    "            ('push-relu1', nn.ReLU(inplace=True)),\n",
    "            # ('push-pool1', nn.MaxPool2d(kernel_size=3, stride=2, padding=0)),\n",
    "            ('push-conv2', nn.Conv2d(64, 32, kernel_size=5, stride=1, padding='same')),\n",
    "            ('push-norm2', nn.BatchNorm2d(32)), # here 2048 is the number of channels for the image\n",
    "            ('push-relu2', nn.ReLU(inplace=True)),\n",
    "            # ('push-pool2', nn.MaxPool2d(kernel_size=3, stride=2, padding=0)) \n",
    "        ])) # Output 7*7*32\n",
    "        \n",
    "        self.linear_layers = nn.Sequential(OrderedDict([\n",
    "            ('push-linear0', nn.Linear(7*7*32, 1024)),\n",
    "            ('push-relu3', nn.ReLU(inplace=True)),\n",
    "            ('push-linear1', nn.Linear(1024, 17)),\n",
    "            # ('push-tanh', nn.Tanh()) # ('push-relu4', nn.ReLU(inplace=True))\n",
    "        ]))\n",
    "\n",
    "        # Weights initialization for the newly added layers\n",
    "\n",
    "        for m in self.named_modules():\n",
    "            if 'push-' in m[0]:\n",
    "                if isinstance(m[1], nn.Conv2d):\n",
    "                    nn.init.kaiming_normal_(m[1].weight.data)\n",
    "                elif isinstance(m[1], nn.BatchNorm2d):\n",
    "                    m[1].weight.data.fill_(1)\n",
    "                    m[1].bias.data.zero_()\n",
    "                elif isinstance(m[1], nn.Linear):\n",
    "                    nn.init.xavier_uniform_(m[1].weight.data)\n",
    "                    m[1].bias.data.fill_(0.01)\n",
    "        \n",
    "        # Initialize output variable (for backprop)\n",
    "        self.interim_feat = []\n",
    "        self.output_prob = []\n",
    "        \n",
    "    def forward(self, input_color_data, input_height_data, is_volatile=False):\n",
    "        # if is_volatile:\n",
    "        #     with torch.no_grad():\n",
    "        #         # Fill up the code later\n",
    "        #         pass\n",
    "        # else:\n",
    "        self.interim_feat = []\n",
    "        self.output_prob = []\n",
    "\n",
    "        interim_color_feat = self.push_color_trunk.features(input_color_data)\n",
    "        interim_height_feat = self.push_height_trunk.features(input_height_data)\n",
    "        interim_push_feat = torch.cat((interim_color_feat, interim_height_feat), dim=1)\n",
    "\n",
    "        appended_activated_feat = self.pushnet(interim_push_feat)\n",
    "        appended_activated_feat = appended_activated_feat.view(appended_activated_feat.size(0), -1)\n",
    "\n",
    "        output_probs = self.linear_layers(appended_activated_feat)\n",
    "    \n",
    "        return output_probs\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "# del model\n",
    "# del policy_net\n",
    "# del target_net\n",
    "# torch.cuda.memory_summary(device=device, abbreviated=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vishal/anaconda3/envs/vft2/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/vishal/anaconda3/envs/vft2/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.4062, -0.6015, -1.1330,  0.4928,  0.1588, -0.0075, -0.3264, -1.1091,\n",
      "         -0.6993, -0.2761,  1.0824,  0.4413, -0.1230, -0.1436,  0.7756, -0.1645,\n",
      "         -0.1676],\n",
      "        [ 1.5104, -0.7261,  0.0078, -1.3493, -0.4104, -0.5523,  0.2192,  0.0642,\n",
      "          0.2218, -0.2024,  0.1039, -0.0814, -0.6820, -0.2294,  0.7852,  1.4242,\n",
      "         -1.0454],\n",
      "        [ 1.1283,  0.3069,  0.0361,  0.0068,  0.1971, -0.6518,  0.0199, -1.1494,\n",
      "         -1.1064,  0.3678,  0.2370,  0.3216, -0.3027,  0.2368,  0.4308,  0.1677,\n",
      "         -0.3347]], device='cuda:0', grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model = pushDQN(use_cuda=True)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "# model(np.ones(shape=(1, 224, 224, 3)), np.ones(shape=(1, 224, 224, 3)))\n",
    "print(model(torch.rand((3, 3, 224, 224)).to(device=device), torch.rand((3, 3, 224, 224)).to(device=device)))\n",
    "# model.eval()\n",
    "# del model\n",
    "# summary(model, ((3, 224, 224), (3, 224, 224)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.6812, -0.1787, -1.2036, -0.6929,  0.0150, -0.5955,  0.6491, -0.3198,\n",
      "         -0.4854, -0.2856,  0.7225, -0.2740, -0.4843, -0.5139,  0.3464,  0.5617,\n",
      "         -0.1480]], device='cuda:0')\n",
      "tensor([[1.6812]], device='cuda:0')\n",
      "tensor([[0]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    qs = model(torch.rand((1, 3, 224, 224)).to(device=device), torch.rand((1, 3, 224, 224)).to(device=device))\n",
    "    print(qs)\n",
    "\n",
    "    print(qs.max(1)[0].view(1, 1))\n",
    "    print(qs.max(1)[1].view(1, 1))\n",
    "    # del model\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training setup "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters setting\n",
    "- We start with EPS_START probability of choosing a random action as our initial policy. \n",
    "- This decays at EPS_DECAY rate \n",
    "- This will reach EPS_END towards the end of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BATCH_SIZE = 4\n",
    "GAMMA = 0.999 # Discount factor\n",
    "EPS_START = 0.9 # Random action choosing probability starts with this value and decays until EPS_END\n",
    "EPS_END = 0.05 # Random action choosing probability starts at EPS_START and decays until EPS_END\n",
    "EPS_DECAY = 200 # Decay rate of random action choosing probability, with the passage of episodes and time\n",
    "TARGET_UPDATE = 10\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vishal/anaconda3/envs/vft2/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/vishal/anaconda3/envs/vft2/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Number of actions\n",
    "n_actions = 17 # 16 push + 1 grasp\n",
    "\n",
    "policy_net = pushDQN(use_cuda=True).to(device)\n",
    "target_net = pushDQN(use_cuda=True).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.RMSprop(policy_net.parameters())\n",
    "memory = ReplayMemory(100) # 10000\n",
    "\n",
    "steps_done = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state_rgb, state_height):\n",
    "    '''Select the next best action \n",
    "    state: {\n",
    "        'rgb': tensor(shape(3*224*224)),\n",
    "        'height_map': tensor(shape(3*224*224))\n",
    "    }\n",
    "    '''\n",
    "    global steps_done\n",
    "    sample = random.uniform(0.0, 1.0) # random.randint(a=0, b=16) \n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1.0*steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "\n",
    "    if sample>eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state_rgb, state_height).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"REWARD SPECIFICATION\n",
    "\n",
    "1. If action=grasp:\n",
    "        if prev_max_extents>THRESHOLD: # Max extents before grasping\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1 \n",
    "2. If action=push:\n",
    "        if cur_max_entents>THRESHOLD: # Max extents after pushing\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "Bellman equation:\n",
    "    Q(s, a) = r + gamma*max(Q(s', a'))\n",
    "\"\"\"\n",
    "\n",
    "def get_reward(action, max_extents, MIN_GRASP_EXTENT_THRESH):\n",
    "    '''\n",
    "    '''\n",
    "    if action=='push':\n",
    "        # if (max_extents[0] > MAX_EXTENT_THRESH) or (max_extents[1] > MAX_EXTENT_THRESH[1]): # check if object fell on the ground\n",
    "        #     return -1.0\n",
    "        if (max_extents[0] > MIN_GRASP_EXTENT_THRESH[0]) or (max_extents[1] > MIN_GRASP_EXTENT_THRESH[1]):\n",
    "            return 1.0\n",
    "        else:\n",
    "            return -0.05 # for fast achievement of goal\n",
    "    elif action=='grasp':\n",
    "        # if (max_extents[0] > MAX_EXTENT_THRESH) or (max_extents[1] > MAX_EXTENT_THRESH[1]): # check if object fell on the ground\n",
    "        #     return -1.0\n",
    "        if (max_extents[0] > MIN_GRASP_EXTENT_THRESH[0]) or (max_extents[1] > MIN_GRASP_EXTENT_THRESH[1]):\n",
    "            return 1.0\n",
    "        else:\n",
    "            return -1.0 # end the episode if it tries to grasp here\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "def get_belman_update_value(state_rgb, state_height, reward):\n",
    "    '''\n",
    "    '''\n",
    "    next_q = policy_net(state_rgb, state_height).max(1)[0].view(1, 1)\n",
    "    update = reward + next_q.max(1)[0].view[1, 1]\n",
    "    return update\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state_rgb)), device=device, dtype=torch.bool)\n",
    "\n",
    "    non_final_next_states_rgb = torch.cat([s for s in batch.next_state_rgb\n",
    "                                                if s is not None])\n",
    "\n",
    "    non_final_next_states_height = torch.cat([s for s in batch.next_state_height\n",
    "                                                if s is not None])\n",
    "\n",
    "    state_rgb_batch = torch.cat(batch.state_rgb)\n",
    "    state_height_batch = torch.cat(batch.state_height)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_rgb_batch, state_height_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states_rgb, non_final_next_states_height).max(1)[0].detach()\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        if param.grad == None:\n",
    "            continue\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "def plot_durations():\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "X connection to :1 broken (explicit kill or server shutdown).\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from itertools import count\n",
    "\n",
    "from Config.constants import MIN_GRASP_THRESHOLDS\n",
    "\n",
    "is_viz = False\n",
    "\n",
    "# env = Environment()\n",
    "env = Environment(gui=True)\n",
    "num_of_envs = 10\n",
    "max_num_of_actions = 10\n",
    "is_viz = False\n",
    "max_extent_threshold = 1 # Max extent threshold of the target object in pixel units\n",
    "push_directions = [0, np.pi/8, np.pi/4, 3*np.pi/8, \n",
    "                    np.pi/2, 5*np.pi/8, 3*np.pi/4, 7*np.pi/8, \n",
    "                    np.pi, 9*np.pi/8, 5*np.pi/4, 11*np.pi/8,  \n",
    "                    3*np.pi/2, 13*np.pi/8, 7*np.pi/4, 15*np.pi/8] # 16 standard directions\n",
    "\n",
    "\n",
    "num_episodes = 50\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    env.reset()\n",
    "    testcase1 = TestCase1(env)\n",
    "    body_ids, success = testcase1.sample_test_case(bottom_obj='random') #'random') # testcase1.create_standard()\n",
    "    color_image, depth_image, _ = env_utils.get_true_heightmap(env)\n",
    "    depth_image = np.stack((depth_image, )*3, axis=-1)\n",
    "    # print(\"Returned body ids: {}, success: {}\".format(body_ids, success))\n",
    "    # last_screen = get_screen()\n",
    "    # current_screen = get_screen()\n",
    "    # state = current_screen - last_screen\n",
    "    state = {\n",
    "        'rgb': torch.tensor([np.transpose(color_image, (2, 0, 1))], dtype=torch.float, device=device), # transpose used in order to convert (224, 224, 3) to (3, 224, 224)\n",
    "        'height_map': torch.tensor([np.transpose(depth_image, (2, 0, 1))], dtype=torch.float, device=device) # torch.tensor([np.transpose(depth_image, (2, 0, 1))], device=device) # transpose used in order to convert (224, 224, 3) to (3, 224, 224)\n",
    "    }\n",
    "    done = False\n",
    "\n",
    "    for t in count():\n",
    "        # Select and perform an action\n",
    "        action = select_action(state['rgb'], state['height_map'])\n",
    "        if action.item() in range(0, 16): # push action\n",
    "            temp = cv2.cvtColor(color_image, cv2.COLOR_RGB2HSV)\n",
    "            target_mask = cv2.inRange(temp, TARGET_LOWER, TARGET_UPPER)\n",
    "            push_dir = push_directions[action.item()] # Sample push directions\n",
    "            push_start, push_end = get_push_start(push_dir, target_mask, body_ids[1])\n",
    "            env.push(push_start, push_end) # Action performed \n",
    "\n",
    "            color_image, depth_image, _ = env_utils.get_true_heightmap(env) # Evaluating the new state for calculating the reward\n",
    "            depth_image = np.stack((depth_image, )*3, axis=-1)\n",
    "            target_mask = cv2.inRange(temp, TARGET_LOWER, TARGET_UPPER)\n",
    "            max_extents = get_max_extent_of_target_from_bottom(target_mask=target_mask, bottom_mask=target_mask, \n",
    "                                        bottom_obj_body_id=body_ids[0], \n",
    "                                        current_bottom_obj_size=testcase1.current_bottom_size, \n",
    "                                        is_viz=False)\n",
    "            \n",
    "            reward = get_reward(action='push', max_extents=max_extents, MIN_GRASP_EXTENT_THRESH=MIN_GRASP_THRESHOLDS) # get_reward(action, max_extents, MAX_EXTENT_THRESH, MIN_GRASP_EXTENT_THRESH)\n",
    "            # belman_update_val = get_belman_update_value()\n",
    "        elif action.item()==16:\n",
    "            # Check if the state is graspable and reward the agent\n",
    "            temp = cv2.cvtColor(color_image, cv2.COLOR_RGB2HSV)\n",
    "            target_mask = cv2.inRange(temp, TARGET_LOWER, TARGET_UPPER)\n",
    "            max_extents = get_max_extent_of_target_from_bottom(target_mask=target_mask, bottom_mask=target_mask, \n",
    "                                        bottom_obj_body_id=body_ids[0], \n",
    "                                        current_bottom_obj_size=testcase1.current_bottom_size, \n",
    "                                        is_viz=False)\n",
    "            \n",
    "            reward = get_reward(action='grasp', max_extents=max_extents, MIN_GRASP_EXTENT_THRESH=MIN_GRASP_THRESHOLDS)\n",
    "            if reward==1:\n",
    "                done = True\n",
    "            # done = True\n",
    "        targetPos, _ = p.getBasePositionAndOrientation(body_ids[1])\n",
    "        bottomPos, _ = p.getBasePositionAndOrientation(body_ids[0])\n",
    "        if targetPos[2] < bottomPos[2] + testcase1.current_bottom_size[2]/2 + testcase1.current_target_size[2]/2 - 0.005:\n",
    "            reward = -1\n",
    "            done = True\n",
    "        # _, reward, done, _, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], dtype=torch.float, device=device)\n",
    "        if reward == -1:\n",
    "            done=True\n",
    "\n",
    "        # Observe new state\n",
    "        # last_screen = current_screen\n",
    "        # current_screen = get_screen()\n",
    "        if not done:\n",
    "            next_state = {\n",
    "                'rgb': torch.tensor([np.transpose(color_image, (2, 0, 1))], dtype=torch.float, device=device), # transpose used in order to convert (224, 224, 3) to (3, 224, 224)\n",
    "                'height_map': torch.tensor([np.transpose(depth_image, (2, 0, 1))], dtype=torch.float, device=device) # transpose used in order to convert (224, 224, 3) to (3, 224, 224)\n",
    "            }\n",
    "        else:\n",
    "            next_state = {\n",
    "                'rgb': None,\n",
    "                'height_map': None\n",
    "            }\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state['rgb'], state['height_map'], action, next_state['rgb'], next_state['height_map'], reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        optimize_model()\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            plot_durations()\n",
    "            break\n",
    "\n",
    "        if t>=10:\n",
    "            done = True\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "print('Complete')\n",
    "# env.render()\n",
    "# env.close()\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startThreads creating 1 threads.\n",
      "starting thread 0\n",
      "started thread 0 \n",
      "argc=2\n",
      "argv[0] = --unused\n",
      "argv[1] = --start_demo_name=Physics Server\n",
      "ExampleBrowserThreadFunc started\n",
      "X11 functions dynamically loaded using dlopen/dlsym OK!\n",
      "X11 functions dynamically loaded using dlopen/dlsym OK!\n",
      "Creating context\n",
      "Created GL 3.3 context\n",
      "Direct GLX rendering context obtained\n",
      "Making context current\n",
      "GL_VENDOR=NVIDIA Corporation\n",
      "GL_RENDERER=NVIDIA GeForce GTX 1050 Ti/PCIe/SSE2\n",
      "GL_VERSION=3.3.0 NVIDIA 510.85.02\n",
      "GL_SHADING_LANGUAGE_VERSION=3.30 NVIDIA via Cg compiler\n",
      "pthread_getconcurrency()=0\n",
      "Version = 3.3.0 NVIDIA 510.85.02\n",
      "Vendor = NVIDIA Corporation\n",
      "Renderer = NVIDIA GeForce GTX 1050 Ti/PCIe/SSE2\n",
      "b3Printf: Selected demo: Physics Server\n",
      "startThreads creating 1 threads.\n",
      "starting thread 0\n",
      "started thread 0 \n",
      "MotionThreadFunc thread started\n",
      "Loading a new scene! ---------------------------------------- : True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23873/599467485.py:38: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1666642991888/work/torch/csrc/utils/tensor_new.cpp:230.)\n",
      "  'rgb': torch.tensor([np.transpose(color_image, (2, 0, 1))], dtype=torch.float, device=device), # transpose used in order to convert (224, 224, 3) to (3, 224, 224)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Push from [ 0.47087442 -0.14805217  0.09026264] to [ 0.44255585 -0.07968508  0.09026264], True\n",
      "Episode: 0, State: 0, Reward: 1.0\n",
      "Push from [0.48805217 0.00287442 0.09031931] to [ 0.41968508 -0.02544415  0.09031931], True\n",
      "Episode: 0, State: 1, Reward: 1.0\n",
      "Push from [0.316      0.109      0.09031863] to [0.316      0.035      0.09031863], True\n",
      "Episode: 0, State: 2, Reward: -1\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Push from [ 0.46594783 -0.04687442  0.08002078] to [ 0.53431492 -0.01855585  0.08002078], True\n",
      "Episode: 1, State: 0, Reward: -0.05\n",
      "Push from [0.66196194 0.16196194 0.08008015] to [0.60963604 0.10963604 0.08008015], True\n",
      "Episode: 1, State: 1, Reward: 1.0\n",
      "Push from [0.705      0.016      0.08008119] to [0.631      0.016      0.08008119], True\n",
      "Episode: 1, State: 2, Reward: 1.0\n",
      "Push from [0.387      0.01       0.08008301] to [0.461      0.01       0.08008301], True\n",
      "Episode: 1, State: 3, Reward: -0.05\n",
      "Push from [0.58087442 0.13805217 0.08008124] to [0.55255585 0.06968508 0.08008124], True\n",
      "Episode: 1, State: 4, Reward: -0.05\n",
      "Push from [ 0.44794783 -0.08487442  0.08008172] to [ 0.51631492 -0.05655585  0.08008172], True\n",
      "Episode: 1, State: 5, Reward: -0.05\n",
      "Episode: 1, State: 6, Reward: 1.0\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Push from [ 0.541      -0.12        0.09780552] to [ 0.467      -0.12        0.09780552], True\n",
      "Episode: 2, State: 0, Reward: -1\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Push from [0.35403806 0.10196194 0.09361521] to [0.40636396 0.04963604 0.09361521], True\n",
      "Episode: 3, State: 0, Reward: -0.05\n",
      "Push from [ 0.36994783 -0.02487442  0.09367235] to [0.43831492 0.00344415 0.09367235], True\n",
      "Episode: 3, State: 1, Reward: -0.05\n",
      "Push from [0.43803806 0.07196194 0.09367254] to [0.49036396 0.01963604 0.09367254], True\n",
      "Episode: 3, State: 2, Reward: -0.05\n",
      "Push from [ 0.62596194 -0.09796194  0.09367191] to [ 0.57363604 -0.04563604  0.09367191], True\n",
      "Episode: 3, State: 3, Reward: -0.05\n",
      "Push from [0.594      0.147      0.09367252] to [0.594      0.073      0.09367252], True\n",
      "Episode: 3, State: 4, Reward: -0.05\n",
      "Push from [0.49112558 0.10605217 0.0936727 ] to [0.51944415 0.03768508 0.0936727 ], True\n",
      "Episode: 3, State: 5, Reward: -0.05\n",
      "Push from [0.64687442 0.01605217 0.09367151] to [ 0.61855585 -0.05231492  0.09367151], True\n",
      "Episode: 3, State: 6, Reward: -0.05\n",
      "Push from [ 0.44803806 -0.01203806  0.09367254] to [ 0.50036396 -0.06436396  0.09367254], True\n",
      "Episode: 3, State: 7, Reward: -0.05\n",
      "Push from [ 0.70405217 -0.05512558  0.09367337] to [ 0.63568508 -0.08344415  0.09367337], True\n",
      "Episode: 3, State: 8, Reward: 1.0\n",
      "Push from [ 0.64405217 -0.13487442  0.09367189] to [ 0.57568508 -0.10655585  0.09367189], True\n",
      "Episode: 3, State: 9, Reward: -0.05\n",
      "Push from [ 0.53487442 -0.20005217  0.09367098] to [ 0.50655585 -0.13168508  0.09367098], True\n",
      "Episode: 3, State: 10, Reward: -0.05\n",
      "Push from [ 0.331      -0.054       0.09367066] to [ 0.405      -0.054       0.09367066], True\n",
      "Episode: 3, State: 11, Reward: -0.05\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Push from [ 0.54687442 -0.21205217  0.09367737] to [ 0.51855585 -0.14368508  0.09367737], True\n",
      "Episode: 4, State: 0, Reward: -0.05\n",
      "Push from [ 0.44603806 -0.13796194  0.09374394] to [ 0.49836396 -0.08563604  0.09374394], True\n",
      "Episode: 4, State: 1, Reward: -0.05\n",
      "Push from [0.38394783 0.01487442 0.09374318] to [ 0.45231492 -0.01344415  0.09374318], True\n",
      "Episode: 4, State: 2, Reward: -0.05\n",
      "Push from [ 0.65805217 -0.04687442  0.09374287] to [ 0.58968508 -0.01855585  0.09374287], True\n",
      "Episode: 4, State: 3, Reward: -0.05\n",
      "Push from [0.389      0.044      0.09374318] to [0.463      0.044      0.09374318], True\n",
      "Episode: 4, State: 4, Reward: -0.05\n",
      "Push from [ 0.49112558 -0.01805217  0.09374106] to [0.51944415 0.05031492 0.09374106], True\n",
      "Episode: 4, State: 5, Reward: -0.05\n",
      "Push from [0.473     0.104     0.0937405] to [0.547     0.104     0.0937405], True\n",
      "Episode: 4, State: 6, Reward: 1.0\n",
      "Push from [0.76605217 0.02912558 0.0937393 ] to [0.69768508 0.05744415 0.0937393 ], True\n",
      "Episode: 4, State: 7, Reward: -1\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Push from [0.69796194 0.04596194 0.08635067] to [ 0.64563604 -0.00636396  0.08635067], True\n",
      "Episode: 5, State: 0, Reward: -0.05\n",
      "Push from [ 0.423      -0.024       0.08641507] to [ 0.497      -0.024       0.08641507], True\n",
      "Episode: 5, State: 1, Reward: -0.05\n",
      "Push from [ 0.487      -0.014       0.08641506] to [ 0.561      -0.014       0.08641506], True\n",
      "Episode: 5, State: 2, Reward: -0.05\n",
      "Push from [ 0.505      -0.08        0.08641401] to [ 0.579      -0.08        0.08641401], True\n",
      "Episode: 5, State: 3, Reward: -1\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Push from [0.64396194 0.05196194 0.08827535] to [ 5.91636039e-01 -3.63961031e-04  8.82753518e-02], True\n",
      "Episode: 6, State: 0, Reward: -0.05\n",
      "Push from [ 0.50287442 -0.14605217  0.08832997] to [ 0.47455585 -0.07768508  0.08832997], True\n",
      "Episode: 6, State: 1, Reward: -0.05\n",
      "Episode: 6, State: 2, Reward: -1.0\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Push from [ 0.54687442 -0.10805217  0.09483065] to [ 0.51855585 -0.03968508  0.09483065], True\n",
      "Episode: 7, State: 0, Reward: 1.0\n",
      "Push from [0.41003806 0.12596194 0.09488733] to [0.46236396 0.07363604 0.09488733], True\n",
      "Episode: 7, State: 1, Reward: 1.0\n",
      "Push from [0.43994783 0.04887442 0.0948864 ] to [0.50831492 0.02055585 0.0948864 ], True\n",
      "Episode: 7, State: 2, Reward: 1.0\n",
      "Push from [ 0.68596194 -0.05196194  0.0948856 ] to [6.33636039e-01 3.63961031e-04 9.48856008e-02], True\n",
      "Episode: 7, State: 3, Reward: 1.0\n",
      "Push from [0.427     0.116     0.0948861] to [0.501     0.116     0.0948861], True\n",
      "Episode: 7, State: 4, Reward: -0.05\n",
      "Push from [0.465      0.026      0.09488704] to [0.539      0.026      0.09488704], True\n",
      "Episode: 7, State: 5, Reward: -1\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Push from [0.37194783 0.14487442 0.09313944] to [0.44031492 0.11655585 0.09313944], True\n",
      "Episode: 8, State: 0, Reward: -0.05\n",
      "Push from [ 0.49287442 -0.07605217  0.09319469] to [ 0.46455585 -0.00768508  0.09319469], True\n",
      "Episode: 8, State: 1, Reward: -0.05\n",
      "Push from [0.367      0.106      0.09319421] to [0.441      0.106      0.09319421], True\n",
      "Episode: 8, State: 2, Reward: 1.0\n",
      "Push from [0.45203806 0.15196194 0.09319454] to [0.50436396 0.09963604 0.09319454], True\n",
      "Episode: 8, State: 3, Reward: 1.0\n",
      "Push from [ 0.56487442 -0.10405217  0.09319544] to [ 0.53655585 -0.03568508  0.09319544], True\n",
      "Episode: 8, State: 4, Reward: 1.0\n",
      "Push from [0.643      0.02       0.09319601] to [0.569      0.02       0.09319601], True\n",
      "Episode: 8, State: 5, Reward: -0.05\n",
      "Push from [0.45603806 0.18596194 0.09319317] to [0.50836396 0.13363604 0.09319317], True\n",
      "Episode: 8, State: 6, Reward: -0.05\n",
      "Push from [0.411      0.062      0.09319495] to [0.485      0.062      0.09319495], True\n",
      "Episode: 8, State: 7, Reward: -0.05\n",
      "Push from [ 0.50403806 -0.04796194  0.09319488] to [0.55636396 0.00436396 0.09319488], True\n",
      "Episode: 8, State: 8, Reward: -1\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Push from [ 0.604      -0.033       0.09586172] to [0.604      0.041      0.09586172], True\n",
      "Episode: 9, State: 0, Reward: -0.05\n",
      "Push from [0.46203806 0.16996194 0.09591687] to [0.51436396 0.11763604 0.09591687], True\n",
      "Episode: 9, State: 1, Reward: -0.05\n",
      "Push from [0.487      0.07       0.09591673] to [0.561      0.07       0.09591673], True\n",
      "Episode: 9, State: 2, Reward: -1\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Push from [0.42403806 0.12596194 0.09835666] to [0.47636396 0.07363604 0.09835666], True\n",
      "Episode: 10, State: 0, Reward: -0.05\n",
      "Push from [ 0.66196194 -0.03596194  0.09842074] to [0.60963604 0.01636396 0.09842074], True\n",
      "Episode: 10, State: 1, Reward: -0.05\n",
      "Push from [0.411      0.01       0.09842113] to [0.485      0.01       0.09842113], True\n",
      "Episode: 10, State: 2, Reward: -0.05\n",
      "Push from [0.44994783 0.09487442 0.09842008] to [0.51831492 0.06655585 0.09842008], True\n",
      "Episode: 10, State: 3, Reward: 1.0\n",
      "Push from [0.505      0.048      0.09842247] to [0.579      0.048      0.09842247], True\n",
      "Episode: 10, State: 4, Reward: -1\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Push from [ 0.287      -0.022       0.08618923] to [ 0.361      -0.022       0.08618923], True\n",
      "Episode: 11, State: 0, Reward: 1.0\n",
      "Push from [ 0.343      -0.022       0.08624808] to [ 0.417      -0.022       0.08624808], True\n",
      "Episode: 11, State: 1, Reward: 1.0\n",
      "Push from [0.63805217 0.03087442 0.08624696] to [0.56968508 0.00255585 0.08624696], True\n",
      "Episode: 11, State: 2, Reward: 1.0\n",
      "Push from [ 0.46887442 -0.12005217  0.0862481 ] to [ 0.44055585 -0.05168508  0.0862481 ], True\n",
      "Episode: 11, State: 3, Reward: 1.0\n",
      "Push from [0.54087442 0.11605217 0.08624744] to [0.51255585 0.04768508 0.08624744], True\n",
      "Episode: 11, State: 4, Reward: 1.0\n",
      "Push from [0.53796194 0.04596194 0.08624713] to [ 0.48563604 -0.00636396  0.08624713], True\n",
      "Episode: 11, State: 5, Reward: 1.0\n",
      "Push from [0.46996194 0.09796194 0.08624632] to [0.41763604 0.04563604 0.08624632], True\n",
      "Episode: 11, State: 6, Reward: 1.0\n",
      "Push from [ 0.505      -0.038       0.08624656] to [ 0.431      -0.038       0.08624656], True\n",
      "Episode: 11, State: 7, Reward: -1\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Push from [ 0.63805217 -0.05912558  0.09706663] to [ 0.56968508 -0.08744415  0.09706663], True\n",
      "Episode: 12, State: 0, Reward: -0.05\n",
      "Push from [ 0.57805217 -0.13087442  0.09712368] to [ 0.50968508 -0.10255585  0.09712368], True\n",
      "Episode: 12, State: 1, Reward: -0.05\n",
      "Push from [ 0.55405217 -0.04087442  0.09712263] to [ 0.48568508 -0.01255585  0.09712263], True\n",
      "Episode: 12, State: 2, Reward: -0.05\n",
      "Push from [ 0.48796194 -0.11796194  0.09712267] to [ 0.43563604 -0.06563604  0.09712267], True\n",
      "Episode: 12, State: 3, Reward: -1\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Push from [ 0.399     -0.118      0.0927599] to [ 0.473     -0.118      0.0927599], True\n",
      "Episode: 13, State: 0, Reward: 1.0\n",
      "Push from [0.62487442 0.01405217 0.09282104] to [ 0.59655585 -0.05431492  0.09282104], True\n",
      "Episode: 13, State: 1, Reward: 1.0\n",
      "Push from [ 0.421      -0.12        0.09282045] to [ 0.495      -0.12        0.09282045], True\n",
      "Episode: 13, State: 2, Reward: 1.0\n",
      "Push from [ 0.69196194 -0.04803806  0.09282181] to [ 0.63963604 -0.10036396  0.09282181], True\n",
      "Episode: 13, State: 3, Reward: 1.0\n",
      "Push from [ 0.447      -0.098       0.09282074] to [ 0.521      -0.098       0.09282074], True\n",
      "Episode: 13, State: 4, Reward: 1.0\n",
      "Push from [ 0.53003806 -0.03603806  0.09282082] to [ 0.58236396 -0.08836396  0.09282082], True\n",
      "Episode: 13, State: 5, Reward: -1\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Push from [ 0.55887442 -0.00794783  0.0877813 ] to [ 0.53055585 -0.07631492  0.0877813 ], True\n",
      "Episode: 14, State: 0, Reward: -1\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Push from [0.38912558 0.01405217 0.09822163] to [ 0.41744415 -0.05431492  0.09822163], True\n",
      "Episode: 15, State: 0, Reward: -1\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Episode: 16, State: 0, Reward: 1.0\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Push from [0.35       0.057      0.10108816] to [ 0.35       -0.017       0.10108816], True\n",
      "Episode: 17, State: 0, Reward: 1.0\n",
      "Push from [ 0.267      -0.072       0.10114673] to [ 0.341      -0.072       0.10114673], True\n",
      "Episode: 17, State: 1, Reward: -1\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Push from [0.357      0.026      0.10202382] to [0.431      0.026      0.10202382], True\n",
      "Episode: 18, State: 0, Reward: -0.05\n",
      "Push from [0.389      0.076      0.10208399] to [0.463      0.076      0.10208399], True\n",
      "Episode: 18, State: 1, Reward: -0.05\n",
      "Push from [ 0.56487442 -0.04005217  0.10208396] to [0.53655585 0.02831492 0.10208396], True\n",
      "Episode: 18, State: 2, Reward: -0.05\n",
      "Push from [0.461      0.082      0.10208491] to [0.535      0.082      0.10208491], True\n",
      "Episode: 18, State: 3, Reward: -1\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Push from [ 0.637      -0.056       0.10805147] to [ 0.563      -0.056       0.10805147], True\n",
      "Episode: 19, State: 0, Reward: -0.05\n",
      "Push from [0.37994783 0.01287442 0.1081083 ] to [ 0.44831492 -0.01544415  0.1081083 ], True\n",
      "Episode: 19, State: 1, Reward: 1.0\n",
      "Push from [ 0.60287442 -0.09605217  0.10810939] to [ 0.57455585 -0.02768508  0.10810939], True\n",
      "Episode: 19, State: 2, Reward: 1.0\n",
      "Push from [0.52887442 0.15405217 0.10810889] to [0.50055585 0.08568508 0.10810889], True\n",
      "Episode: 19, State: 3, Reward: 1.0\n",
      "Push from [ 0.389      -0.036       0.10810917] to [ 0.463      -0.036       0.10810917], True\n",
      "Episode: 19, State: 4, Reward: 1.0\n",
      "Push from [0.419      0.03       0.10810837] to [0.493      0.03       0.10810837], True\n",
      "Episode: 19, State: 5, Reward: 1.0\n",
      "Push from [ 0.481      -0.036       0.10810873] to [ 0.555      -0.036       0.10810873], True\n",
      "Episode: 19, State: 6, Reward: -0.05\n",
      "Push from [0.507      0.026      0.10810651] to [0.581      0.026      0.10810651], True\n",
      "Episode: 19, State: 7, Reward: -1\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Push from [0.46912558 0.12605217 0.09838504] to [0.49744415 0.05768508 0.09838504], True\n",
      "Episode: 20, State: 0, Reward: -0.05\n",
      "Push from [0.56796194 0.05796194 0.09844723] to [0.51563604 0.00563604 0.09844723], True\n",
      "Episode: 20, State: 1, Reward: -0.05\n",
      "Push from [ 0.41794783 -0.00712558  0.09844555] to [ 0.48631492 -0.03544415  0.09844555], True\n",
      "Episode: 20, State: 2, Reward: -0.05\n",
      "Push from [ 0.407      -0.102       0.09844698] to [ 0.481      -0.102       0.09844698], True\n",
      "Episode: 20, State: 3, Reward: 1.0\n",
      "Push from [ 0.467     -0.088      0.0984459] to [ 0.541     -0.088      0.0984459], True\n",
      "Episode: 20, State: 4, Reward: -1\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Push from [ 0.417      -0.008       0.07693798] to [ 0.491      -0.008       0.07693798], True\n",
      "Episode: 21, State: 0, Reward: -0.05\n",
      "Push from [0.47994783 0.01687442 0.07699568] to [ 0.54831492 -0.01144415  0.07699568], True\n",
      "Episode: 21, State: 1, Reward: -1\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Push from [ 0.392      -0.111       0.09394623] to [ 0.392      -0.037       0.09394623], True\n",
      "Episode: 22, State: 0, Reward: 1.0\n",
      "Push from [0.287      0.086      0.09400411] to [0.361      0.086      0.09400411], True\n",
      "Episode: 22, State: 1, Reward: 1.0\n",
      "Push from [0.349      0.02       0.09400653] to [0.423      0.02       0.09400653], True\n",
      "Episode: 22, State: 2, Reward: -0.05\n",
      "Push from [ 0.50287442 -0.03605217  0.09400557] to [0.47455585 0.03231492 0.09400557], True\n",
      "Episode: 22, State: 3, Reward: -0.05\n",
      "Push from [0.40603806 0.03803806 0.09400573] to [0.45836396 0.09036396 0.09400573], True\n",
      "Episode: 22, State: 4, Reward: -1\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Push from [0.273      0.124      0.10362527] to [0.347      0.124      0.10362527], True\n",
      "Episode: 23, State: 0, Reward: 1.0\n",
      "Push from [0.335     0.124     0.1036873] to [0.409     0.124     0.1036873], True\n",
      "Episode: 23, State: 1, Reward: 1.0\n",
      "Push from [0.466      0.195      0.10368634] to [0.466      0.121      0.10368634], True\n",
      "Episode: 23, State: 2, Reward: -0.05\n",
      "Push from [0.64205217 0.02912558 0.10368688] to [0.57368508 0.05744415 0.10368688], True\n",
      "Episode: 23, State: 3, Reward: -0.05\n",
      "Push from [0.37594783 0.00712558 0.10368493] to [0.44431492 0.03544415 0.10368493], True\n",
      "Episode: 23, State: 4, Reward: -0.05\n",
      "Push from [0.427      0.054      0.10368697] to [0.501      0.054      0.10368697], True\n",
      "Episode: 23, State: 5, Reward: -0.05\n",
      "Push from [ 0.58487442 -0.01405217  0.10368602] to [0.55655585 0.05431492 0.10368602], True\n",
      "Episode: 23, State: 6, Reward: -1\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Push from [0.277      0.11       0.10146609] to [0.351      0.11       0.10146609], True\n",
      "Episode: 24, State: 0, Reward: 1.0\n",
      "Push from [0.335      0.116      0.10150026] to [0.409      0.116      0.10150026], True\n",
      "Episode: 24, State: 1, Reward: 1.0\n",
      "Push from [0.385      0.034      0.10150266] to [0.459      0.034      0.10150266], True\n",
      "Episode: 24, State: 2, Reward: -0.05\n",
      "Push from [0.413      0.116      0.10150159] to [0.487      0.116      0.10150159], True\n",
      "Episode: 24, State: 3, Reward: -0.05\n",
      "Push from [0.467      0.04       0.10149969] to [0.541      0.04       0.10149969], True\n",
      "Episode: 24, State: 4, Reward: -1\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Push from [ 0.327      -0.04        0.08676492] to [ 0.401      -0.04        0.08676492], True\n",
      "Episode: 25, State: 0, Reward: -0.05\n",
      "Push from [ 0.51      -0.163      0.0868201] to [ 0.51      -0.089      0.0868201], True\n",
      "Episode: 25, State: 1, Reward: 1.0\n",
      "Push from [ 0.64205217 -0.00287442  0.08682056] to [0.57368508 0.02544415 0.08682056], True\n",
      "Episode: 25, State: 2, Reward: 1.0\n",
      "Push from [ 0.56005217 -0.05487442  0.08681979] to [ 0.49168508 -0.02655585  0.08681979], True\n",
      "Episode: 25, State: 3, Reward: -0.05\n",
      "Push from [0.331      0.068      0.08681817] to [0.405      0.068      0.08681817], True\n",
      "Episode: 25, State: 4, Reward: -0.05\n",
      "Push from [0.39794783 0.09287442 0.08681956] to [0.46631492 0.06455585 0.08681956], True\n",
      "Episode: 25, State: 5, Reward: 1.0\n",
      "Push from [0.443      0.032      0.08681823] to [0.517      0.032      0.08681823], True\n",
      "Episode: 25, State: 6, Reward: -0.05\n",
      "Push from [0.49394783 0.13887442 0.08681961] to [0.56231492 0.11055585 0.08681961], True\n",
      "Episode: 25, State: 7, Reward: -1\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Push from [ 0.44687442 -0.20405217  0.10481208] to [ 0.41855585 -0.13568508  0.10481208], True\n",
      "Episode: 26, State: 0, Reward: 1.0\n",
      "Push from [ 0.329      -0.078       0.10487529] to [ 0.403      -0.078       0.10487529], True\n",
      "Episode: 26, State: 1, Reward: 1.0\n",
      "Episode: 26, State: 2, Reward: 1.0\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Push from [ 0.552     -0.007      0.0907733] to [0.552     0.067     0.0907733], True\n",
      "Episode: 27, State: 0, Reward: -1\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Push from [ 0.427      -0.102       0.09532204] to [ 0.501      -0.102       0.09532204], True\n",
      "Episode: 28, State: 0, Reward: -1\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Push from [ 0.52287442 -0.17805217  0.08101883] to [ 0.49455585 -0.10968508  0.08101883], True\n",
      "Episode: 29, State: 0, Reward: -0.05\n",
      "Push from [ 0.319     -0.022      0.0810729] to [ 0.393     -0.022      0.0810729], True\n",
      "Episode: 29, State: 1, Reward: -0.05\n",
      "Push from [0.371      0.056      0.08107331] to [0.445      0.056      0.08107331], True\n",
      "Episode: 29, State: 2, Reward: -0.05\n",
      "Push from [ 0.397      -0.016       0.08107459] to [ 0.471      -0.016       0.08107459], True\n",
      "Episode: 29, State: 3, Reward: -0.05\n",
      "Push from [ 0.459      -0.018       0.08107417] to [ 0.533      -0.018       0.08107417], True\n",
      "Episode: 29, State: 4, Reward: -0.05\n",
      "Push from [0.501      0.056      0.08107155] to [0.575      0.056      0.08107155], True\n",
      "Episode: 29, State: 5, Reward: -1\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Push from [ 0.309      -0.102       0.07973794] to [ 0.383      -0.102       0.07973794], True\n",
      "Episode: 30, State: 0, Reward: 1.0\n",
      "Push from [ 0.448     -0.169      0.0797985] to [ 0.448     -0.095      0.0797985], True\n",
      "Episode: 30, State: 1, Reward: 1.0\n",
      "Push from [0.345      0.044      0.07979755] to [0.419      0.044      0.07979755], True\n",
      "Episode: 30, State: 2, Reward: 1.0\n",
      "Push from [ 0.57605217 -0.08287442  0.07979915] to [ 0.50768508 -0.05455585  0.07979915], True\n",
      "Episode: 30, State: 3, Reward: 1.0\n",
      "Push from [0.357      0.022      0.07979829] to [0.431      0.022      0.07979829], True\n",
      "Episode: 30, State: 4, Reward: 1.0\n",
      "Push from [0.413      0.024      0.07979897] to [0.487      0.024      0.07979897], True\n",
      "Episode: 30, State: 5, Reward: 1.0\n",
      "Episode: 30, State: 6, Reward: 1.0\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Push from [ 0.419     -0.06       0.0883516] to [ 0.493     -0.06       0.0883516], True\n",
      "Episode: 31, State: 0, Reward: -0.05\n",
      "Push from [0.447      0.004      0.08841628] to [0.521      0.004      0.08841628], True\n",
      "Episode: 31, State: 1, Reward: -0.05\n",
      "Push from [0.51794783 0.03487442 0.08841471] to [0.58631492 0.00655585 0.08841471], True\n",
      "Episode: 31, State: 2, Reward: -1\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Push from [0.54912558 0.22005217 0.07829513] to [0.57744415 0.15168508 0.07829513], True\n",
      "Episode: 32, State: 0, Reward: 1.0\n",
      "Push from [0.485      0.054      0.07835316] to [0.559      0.054      0.07835316], True\n",
      "Episode: 32, State: 1, Reward: -1\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Push from [ 0.289      -0.102       0.09883823] to [ 0.363      -0.102       0.09883823], True\n",
      "Episode: 33, State: 0, Reward: -0.05\n",
      "Push from [ 0.339      -0.04        0.09889465] to [ 0.413      -0.04        0.09889465], True\n",
      "Episode: 33, State: 1, Reward: -0.05\n",
      "Push from [ 0.59205217 -0.10287442  0.09889398] to [ 0.52368508 -0.07455585  0.09889398], True\n",
      "Episode: 33, State: 2, Reward: -0.05\n",
      "Push from [0.47       0.009      0.09889397] to [ 0.47       -0.065       0.09889397], True\n",
      "Episode: 33, State: 3, Reward: -1\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Push from [0.46996194 0.01596194 0.09633897] to [ 0.41763604 -0.03636396  0.09633897], True\n",
      "Episode: 34, State: 0, Reward: -1\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Push from [ 0.69396194 -0.04996194  0.08644834] to [0.64163604 0.00236396 0.08644834], True\n",
      "Episode: 35, State: 0, Reward: -0.05\n",
      "Push from [0.663      0.04       0.08650605] to [0.589      0.04       0.08650605], True\n",
      "Episode: 35, State: 1, Reward: -0.05\n",
      "Push from [0.383      0.014      0.08650782] to [0.457      0.014      0.08650782], True\n",
      "Episode: 35, State: 2, Reward: -0.05\n",
      "Push from [ 0.423     -0.068      0.0865075] to [ 0.497     -0.068      0.0865075], True\n",
      "Episode: 35, State: 3, Reward: -0.05\n",
      "Push from [ 0.51003806 -0.11196194  0.08650587] to [ 0.56236396 -0.05963604  0.08650587], True\n",
      "Episode: 35, State: 4, Reward: -0.05\n",
      "Push from [0.469      0.022      0.08650542] to [0.543      0.022      0.08650542], True\n",
      "Episode: 35, State: 5, Reward: -0.05\n",
      "Push from [0.531      0.092      0.08650521] to [0.605      0.092      0.08650521], True\n",
      "Episode: 35, State: 6, Reward: -1\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Push from [0.527      0.038      0.08758414] to [0.453      0.038      0.08758414], True\n",
      "Episode: 36, State: 0, Reward: -1\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Push from [0.425      0.054      0.09289657] to [0.499      0.054      0.09289657], True\n",
      "Episode: 37, State: 0, Reward: -0.05\n",
      "Push from [ 0.451      -0.008       0.09295999] to [ 0.525      -0.008       0.09295999], True\n",
      "Episode: 37, State: 1, Reward: -0.05\n",
      "Push from [ 0.513      -0.008       0.09295768] to [ 0.587      -0.008       0.09295768], True\n",
      "Episode: 37, State: 2, Reward: -1\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Push from [0.371      0.034      0.10157147] to [0.445      0.034      0.10157147], True\n",
      "Episode: 38, State: 0, Reward: -0.05\n",
      "Push from [0.413      0.086      0.10163016] to [0.487      0.086      0.10163016], True\n",
      "Episode: 38, State: 1, Reward: -0.05\n",
      "Push from [0.475      0.088      0.10163002] to [0.549      0.088      0.10163002], True\n",
      "Episode: 38, State: 2, Reward: -0.05\n",
      "Push from [0.511      0.038      0.10163272] to [0.585      0.038      0.10163272], True\n",
      "Episode: 38, State: 3, Reward: -1\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Push from [0.367     0.05      0.0983307] to [0.441     0.05      0.0983307], True\n",
      "Episode: 39, State: 0, Reward: 1.0\n",
      "Push from [0.429      0.036      0.09838862] to [0.503      0.036      0.09838862], True\n",
      "Episode: 39, State: 1, Reward: 1.0\n",
      "Push from [0.445      0.106      0.09838716] to [0.519      0.106      0.09838716], True\n",
      "Episode: 39, State: 2, Reward: -1\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Push from [0.257      0.044      0.10272178] to [0.331      0.044      0.10272178], True\n",
      "Episode: 40, State: 0, Reward: 1.0\n",
      "Push from [0.319      0.046      0.10278173] to [0.393      0.046      0.10278173], True\n",
      "Episode: 40, State: 1, Reward: -0.05\n",
      "Push from [0.381      0.046      0.10278311] to [0.455      0.046      0.10278311], True\n",
      "Episode: 40, State: 2, Reward: -0.05\n",
      "Push from [0.443      0.03       0.10278345] to [0.517      0.03       0.10278345], True\n",
      "Episode: 40, State: 3, Reward: -0.05\n",
      "Push from [0.465      0.104      0.10278272] to [0.539      0.104      0.10278272], True\n",
      "Episode: 40, State: 4, Reward: -0.05\n",
      "Push from [0.525      0.034      0.10278145] to [0.599      0.034      0.10278145], True\n",
      "Episode: 40, State: 5, Reward: -1\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Push from [ 0.413      -0.04        0.09475016] to [ 0.487      -0.04        0.09475016], True\n",
      "Episode: 41, State: 0, Reward: -0.05\n",
      "Push from [ 0.473    -0.044     0.094805] to [ 0.547    -0.044     0.094805], True\n",
      "Episode: 41, State: 1, Reward: -1\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Push from [ 0.477      -0.038       0.08298642] to [ 0.551      -0.038       0.08298642], True\n",
      "Episode: 42, State: 0, Reward: -0.05\n",
      "Push from [ 0.531      -0.038       0.08305287] to [ 0.605      -0.038       0.08305287], True\n",
      "Episode: 42, State: 1, Reward: -1\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Push from [ 0.45003806 -0.03403806  0.08064396] to [ 0.50236396 -0.08636396  0.08064396], True\n",
      "Episode: 43, State: 0, Reward: 1.0\n",
      "Push from [ 0.473      -0.124       0.08069827] to [ 0.547      -0.124       0.08069827], True\n",
      "Episode: 43, State: 1, Reward: -1\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Push from [0.331      0.026      0.07791172] to [0.405      0.026      0.07791172], True\n",
      "Episode: 44, State: 0, Reward: 1.0\n",
      "Push from [0.389      0.026      0.07796846] to [0.463      0.026      0.07796846], True\n",
      "Episode: 44, State: 1, Reward: 1.0\n",
      "Push from [0.427      0.098      0.07796735] to [0.501      0.098      0.07796735], True\n",
      "Episode: 44, State: 2, Reward: 1.0\n",
      "Push from [0.465      0.024      0.07796701] to [0.539      0.024      0.07796701], True\n",
      "Episode: 44, State: 3, Reward: -1\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Push from [ 0.343      -0.108       0.07723278] to [ 0.417      -0.108       0.07723278], True\n",
      "Episode: 45, State: 0, Reward: -0.05\n",
      "Push from [ 0.403      -0.11        0.07729031] to [ 0.477      -0.11        0.07729031], True\n",
      "Episode: 45, State: 1, Reward: 1.0\n",
      "Push from [ 0.46194783 -0.14287442  0.07729087] to [ 0.53031492 -0.11455585  0.07729087], True\n",
      "Episode: 45, State: 2, Reward: -0.05\n",
      "Push from [ 0.69605217 -0.03912558  0.07729248] to [ 0.62768508 -0.06744415  0.07729248], True\n",
      "Episode: 45, State: 3, Reward: -0.05\n",
      "Push from [ 0.447      -0.026       0.07729217] to [ 0.521      -0.026       0.07729217], True\n",
      "Episode: 45, State: 4, Reward: -0.05\n",
      "Push from [ 0.465      -0.088       0.07729174] to [ 0.539      -0.088       0.07729174], True\n",
      "Episode: 45, State: 5, Reward: -0.05\n",
      "Push from [ 0.72196194 -0.01003806  0.07728995] to [ 0.66963604 -0.06236396  0.07728995], True\n",
      "Episode: 45, State: 6, Reward: 1.0\n",
      "Push from [0.594      0.005      0.07729001] to [ 0.594      -0.069       0.07729001], True\n",
      "Episode: 45, State: 7, Reward: -1\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Push from [0.443      0.096      0.09847009] to [0.517      0.096      0.09847009], True\n",
      "Episode: 46, State: 0, Reward: -0.05\n",
      "Push from [0.493      0.018      0.09853164] to [0.567      0.018      0.09853164], True\n",
      "Episode: 46, State: 1, Reward: -1\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Push from [0.445      0.038      0.09710327] to [0.519      0.038      0.09710327], True\n",
      "Episode: 47, State: 0, Reward: 1.0\n",
      "Push from [0.69196194 0.04003806 0.09716248] to [0.63963604 0.09236396 0.09716248], True\n",
      "Episode: 47, State: 1, Reward: 1.0\n",
      "Push from [0.429      0.098      0.09716697] to [0.503      0.098      0.09716697], True\n",
      "Episode: 47, State: 2, Reward: -1\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Push from [ 0.51487442 -0.14605217  0.0860611 ] to [ 0.48655585 -0.07768508  0.0860611 ], True\n",
      "Episode: 48, State: 0, Reward: -0.05\n",
      "Push from [0.351     0.042     0.0861179] to [0.425     0.042     0.0861179], True\n",
      "Episode: 48, State: 1, Reward: -0.05\n",
      "Push from [0.413      0.048      0.08611722] to [0.487      0.048      0.08611722], True\n",
      "Episode: 48, State: 2, Reward: -0.05\n",
      "Push from [ 0.441    -0.022     0.086117] to [ 0.515    -0.022     0.086117], True\n",
      "Episode: 48, State: 3, Reward: -0.05\n",
      "Push from [0.489      0.048      0.08611508] to [0.563      0.048      0.08611508], True\n",
      "Episode: 48, State: 4, Reward: -0.05\n",
      "Push from [ 0.67996194 -0.09996194  0.08611398] to [ 0.62763604 -0.04763604  0.08611398], True\n",
      "Episode: 48, State: 5, Reward: 1.0\n",
      "Push from [0.646      0.141      0.08611421] to [0.646      0.067      0.08611421], True\n",
      "Episode: 48, State: 6, Reward: 1.0\n",
      "Push from [ 7.11000000e-01 -7.96020419e-18  8.61159627e-02] to [6.37000000e-01 1.10218212e-18 8.61159627e-02], True\n",
      "Episode: 48, State: 7, Reward: 1.0\n",
      "Push from [ 0.423      -0.03        0.08611742] to [ 0.497      -0.03        0.08611742], True\n",
      "Episode: 48, State: 8, Reward: -0.05\n",
      "Push from [0.467      0.028      0.08611605] to [0.541      0.028      0.08611605], True\n",
      "Episode: 48, State: 9, Reward: -0.05\n",
      "Push from [ 0.513      -0.03        0.08611405] to [ 0.587      -0.03        0.08611405], True\n",
      "Episode: 48, State: 10, Reward: -1\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Push from [0.475      0.046      0.09808452] to [0.549      0.046      0.09808452], True\n",
      "Episode: 49, State: 0, Reward: 1.0\n",
      "Push from [0.55203806 0.08196194 0.09814149] to [0.60436396 0.02963604 0.09814149], True\n",
      "Episode: 49, State: 1, Reward: 1.0\n",
      "Push from [ 0.67087442 -0.13405217  0.09813961] to [ 0.64255585 -0.06568508  0.09813961], True\n",
      "Episode: 49, State: 2, Reward: 1.0\n",
      "Push from [0.53803806 0.07596194 0.09814061] to [0.59036396 0.02363604 0.09814061], True\n",
      "Episode: 49, State: 3, Reward: 1.0\n",
      "Push from [0.622      0.045      0.09813925] to [ 0.622      -0.029       0.09813925], True\n",
      "Episode: 49, State: 4, Reward: 1.0\n",
      "Push from [ 0.457      -0.078       0.09814031] to [ 0.531      -0.078       0.09814031], True\n",
      "Episode: 49, State: 5, Reward: -1\n",
      "Complete\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from itertools import count\n",
    "\n",
    "from Config.constants import MIN_GRASP_THRESHOLDS\n",
    "\n",
    "is_viz = False\n",
    "\n",
    "# env = Environment()\n",
    "env = Environment(gui=True)\n",
    "num_of_envs = 10\n",
    "max_num_of_actions = 10\n",
    "is_viz = False\n",
    "max_extent_threshold = 1 # Max extent threshold of the target object in pixel units\n",
    "push_directions = [0, np.pi/8, np.pi/4, 3*np.pi/8, \n",
    "                    np.pi/2, 5*np.pi/8, 3*np.pi/4, 7*np.pi/8, \n",
    "                    np.pi, 9*np.pi/8, 5*np.pi/4, 11*np.pi/8,  \n",
    "                    3*np.pi/2, 13*np.pi/8, 7*np.pi/4, 15*np.pi/8] # 16 standard directions\n",
    "\n",
    "\n",
    "num_episodes = 50\n",
    "\n",
    "checkpoint = torch.load('/home/vishal/Volume_E/Active/Undergrad_research/ICRA22/codebases/Mid-Level-Planner/V2_next_best_action/models/model_checkpoints/100.pt')\n",
    "policy_net.load_state_dict(checkpoint)\n",
    "policy_net.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i_episode in range(num_episodes):\n",
    "        # Initialize the environment and state\n",
    "        env.reset()\n",
    "        testcase1 = TestCase1(env)\n",
    "        body_ids, success = testcase1.sample_test_case(bottom_obj='random') #'random') # testcase1.create_standard()\n",
    "        color_image, depth_image, _ = env_utils.get_true_heightmap(env)\n",
    "        depth_image = np.stack((depth_image, )*3, axis=-1)\n",
    "        # print(\"Returned body ids: {}, success: {}\".format(body_ids, success))\n",
    "        # last_screen = get_screen()\n",
    "        # current_screen = get_screen()\n",
    "        # state = current_screen - last_screen\n",
    "        state = {\n",
    "            'rgb': torch.tensor([np.transpose(color_image, (2, 0, 1))], dtype=torch.float, device=device), # transpose used in order to convert (224, 224, 3) to (3, 224, 224)\n",
    "            'height_map': torch.tensor([np.transpose(depth_image, (2, 0, 1))], dtype=torch.float, device=device) # torch.tensor([np.transpose(depth_image, (2, 0, 1))], device=device) # transpose used in order to convert (224, 224, 3) to (3, 224, 224)\n",
    "        }\n",
    "        done = False\n",
    "\n",
    "        for t in count():\n",
    "            # Select and perform an action\n",
    "            action = select_action(state['rgb'], state['height_map'])\n",
    "            if action.item() in range(0, 16): # push action\n",
    "                temp = cv2.cvtColor(color_image, cv2.COLOR_RGB2HSV)\n",
    "                target_mask = cv2.inRange(temp, TARGET_LOWER, TARGET_UPPER)\n",
    "                push_dir = push_directions[action.item()] # Sample push directions\n",
    "                push_start, push_end = get_push_start(push_dir, target_mask, body_ids[1])\n",
    "                env.push(push_start, push_end) # Action performed \n",
    "\n",
    "                color_image, depth_image, _ = env_utils.get_true_heightmap(env) # Evaluating the new state for calculating the reward\n",
    "                depth_image = np.stack((depth_image, )*3, axis=-1)\n",
    "                target_mask = cv2.inRange(temp, TARGET_LOWER, TARGET_UPPER)\n",
    "                max_extents = get_max_extent_of_target_from_bottom(target_mask=target_mask, bottom_mask=target_mask, \n",
    "                                            bottom_obj_body_id=body_ids[0], \n",
    "                                            current_bottom_obj_size=testcase1.current_bottom_size, \n",
    "                                            is_viz=False)\n",
    "                \n",
    "                reward = get_reward(action='push', max_extents=max_extents, MIN_GRASP_EXTENT_THRESH=MIN_GRASP_THRESHOLDS) # get_reward(action, max_extents, MAX_EXTENT_THRESH, MIN_GRASP_EXTENT_THRESH)\n",
    "                # belman_update_val = get_belman_update_value()\n",
    "            elif action.item()==16:\n",
    "                # Check if the state is graspable and reward the agent\n",
    "                temp = cv2.cvtColor(color_image, cv2.COLOR_RGB2HSV)\n",
    "                target_mask = cv2.inRange(temp, TARGET_LOWER, TARGET_UPPER)\n",
    "                max_extents = get_max_extent_of_target_from_bottom(target_mask=target_mask, bottom_mask=target_mask, \n",
    "                                            bottom_obj_body_id=body_ids[0], \n",
    "                                            current_bottom_obj_size=testcase1.current_bottom_size, \n",
    "                                            is_viz=False)\n",
    "                \n",
    "                reward = get_reward(action='grasp', max_extents=max_extents, MIN_GRASP_EXTENT_THRESH=MIN_GRASP_THRESHOLDS)\n",
    "                if reward==1:\n",
    "                    done = True\n",
    "                # done = True\n",
    "            targetPos, _ = p.getBasePositionAndOrientation(body_ids[1])\n",
    "            bottomPos, _ = p.getBasePositionAndOrientation(body_ids[0])\n",
    "            if targetPos[2] < bottomPos[2] + testcase1.current_bottom_size[2]/2 + testcase1.current_target_size[2]/2 - 0.005:\n",
    "                reward = -1\n",
    "                done = True\n",
    "\n",
    "            print(\"Episode: {}, State: {}, Reward: {}\".format(i_episode, t, reward))\n",
    "            # _, reward, done, _, _ = env.step(action.item())\n",
    "            reward = torch.tensor([reward], dtype=torch.float, device=device)\n",
    "\n",
    "            if reward == -1:\n",
    "                done=True\n",
    "\n",
    "            # Observe new state\n",
    "            # last_screen = current_screen\n",
    "            # current_screen = get_screen()\n",
    "            if not done:\n",
    "                next_state = {\n",
    "                    'rgb': torch.tensor([np.transpose(color_image, (2, 0, 1))], dtype=torch.float, device=device), # transpose used in order to convert (224, 224, 3) to (3, 224, 224)\n",
    "                    'height_map': torch.tensor([np.transpose(depth_image, (2, 0, 1))], dtype=torch.float, device=device) # transpose used in order to convert (224, 224, 3) to (3, 224, 224)\n",
    "                }\n",
    "            else:\n",
    "                next_state = {\n",
    "                    'rgb': None,\n",
    "                    'height_map': None\n",
    "                }\n",
    "\n",
    "            # Store the transition in memory\n",
    "            # memory.push(state['rgb'], state['height_map'], action, next_state['rgb'], next_state['height_map'], reward)\n",
    "\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "\n",
    "            # Perform one step of the optimization (on the policy network)\n",
    "            # optimize_model()\n",
    "            if done:\n",
    "                # episode_durations.append(t + 1)\n",
    "                # plot_durations()\n",
    "                break\n",
    "\n",
    "            if t>=10:\n",
    "                done = True\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    # if i_episode % TARGET_UPDATE == 0:\n",
    "        # target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "print('Complete')\n",
    "# env.render()\n",
    "# env.close()\n",
    "# plt.ioff()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'policy_net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mdel\u001b[39;00m policy_net\n\u001b[1;32m      2\u001b[0m \u001b[39mdel\u001b[39;00m target_net\n",
      "\u001b[0;31mNameError\u001b[0m: name 'policy_net' is not defined"
     ]
    }
   ],
   "source": [
    "del policy_net\n",
    "del target_net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Residual code \n",
    "Ignore the parts of code that follow this block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "push_color_trunk = torchvision.models.densenet.densenet121(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─Sequential: 1-1                        [-1, 1024, 7, 7]          --\n",
      "|    └─Conv2d: 2-1                       [-1, 64, 112, 112]        9,408\n",
      "|    └─BatchNorm2d: 2-2                  [-1, 64, 112, 112]        128\n",
      "|    └─ReLU: 2-3                         [-1, 64, 112, 112]        --\n",
      "|    └─MaxPool2d: 2-4                    [-1, 64, 56, 56]          --\n",
      "|    └─_DenseBlock: 2-5                  [-1, 256, 56, 56]         --\n",
      "|    |    └─_DenseLayer: 3-1             [-1, 32, 56, 56]          45,440\n",
      "|    |    └─_DenseLayer: 3-2             [-1, 32, 56, 56]          49,600\n",
      "|    |    └─_DenseLayer: 3-3             [-1, 32, 56, 56]          53,760\n",
      "|    |    └─_DenseLayer: 3-4             [-1, 32, 56, 56]          57,920\n",
      "|    |    └─_DenseLayer: 3-5             [-1, 32, 56, 56]          62,080\n",
      "|    |    └─_DenseLayer: 3-6             [-1, 32, 56, 56]          66,240\n",
      "|    └─_Transition: 2-6                  [-1, 128, 28, 28]         --\n",
      "|    |    └─BatchNorm2d: 3-7             [-1, 256, 56, 56]         512\n",
      "|    |    └─ReLU: 3-8                    [-1, 256, 56, 56]         --\n",
      "|    |    └─Conv2d: 3-9                  [-1, 128, 56, 56]         32,768\n",
      "|    |    └─AvgPool2d: 3-10              [-1, 128, 28, 28]         --\n",
      "|    └─_DenseBlock: 2-7                  [-1, 512, 28, 28]         --\n",
      "|    |    └─_DenseLayer: 3-11            [-1, 32, 28, 28]          53,760\n",
      "|    |    └─_DenseLayer: 3-12            [-1, 32, 28, 28]          57,920\n",
      "|    |    └─_DenseLayer: 3-13            [-1, 32, 28, 28]          62,080\n",
      "|    |    └─_DenseLayer: 3-14            [-1, 32, 28, 28]          66,240\n",
      "|    |    └─_DenseLayer: 3-15            [-1, 32, 28, 28]          70,400\n",
      "|    |    └─_DenseLayer: 3-16            [-1, 32, 28, 28]          74,560\n",
      "|    |    └─_DenseLayer: 3-17            [-1, 32, 28, 28]          78,720\n",
      "|    |    └─_DenseLayer: 3-18            [-1, 32, 28, 28]          82,880\n",
      "|    |    └─_DenseLayer: 3-19            [-1, 32, 28, 28]          87,040\n",
      "|    |    └─_DenseLayer: 3-20            [-1, 32, 28, 28]          91,200\n",
      "|    |    └─_DenseLayer: 3-21            [-1, 32, 28, 28]          95,360\n",
      "|    |    └─_DenseLayer: 3-22            [-1, 32, 28, 28]          99,520\n",
      "|    └─_Transition: 2-8                  [-1, 256, 14, 14]         --\n",
      "|    |    └─BatchNorm2d: 3-23            [-1, 512, 28, 28]         1,024\n",
      "|    |    └─ReLU: 3-24                   [-1, 512, 28, 28]         --\n",
      "|    |    └─Conv2d: 3-25                 [-1, 256, 28, 28]         131,072\n",
      "|    |    └─AvgPool2d: 3-26              [-1, 256, 14, 14]         --\n",
      "|    └─_DenseBlock: 2-9                  [-1, 1024, 14, 14]        --\n",
      "|    |    └─_DenseLayer: 3-27            [-1, 32, 14, 14]          70,400\n",
      "|    |    └─_DenseLayer: 3-28            [-1, 32, 14, 14]          74,560\n",
      "|    |    └─_DenseLayer: 3-29            [-1, 32, 14, 14]          78,720\n",
      "|    |    └─_DenseLayer: 3-30            [-1, 32, 14, 14]          82,880\n",
      "|    |    └─_DenseLayer: 3-31            [-1, 32, 14, 14]          87,040\n",
      "|    |    └─_DenseLayer: 3-32            [-1, 32, 14, 14]          91,200\n",
      "|    |    └─_DenseLayer: 3-33            [-1, 32, 14, 14]          95,360\n",
      "|    |    └─_DenseLayer: 3-34            [-1, 32, 14, 14]          99,520\n",
      "|    |    └─_DenseLayer: 3-35            [-1, 32, 14, 14]          103,680\n",
      "|    |    └─_DenseLayer: 3-36            [-1, 32, 14, 14]          107,840\n",
      "|    |    └─_DenseLayer: 3-37            [-1, 32, 14, 14]          112,000\n",
      "|    |    └─_DenseLayer: 3-38            [-1, 32, 14, 14]          116,160\n",
      "|    |    └─_DenseLayer: 3-39            [-1, 32, 14, 14]          120,320\n",
      "|    |    └─_DenseLayer: 3-40            [-1, 32, 14, 14]          124,480\n",
      "|    |    └─_DenseLayer: 3-41            [-1, 32, 14, 14]          128,640\n",
      "|    |    └─_DenseLayer: 3-42            [-1, 32, 14, 14]          132,800\n",
      "|    |    └─_DenseLayer: 3-43            [-1, 32, 14, 14]          136,960\n",
      "|    |    └─_DenseLayer: 3-44            [-1, 32, 14, 14]          141,120\n",
      "|    |    └─_DenseLayer: 3-45            [-1, 32, 14, 14]          145,280\n",
      "|    |    └─_DenseLayer: 3-46            [-1, 32, 14, 14]          149,440\n",
      "|    |    └─_DenseLayer: 3-47            [-1, 32, 14, 14]          153,600\n",
      "|    |    └─_DenseLayer: 3-48            [-1, 32, 14, 14]          157,760\n",
      "|    |    └─_DenseLayer: 3-49            [-1, 32, 14, 14]          161,920\n",
      "|    |    └─_DenseLayer: 3-50            [-1, 32, 14, 14]          166,080\n",
      "|    └─_Transition: 2-10                 [-1, 512, 7, 7]           --\n",
      "|    |    └─BatchNorm2d: 3-51            [-1, 1024, 14, 14]        2,048\n",
      "|    |    └─ReLU: 3-52                   [-1, 1024, 14, 14]        --\n",
      "|    |    └─Conv2d: 3-53                 [-1, 512, 14, 14]         524,288\n",
      "|    |    └─AvgPool2d: 3-54              [-1, 512, 7, 7]           --\n",
      "|    └─_DenseBlock: 2-11                 [-1, 1024, 7, 7]          --\n",
      "|    |    └─_DenseLayer: 3-55            [-1, 32, 7, 7]            103,680\n",
      "|    |    └─_DenseLayer: 3-56            [-1, 32, 7, 7]            107,840\n",
      "|    |    └─_DenseLayer: 3-57            [-1, 32, 7, 7]            112,000\n",
      "|    |    └─_DenseLayer: 3-58            [-1, 32, 7, 7]            116,160\n",
      "|    |    └─_DenseLayer: 3-59            [-1, 32, 7, 7]            120,320\n",
      "|    |    └─_DenseLayer: 3-60            [-1, 32, 7, 7]            124,480\n",
      "|    |    └─_DenseLayer: 3-61            [-1, 32, 7, 7]            128,640\n",
      "|    |    └─_DenseLayer: 3-62            [-1, 32, 7, 7]            132,800\n",
      "|    |    └─_DenseLayer: 3-63            [-1, 32, 7, 7]            136,960\n",
      "|    |    └─_DenseLayer: 3-64            [-1, 32, 7, 7]            141,120\n",
      "|    |    └─_DenseLayer: 3-65            [-1, 32, 7, 7]            145,280\n",
      "|    |    └─_DenseLayer: 3-66            [-1, 32, 7, 7]            149,440\n",
      "|    |    └─_DenseLayer: 3-67            [-1, 32, 7, 7]            153,600\n",
      "|    |    └─_DenseLayer: 3-68            [-1, 32, 7, 7]            157,760\n",
      "|    |    └─_DenseLayer: 3-69            [-1, 32, 7, 7]            161,920\n",
      "|    |    └─_DenseLayer: 3-70            [-1, 32, 7, 7]            166,080\n",
      "|    └─BatchNorm2d: 2-12                 [-1, 1024, 7, 7]          2,048\n",
      "├─Linear: 1-2                            [-1, 1000]                1,025,000\n",
      "==========================================================================================\n",
      "Total params: 7,978,856\n",
      "Trainable params: 7,978,856\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 2.85\n",
      "==========================================================================================\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 172.18\n",
      "Params size (MB): 30.44\n",
      "Estimated Total Size (MB): 203.19\n",
      "==========================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "├─Sequential: 1-1                        [-1, 1024, 7, 7]          --\n",
       "|    └─Conv2d: 2-1                       [-1, 64, 112, 112]        9,408\n",
       "|    └─BatchNorm2d: 2-2                  [-1, 64, 112, 112]        128\n",
       "|    └─ReLU: 2-3                         [-1, 64, 112, 112]        --\n",
       "|    └─MaxPool2d: 2-4                    [-1, 64, 56, 56]          --\n",
       "|    └─_DenseBlock: 2-5                  [-1, 256, 56, 56]         --\n",
       "|    |    └─_DenseLayer: 3-1             [-1, 32, 56, 56]          45,440\n",
       "|    |    └─_DenseLayer: 3-2             [-1, 32, 56, 56]          49,600\n",
       "|    |    └─_DenseLayer: 3-3             [-1, 32, 56, 56]          53,760\n",
       "|    |    └─_DenseLayer: 3-4             [-1, 32, 56, 56]          57,920\n",
       "|    |    └─_DenseLayer: 3-5             [-1, 32, 56, 56]          62,080\n",
       "|    |    └─_DenseLayer: 3-6             [-1, 32, 56, 56]          66,240\n",
       "|    └─_Transition: 2-6                  [-1, 128, 28, 28]         --\n",
       "|    |    └─BatchNorm2d: 3-7             [-1, 256, 56, 56]         512\n",
       "|    |    └─ReLU: 3-8                    [-1, 256, 56, 56]         --\n",
       "|    |    └─Conv2d: 3-9                  [-1, 128, 56, 56]         32,768\n",
       "|    |    └─AvgPool2d: 3-10              [-1, 128, 28, 28]         --\n",
       "|    └─_DenseBlock: 2-7                  [-1, 512, 28, 28]         --\n",
       "|    |    └─_DenseLayer: 3-11            [-1, 32, 28, 28]          53,760\n",
       "|    |    └─_DenseLayer: 3-12            [-1, 32, 28, 28]          57,920\n",
       "|    |    └─_DenseLayer: 3-13            [-1, 32, 28, 28]          62,080\n",
       "|    |    └─_DenseLayer: 3-14            [-1, 32, 28, 28]          66,240\n",
       "|    |    └─_DenseLayer: 3-15            [-1, 32, 28, 28]          70,400\n",
       "|    |    └─_DenseLayer: 3-16            [-1, 32, 28, 28]          74,560\n",
       "|    |    └─_DenseLayer: 3-17            [-1, 32, 28, 28]          78,720\n",
       "|    |    └─_DenseLayer: 3-18            [-1, 32, 28, 28]          82,880\n",
       "|    |    └─_DenseLayer: 3-19            [-1, 32, 28, 28]          87,040\n",
       "|    |    └─_DenseLayer: 3-20            [-1, 32, 28, 28]          91,200\n",
       "|    |    └─_DenseLayer: 3-21            [-1, 32, 28, 28]          95,360\n",
       "|    |    └─_DenseLayer: 3-22            [-1, 32, 28, 28]          99,520\n",
       "|    └─_Transition: 2-8                  [-1, 256, 14, 14]         --\n",
       "|    |    └─BatchNorm2d: 3-23            [-1, 512, 28, 28]         1,024\n",
       "|    |    └─ReLU: 3-24                   [-1, 512, 28, 28]         --\n",
       "|    |    └─Conv2d: 3-25                 [-1, 256, 28, 28]         131,072\n",
       "|    |    └─AvgPool2d: 3-26              [-1, 256, 14, 14]         --\n",
       "|    └─_DenseBlock: 2-9                  [-1, 1024, 14, 14]        --\n",
       "|    |    └─_DenseLayer: 3-27            [-1, 32, 14, 14]          70,400\n",
       "|    |    └─_DenseLayer: 3-28            [-1, 32, 14, 14]          74,560\n",
       "|    |    └─_DenseLayer: 3-29            [-1, 32, 14, 14]          78,720\n",
       "|    |    └─_DenseLayer: 3-30            [-1, 32, 14, 14]          82,880\n",
       "|    |    └─_DenseLayer: 3-31            [-1, 32, 14, 14]          87,040\n",
       "|    |    └─_DenseLayer: 3-32            [-1, 32, 14, 14]          91,200\n",
       "|    |    └─_DenseLayer: 3-33            [-1, 32, 14, 14]          95,360\n",
       "|    |    └─_DenseLayer: 3-34            [-1, 32, 14, 14]          99,520\n",
       "|    |    └─_DenseLayer: 3-35            [-1, 32, 14, 14]          103,680\n",
       "|    |    └─_DenseLayer: 3-36            [-1, 32, 14, 14]          107,840\n",
       "|    |    └─_DenseLayer: 3-37            [-1, 32, 14, 14]          112,000\n",
       "|    |    └─_DenseLayer: 3-38            [-1, 32, 14, 14]          116,160\n",
       "|    |    └─_DenseLayer: 3-39            [-1, 32, 14, 14]          120,320\n",
       "|    |    └─_DenseLayer: 3-40            [-1, 32, 14, 14]          124,480\n",
       "|    |    └─_DenseLayer: 3-41            [-1, 32, 14, 14]          128,640\n",
       "|    |    └─_DenseLayer: 3-42            [-1, 32, 14, 14]          132,800\n",
       "|    |    └─_DenseLayer: 3-43            [-1, 32, 14, 14]          136,960\n",
       "|    |    └─_DenseLayer: 3-44            [-1, 32, 14, 14]          141,120\n",
       "|    |    └─_DenseLayer: 3-45            [-1, 32, 14, 14]          145,280\n",
       "|    |    └─_DenseLayer: 3-46            [-1, 32, 14, 14]          149,440\n",
       "|    |    └─_DenseLayer: 3-47            [-1, 32, 14, 14]          153,600\n",
       "|    |    └─_DenseLayer: 3-48            [-1, 32, 14, 14]          157,760\n",
       "|    |    └─_DenseLayer: 3-49            [-1, 32, 14, 14]          161,920\n",
       "|    |    └─_DenseLayer: 3-50            [-1, 32, 14, 14]          166,080\n",
       "|    └─_Transition: 2-10                 [-1, 512, 7, 7]           --\n",
       "|    |    └─BatchNorm2d: 3-51            [-1, 1024, 14, 14]        2,048\n",
       "|    |    └─ReLU: 3-52                   [-1, 1024, 14, 14]        --\n",
       "|    |    └─Conv2d: 3-53                 [-1, 512, 14, 14]         524,288\n",
       "|    |    └─AvgPool2d: 3-54              [-1, 512, 7, 7]           --\n",
       "|    └─_DenseBlock: 2-11                 [-1, 1024, 7, 7]          --\n",
       "|    |    └─_DenseLayer: 3-55            [-1, 32, 7, 7]            103,680\n",
       "|    |    └─_DenseLayer: 3-56            [-1, 32, 7, 7]            107,840\n",
       "|    |    └─_DenseLayer: 3-57            [-1, 32, 7, 7]            112,000\n",
       "|    |    └─_DenseLayer: 3-58            [-1, 32, 7, 7]            116,160\n",
       "|    |    └─_DenseLayer: 3-59            [-1, 32, 7, 7]            120,320\n",
       "|    |    └─_DenseLayer: 3-60            [-1, 32, 7, 7]            124,480\n",
       "|    |    └─_DenseLayer: 3-61            [-1, 32, 7, 7]            128,640\n",
       "|    |    └─_DenseLayer: 3-62            [-1, 32, 7, 7]            132,800\n",
       "|    |    └─_DenseLayer: 3-63            [-1, 32, 7, 7]            136,960\n",
       "|    |    └─_DenseLayer: 3-64            [-1, 32, 7, 7]            141,120\n",
       "|    |    └─_DenseLayer: 3-65            [-1, 32, 7, 7]            145,280\n",
       "|    |    └─_DenseLayer: 3-66            [-1, 32, 7, 7]            149,440\n",
       "|    |    └─_DenseLayer: 3-67            [-1, 32, 7, 7]            153,600\n",
       "|    |    └─_DenseLayer: 3-68            [-1, 32, 7, 7]            157,760\n",
       "|    |    └─_DenseLayer: 3-69            [-1, 32, 7, 7]            161,920\n",
       "|    |    └─_DenseLayer: 3-70            [-1, 32, 7, 7]            166,080\n",
       "|    └─BatchNorm2d: 2-12                 [-1, 1024, 7, 7]          2,048\n",
       "├─Linear: 1-2                            [-1, 1000]                1,025,000\n",
       "==========================================================================================\n",
       "Total params: 7,978,856\n",
       "Trainable params: 7,978,856\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 2.85\n",
       "==========================================================================================\n",
       "Input size (MB): 0.57\n",
       "Forward/backward pass size (MB): 172.18\n",
       "Params size (MB): 30.44\n",
       "Estimated Total Size (MB): 203.19\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    push_color_trunk.cuda()\n",
    "\n",
    "summary(push_color_trunk, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseNet(\n",
      "  (features): Sequential(\n",
      "    (conv0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (norm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu0): ReLU(inplace=True)\n",
      "    (pool0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (denseblock1): _DenseBlock(\n",
      "      (denselayer1): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer2): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer3): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer4): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer5): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer6): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (transition1): _Transition(\n",
      "      (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    )\n",
      "    (denseblock2): _DenseBlock(\n",
      "      (denselayer1): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer2): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer3): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer4): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer5): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer6): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer7): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer8): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer9): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer10): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer11): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer12): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (transition2): _Transition(\n",
      "      (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    )\n",
      "    (denseblock3): _DenseBlock(\n",
      "      (denselayer1): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer2): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer3): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer4): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer5): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer6): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer7): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer8): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer9): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer10): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer11): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer12): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer13): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer14): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer15): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer16): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer17): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer18): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer19): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer20): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer21): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer22): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer23): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer24): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (transition3): _Transition(\n",
      "      (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    )\n",
      "    (denseblock4): _DenseBlock(\n",
      "      (denselayer1): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer2): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer3): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer4): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer5): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer6): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer7): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer8): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer9): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer10): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer11): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer12): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer13): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer14): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer15): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer16): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (norm5): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (classifier): Linear(in_features=1024, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(push_color_trunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.7.1'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vft2",
   "language": "python",
   "name": "vft2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5 (default, Sep  4 2020, 07:30:14) \n[GCC 7.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8654fed7540277ddf6b9b5ab1dc4e73f02ee108c349421d9d519d216622133b6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
