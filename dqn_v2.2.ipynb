{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pybullet build time: Oct 14 2022 01:09:34\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import imp\n",
    "import math\n",
    "import gc\n",
    "import os\n",
    "from sre_constants import SUCCESS\n",
    "import time\n",
    "import datetime\n",
    "import pybullet as p\n",
    "import cv2\n",
    "import numpy as np\n",
    "from graphviz import Digraph\n",
    "import argparse\n",
    "import random\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from time import sleep\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "from Config.constants import (\n",
    "    GRIPPER_PUSH_RADIUS,\n",
    "    PIXEL_SIZE,\n",
    "    PUSH_DISTANCE,\n",
    "    WORKSPACE_LIMITS,\n",
    "    TARGET_LOWER,\n",
    "    TARGET_UPPER,\n",
    "    orange_lower,\n",
    "    orange_upper,\n",
    "    BG_THRESHOLD,\n",
    "    MIN_GRASP_THRESHOLDS\n",
    ")\n",
    "\n",
    "from Environments.environment_sim import Environment\n",
    "import Environments.utils as env_utils\n",
    "from V1_destination_prediction.Test_cases.tc1 import TestCase1\n",
    "\n",
    "from create_env import get_push_start, get_max_extent_of_target_from_bottom\n",
    "\n",
    "\n",
    "from collections import namedtuple, deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity) -> None:\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \n",
    "        '''Save a transition'''\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from V2_next_best_action.models.dqn_v2 import pushDQN2\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─Linear: 1-1                            [-1, 3, 128]              896\n",
      "├─Linear: 1-2                            [-1, 3, 128]              16,512\n",
      "├─Linear: 1-3                            [-1, 3, 16]               2,064\n",
      "==========================================================================================\n",
      "Total params: 19,472\n",
      "Trainable params: 19,472\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 0.02\n",
      "==========================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 0.07\n",
      "Estimated Total Size (MB): 0.08\n",
      "==========================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "├─Linear: 1-1                            [-1, 3, 128]              896\n",
       "├─Linear: 1-2                            [-1, 3, 128]              16,512\n",
       "├─Linear: 1-3                            [-1, 3, 16]               2,064\n",
       "==========================================================================================\n",
       "Total params: 19,472\n",
       "Trainable params: 19,472\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.02\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.01\n",
       "Params size (MB): 0.07\n",
       "Estimated Total Size (MB): 0.08\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model = pushDQN2(n_observations=6, n_actions=16, use_cuda=True)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "# model(np.ones(shape=(1, 224, 224, 3)), np.ones(shape=(1, 224, 224, 3)))\n",
    "# print(model(torch.rand((3, 6)).to(device=device)))\n",
    "summary(model, (3, 6))\n",
    "# del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0417,  0.0830, -0.1164, -0.1587,  0.0085, -0.1066, -0.0746,  0.0431,\n",
      "         -0.0303,  0.0228,  0.0194,  0.1637,  0.1193,  0.0165,  0.0143, -0.0143]],\n",
      "       device='cuda:0')\n",
      "tensor([[0.1637]], device='cuda:0')\n",
      "tensor([[11]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    qs = model(torch.rand((1, 6)).to(device=device))\n",
    "    print(qs)\n",
    "\n",
    "    print(qs.max(1)[0].view(1, 1))\n",
    "    print(qs.max(1)[1].view(1, 1))\n",
    "    # del model\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 1 # 0.999 # Discount factor\n",
    "EPS_START = 0.9 # Random action choosing probability starts with this value and decays until EPS_END\n",
    "EPS_END = 0.05 # Random action choosing probability starts at EPS_START and decays until EPS_END\n",
    "EPS_DECAY = 200 # Decay rate of random action choosing probability, with the passage of episodes and time\n",
    "TARGET_UPDATE = 10\n",
    "TARGET_SAVE_CHECKPOINTS = [200, 500, 1000, 2000, 3000, 4000, 5000]\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_observations = 6 # 3 for initial state, 3 for goal state\n",
    "n_actions = 16 # 16 push + 1 grasp\n",
    "\n",
    "policy_net = pushDQN2(n_observations, n_actions, use_cuda=True).to(device)\n",
    "target_net = pushDQN2(n_observations, n_actions, use_cuda=True).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.RMSprop(policy_net.parameters())\n",
    "memory = ReplayMemory(10000) # 10000\n",
    "\n",
    "steps_done = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    '''Select the next best action \n",
    "    state: tensor(shape=(6))\n",
    "    '''\n",
    "    global steps_done\n",
    "    sample = random.uniform(0.0, 1.0) # random.randint(a=0, b=16) \n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1.0*steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "\n",
    "    if sample>eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"REWARD SPECIFICATION\n",
    "\n",
    "1. If action=grasp:\n",
    "        if prev_max_extents>THRESHOLD: # Max extents before grasping\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1 \n",
    "2. If action=push:\n",
    "        if cur_max_entents>THRESHOLD: # Max extents after pushing\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "Bellman equation:\n",
    "    Q(s, a) = r + gamma*max(Q(s', a'))\n",
    "\"\"\"\n",
    "\n",
    "def get_reward(prev_state, current_state):\n",
    "    '''\n",
    "    prev_state: (x1, y1, theta1, x2, y2, theta2)\n",
    "    current_state: (x3, y3, theta3, _, _, _)\n",
    "    '''\n",
    "    reward = np.linalg.norm(current_state[0:3] - prev_state[3:6])\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(timestep=0, batch_num=0):\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    print(\"Optimization!\")\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startThreads creating 1 threads.\n",
      "starting thread 0\n",
      "started thread 0 \n",
      "argc=2\n",
      "argv[0] = --unused\n",
      "argv[1] = --start_demo_name=Physics Server\n",
      "ExampleBrowserThreadFunc started\n",
      "X11 functions dynamically loaded using dlopen/dlsym OK!\n",
      "X11 functions dynamically loaded using dlopen/dlsym OK!\n",
      "Creating context\n",
      "Created GL 3.3 context\n",
      "Direct GLX rendering context obtained\n",
      "Making context current\n",
      "GL_VENDOR=NVIDIA Corporation\n",
      "GL_RENDERER=NVIDIA GeForce GTX 1050 Ti/PCIe/SSE2\n",
      "GL_VERSION=3.3.0 NVIDIA 510.85.02\n",
      "GL_SHADING_LANGUAGE_VERSION=3.30 NVIDIA via Cg compiler\n",
      "pthread_getconcurrency()=0\n",
      "Version = 3.3.0 NVIDIA 510.85.02\n",
      "Vendor = NVIDIA Corporation\n",
      "Renderer = NVIDIA GeForce GTX 1050 Ti/PCIe/SSE2\n",
      "b3Printf: Selected demo: Physics Server\n",
      "startThreads creating 1 threads.\n",
      "starting thread 0\n",
      "started thread 0 \n",
      "MotionThreadFunc thread started\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Timestep: 1\tReward: 4.570579464246407\n",
      "Timestep: 2\tReward: 2.9358213690629333\n",
      "Timestep: 3\tReward: 3.841639674452167\n",
      "Timestep: 4\tReward: 6.107864383856376\n",
      "Timestep: 5\tReward: 6.368973235289625\n",
      "Timestep: 6\tReward: 5.5423860091433514\n",
      "Timestep: 7\tReward: 6.072178122356124\n",
      "Timestep: 8\tReward: 6.014375081391229\n",
      "Timestep: 9\tReward: 5.116589630843091\n",
      "Timestep: 10\tReward: 5.506430453802939\n",
      "Target updated\n",
      "Timestep: 11\tReward: 1.3851639666182427\n",
      "Timestep: 12\tReward: 5.5281661017210615\n",
      "Timestep: 13\tReward: 4.687001010858637\n",
      "Timestep: 14\tReward: 6.823055480417773\n",
      "Timestep: 15\tReward: 5.548073524253257\n",
      "Timestep: 16\tReward: 6.028161816588552\n",
      "Timestep: 17\tReward: 4.88693618435255\n",
      "Timestep: 18\tReward: 5.253101626278493\n",
      "Timestep: 19\tReward: 2.6214454293650156\n",
      "Timestep: 20\tReward: 6.160928539684486\n",
      "Target updated\n",
      "Timestep: 21\tReward: 4.46958990054587\n",
      "Timestep: 22\tReward: 5.9486801673926895\n",
      "Timestep: 23\tReward: 4.199730681529599\n",
      "Timestep: 24\tReward: 5.041707224739432\n",
      "Timestep: 25\tReward: 7.797599943722918\n",
      "Timestep: 26\tReward: 2.8882188764852548\n",
      "Timestep: 27\tReward: 4.335487543482099\n",
      "Timestep: 28\tReward: 2.7849907140326473\n",
      "Timestep: 29\tReward: 4.086184592638975\n",
      "Timestep: 30\tReward: 3.427284061081183\n",
      "Target updated\n",
      "Timestep: 31\tReward: 6.051130989402323\n",
      "Timestep: 32\tReward: 5.076426586556399\n",
      "Timestep: 33\tReward: 6.272670291429828\n",
      "Timestep: 34\tReward: 4.1578352291898195\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Timestep: 35\tReward: 7.645130310202389\n",
      "Timestep: 36\tReward: 3.6724496295353024\n",
      "Timestep: 37\tReward: 5.733021049433505\n",
      "Timestep: 38\tReward: 7.757286591514619\n",
      "Timestep: 39\tReward: 7.459949026259293\n",
      "Timestep: 40\tReward: 4.821875828595564\n",
      "Target updated\n",
      "Timestep: 41\tReward: 6.040339074960434\n",
      "Timestep: 42\tReward: 7.726964748848448\n",
      "Timestep: 43\tReward: 5.042126864251428\n",
      "Timestep: 44\tReward: 5.601417460256272\n",
      "Timestep: 45\tReward: 5.90133309233098\n",
      "Timestep: 46\tReward: 5.712586076462529\n",
      "Timestep: 47\tReward: 4.789329123134506\n",
      "Timestep: 48\tReward: 6.4858605673859016\n",
      "Timestep: 49\tReward: 3.901799546407931\n",
      "Timestep: 50\tReward: 7.383376378925133\n",
      "Target updated\n",
      "Timestep: 51\tReward: 3.704401305633403\n",
      "Timestep: 52\tReward: 3.850735739682379\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Timestep: 53\tReward: 6.623087258649455\n",
      "Timestep: 54\tReward: 5.199689976194914\n",
      "Timestep: 55\tReward: 3.716696263134919\n",
      "Timestep: 56\tReward: 1.8677884239564533\n",
      "Timestep: 57\tReward: 1.2315361382864927\n",
      "Timestep: 58\tReward: 5.033446646079501\n",
      "Timestep: 59\tReward: 3.6897914923858157\n",
      "Timestep: 60\tReward: 6.476678489925664\n",
      "Target updated\n",
      "Timestep: 61\tReward: 5.18009510769905\n",
      "Timestep: 62\tReward: 7.4339990681030645\n",
      "Timestep: 63\tReward: 3.6667009069814567\n",
      "Timestep: 64\tReward: 8.305802163823692\n",
      "Timestep: 65\tReward: 1.8040866825794057\n",
      "Timestep: 66\tReward: 4.945092034729255\n",
      "Timestep: 67\tReward: 7.451734408942633\n",
      "Timestep: 68\tReward: 7.351140864903367\n",
      "Timestep: 69\tReward: 2.2476316581169167\n",
      "Timestep: 70\tReward: 6.583698304774479\n",
      "Target updated\n",
      "Timestep: 71\tReward: 8.528754370990441\n",
      "Timestep: 72\tReward: 2.5185389331606545\n",
      "Timestep: 73\tReward: 3.64094263558886\n",
      "Timestep: 74\tReward: 4.048484764453973\n",
      "Timestep: 75\tReward: 5.715927043979997\n",
      "Timestep: 76\tReward: 4.707123513359636\n",
      "Timestep: 77\tReward: 6.6268603294915955\n",
      "Timestep: 78\tReward: 5.1894581272664535\n",
      "Timestep: 79\tReward: 4.042034976020977\n",
      "Timestep: 80\tReward: 3.3995432369961005\n",
      "Target updated\n",
      "Timestep: 81\tReward: 6.326658295889199\n",
      "Timestep: 82\tReward: 4.1627547031119985\n",
      "Timestep: 83\tReward: 7.06206091603054\n",
      "Timestep: 84\tReward: 6.718573268650483\n",
      "Timestep: 85\tReward: 3.447506482949397\n",
      "Timestep: 86\tReward: 8.241917144498238\n",
      "Timestep: 87\tReward: 4.105271854937569\n",
      "Timestep: 88\tReward: 6.254605588776187\n",
      "Timestep: 89\tReward: 7.288448589297862\n",
      "Timestep: 90\tReward: 6.493082872963183\n",
      "Target updated\n",
      "Timestep: 91\tReward: 3.6906143608461535\n",
      "Timestep: 92\tReward: 4.079953521686268\n",
      "Timestep: 93\tReward: 4.6944787874071325\n",
      "Timestep: 94\tReward: 2.869190506173856\n",
      "Timestep: 95\tReward: 5.528200545852353\n",
      "Timestep: 96\tReward: 6.8672214912471645\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Timestep: 97\tReward: 4.160425250370453\n",
      "Timestep: 98\tReward: 5.753510208298322\n",
      "Timestep: 99\tReward: 1.7078291982380802\n",
      "Timestep: 100\tReward: 4.233270983547107\n",
      "Target updated\n",
      "Timestep: 101\tReward: 8.726971964047664\n",
      "Timestep: 102\tReward: 6.412866832859337\n",
      "Timestep: 103\tReward: 6.4169979417387015\n",
      "Timestep: 104\tReward: 5.108477125888937\n",
      "Timestep: 105\tReward: 7.345392063980415\n",
      "Timestep: 106\tReward: 3.9058331617482915\n",
      "Timestep: 107\tReward: 5.322632167754478\n",
      "Timestep: 108\tReward: 6.389308128387557\n",
      "Timestep: 109\tReward: 6.7226629060807\n",
      "Timestep: 110\tReward: 2.394403192673733\n",
      "Target updated\n",
      "Timestep: 111\tReward: 7.080937089994782\n",
      "Timestep: 112\tReward: 7.819290567232242\n",
      "Timestep: 113\tReward: 5.071329699071\n",
      "Timestep: 114\tReward: 3.657025754283222\n",
      "Timestep: 115\tReward: 4.503300283665119\n",
      "Timestep: 116\tReward: 7.256301056253434\n",
      "Timestep: 117\tReward: 4.803503468512177\n",
      "Timestep: 118\tReward: 9.203615479933664\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Timestep: 119\tReward: 4.381953334289019\n",
      "Timestep: 120\tReward: 6.3016557597313385\n",
      "Target updated\n",
      "Timestep: 121\tReward: 3.7111464655282624\n",
      "Timestep: 122\tReward: 4.6114876688161175\n",
      "Timestep: 123\tReward: 7.283965033718731\n",
      "Timestep: 124\tReward: 3.825631801818022\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Timestep: 125\tReward: 5.718716359959067\n",
      "Timestep: 126\tReward: 5.405920895480409\n",
      "Timestep: 127\tReward: 5.51788439578043\n",
      "Timestep: 128\tReward: 4.593730905411892\n",
      "Optimization!\n",
      "Timestep: 129\tReward: 5.2264192905942375\n",
      "Optimization!\n",
      "Timestep: 130\tReward: 4.911674924467508\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 131\tReward: 5.940517104642531\n",
      "Optimization!\n",
      "Timestep: 132\tReward: 5.021433854682076\n",
      "Optimization!\n",
      "Timestep: 133\tReward: 4.388817352226894\n",
      "Optimization!\n",
      "Timestep: 134\tReward: 4.486011007790268\n",
      "Optimization!\n",
      "Timestep: 135\tReward: 2.8858075858681613\n",
      "Optimization!\n",
      "Timestep: 136\tReward: 1.737726379297031\n",
      "Optimization!\n",
      "Timestep: 137\tReward: 4.741504983900368\n",
      "Optimization!\n",
      "Timestep: 138\tReward: 4.8914328175903155\n",
      "Optimization!\n",
      "Timestep: 139\tReward: 4.751625503739694\n",
      "Optimization!\n",
      "Timestep: 140\tReward: 6.773229520537825\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 141\tReward: 5.623457731198185\n",
      "Optimization!\n",
      "Timestep: 142\tReward: 2.054245272994686\n",
      "Optimization!\n",
      "Timestep: 143\tReward: 6.494133625157889\n",
      "Optimization!\n",
      "Timestep: 144\tReward: 2.192818715825786\n",
      "Optimization!\n",
      "Timestep: 145\tReward: 6.542020431614813\n",
      "Optimization!\n",
      "Timestep: 146\tReward: 5.9248833920712185\n",
      "Optimization!\n",
      "Timestep: 147\tReward: 3.215395120932538\n",
      "Optimization!\n",
      "Timestep: 148\tReward: 7.418667152251758\n",
      "Optimization!\n",
      "Timestep: 149\tReward: 3.783122351301787\n",
      "Optimization!\n",
      "Timestep: 150\tReward: 5.486570069781058\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 151\tReward: 6.592674486884303\n",
      "Optimization!\n",
      "Timestep: 152\tReward: 4.3098468019104\n",
      "Optimization!\n",
      "Timestep: 153\tReward: 5.953723006548403\n",
      "Optimization!\n",
      "Timestep: 154\tReward: 5.471603464588957\n",
      "Optimization!\n",
      "Timestep: 155\tReward: 6.420755564713372\n",
      "Optimization!\n",
      "Timestep: 156\tReward: 6.7155155102778155\n",
      "Optimization!\n",
      "Timestep: 157\tReward: 7.593265727809716\n",
      "Optimization!\n",
      "Timestep: 158\tReward: 4.354882733052371\n",
      "Optimization!\n",
      "Timestep: 159\tReward: 4.089332638878615\n",
      "Optimization!\n",
      "Timestep: 160\tReward: 6.760903095201855\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 161\tReward: 5.674665313359741\n",
      "Optimization!\n",
      "Timestep: 162\tReward: 5.5816995446977735\n",
      "Optimization!\n",
      "Timestep: 163\tReward: 8.236175530801756\n",
      "Optimization!\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Timestep: 164\tReward: 7.3323408973409645\n",
      "Optimization!\n",
      "Timestep: 165\tReward: 5.513991428635584\n",
      "Optimization!\n",
      "Timestep: 166\tReward: 6.11543982175734\n",
      "Optimization!\n",
      "Timestep: 167\tReward: 5.294265172435076\n",
      "Optimization!\n",
      "Timestep: 168\tReward: 7.087751549853929\n",
      "Optimization!\n",
      "Timestep: 169\tReward: 2.6365167102816907\n",
      "Optimization!\n",
      "Timestep: 170\tReward: 5.5490484913288665\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 171\tReward: 4.105039980362384\n",
      "Optimization!\n",
      "Timestep: 172\tReward: 5.583224933766174\n",
      "Optimization!\n",
      "Timestep: 173\tReward: 4.149420608951333\n",
      "Optimization!\n",
      "Timestep: 174\tReward: 5.044682652798166\n",
      "Optimization!\n",
      "Timestep: 175\tReward: 5.893963585863835\n",
      "Optimization!\n",
      "Timestep: 176\tReward: 5.264215618230261\n",
      "Optimization!\n",
      "Timestep: 177\tReward: 4.404256842369104\n",
      "Optimization!\n",
      "Timestep: 178\tReward: 5.510894796668764\n",
      "Optimization!\n",
      "Timestep: 179\tReward: 1.8223987131257982\n",
      "Optimization!\n",
      "Timestep: 180\tReward: 6.74451528382903\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 181\tReward: 6.396158963517365\n",
      "Optimization!\n",
      "Timestep: 182\tReward: 6.802811825635081\n",
      "Optimization!\n",
      "Timestep: 183\tReward: 3.97322307598246\n",
      "Optimization!\n",
      "Timestep: 184\tReward: 6.870629819941896\n",
      "Optimization!\n",
      "Timestep: 185\tReward: 3.85777291051425\n",
      "Optimization!\n",
      "Timestep: 186\tReward: 5.642198627336667\n",
      "Optimization!\n",
      "Timestep: 187\tReward: 7.740752289532987\n",
      "Optimization!\n",
      "Timestep: 188\tReward: 4.88334912636226\n",
      "Optimization!\n",
      "Timestep: 189\tReward: 6.7296338401331735\n",
      "Optimization!\n",
      "Timestep: 190\tReward: 4.514176974730804\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 191\tReward: 5.645566473187381\n",
      "Optimization!\n",
      "Timestep: 192\tReward: 5.921599823155229\n",
      "Optimization!\n",
      "Timestep: 193\tReward: 6.2144538067305595\n",
      "Optimization!\n",
      "Timestep: 194\tReward: 4.29645634453396\n",
      "Optimization!\n",
      "Timestep: 195\tReward: 5.211541138170903\n",
      "Optimization!\n",
      "Timestep: 196\tReward: 4.761940498047094\n",
      "Optimization!\n",
      "Timestep: 197\tReward: 4.583714255631139\n",
      "Optimization!\n",
      "Timestep: 198\tReward: 7.134548588179796\n",
      "Optimization!\n",
      "Timestep: 199\tReward: 4.876295426152708\n",
      "Optimization!\n",
      "Timestep: 200\tReward: 5.572227412208681\n",
      "Optimization!\n",
      "Target updated\n",
      "Saved\n",
      "Timestep: 201\tReward: 2.9243835487961745\n",
      "Optimization!\n",
      "Timestep: 202\tReward: 5.6823262084439286\n",
      "Optimization!\n",
      "Timestep: 203\tReward: 5.630312599833188\n",
      "Optimization!\n",
      "Timestep: 204\tReward: 4.950399191631055\n",
      "Optimization!\n",
      "Timestep: 205\tReward: 5.531350879863454\n",
      "Optimization!\n",
      "Timestep: 206\tReward: 4.407764746921748\n",
      "Optimization!\n",
      "Timestep: 207\tReward: 6.63289934287057\n",
      "Optimization!\n",
      "Timestep: 208\tReward: 6.870315890876162\n",
      "Optimization!\n",
      "Timestep: 209\tReward: 4.740101443524036\n",
      "Optimization!\n",
      "Timestep: 210\tReward: 5.530306967781456\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 211\tReward: 4.245211326806521\n",
      "Optimization!\n",
      "Timestep: 212\tReward: 2.0462741915991303\n",
      "Optimization!\n",
      "Timestep: 213\tReward: 5.935017526664352\n",
      "Optimization!\n",
      "Timestep: 214\tReward: 9.488226570230314\n",
      "Optimization!\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Timestep: 215\tReward: 6.168033248504916\n",
      "Optimization!\n",
      "Timestep: 216\tReward: 8.125110996332713\n",
      "Optimization!\n",
      "Timestep: 217\tReward: 5.861887360366402\n",
      "Optimization!\n",
      "Timestep: 218\tReward: 6.833196158219749\n",
      "Optimization!\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Timestep: 219\tReward: 1.8774578523274508\n",
      "Optimization!\n",
      "Timestep: 220\tReward: 2.5789477203504623\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 221\tReward: 4.895309995818806\n",
      "Optimization!\n",
      "Timestep: 222\tReward: 4.967084329885212\n",
      "Optimization!\n",
      "Timestep: 223\tReward: 1.4034456065675065\n",
      "Optimization!\n",
      "Timestep: 224\tReward: 6.612943097795387\n",
      "Optimization!\n",
      "Timestep: 225\tReward: 2.573152455154186\n",
      "Optimization!\n",
      "Timestep: 226\tReward: 4.1727966751805745\n",
      "Optimization!\n",
      "Timestep: 227\tReward: 7.411100723443495\n",
      "Optimization!\n",
      "Timestep: 228\tReward: 5.392064074826926\n",
      "Optimization!\n",
      "Timestep: 229\tReward: 7.257897832343621\n",
      "Optimization!\n",
      "Timestep: 230\tReward: 6.8380773299713935\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 231\tReward: 5.93180129627129\n",
      "Optimization!\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Timestep: 232\tReward: 4.397192184462086\n",
      "Optimization!\n",
      "Timestep: 233\tReward: 2.9058199113120646\n",
      "Optimization!\n",
      "Timestep: 234\tReward: 6.384388968833384\n",
      "Optimization!\n",
      "Timestep: 235\tReward: 2.4353047026800416\n",
      "Optimization!\n",
      "Timestep: 236\tReward: 5.926657365154235\n",
      "Optimization!\n",
      "Timestep: 237\tReward: 2.7054072706193284\n",
      "Optimization!\n",
      "Timestep: 238\tReward: 4.623782789847715\n",
      "Optimization!\n",
      "Timestep: 239\tReward: 5.400406821549801\n",
      "Optimization!\n",
      "Timestep: 240\tReward: 5.634003490060072\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 241\tReward: 5.131803878763182\n",
      "Optimization!\n",
      "Timestep: 242\tReward: 4.462103372154841\n",
      "Optimization!\n",
      "Timestep: 243\tReward: 7.141274595742407\n",
      "Optimization!\n",
      "Timestep: 244\tReward: 6.307146267296768\n",
      "Optimization!\n",
      "Timestep: 245\tReward: 4.227446227698117\n",
      "Optimization!\n",
      "Timestep: 246\tReward: 3.7500508071217364\n",
      "Optimization!\n",
      "Timestep: 247\tReward: 7.6185396027288546\n",
      "Optimization!\n",
      "Timestep: 248\tReward: 5.698337931361625\n",
      "Optimization!\n",
      "Timestep: 249\tReward: 4.71982431520369\n",
      "Optimization!\n",
      "Timestep: 250\tReward: 3.7664847540004027\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 251\tReward: 2.223499386911738\n",
      "Optimization!\n",
      "Timestep: 252\tReward: 2.3729928493056267\n",
      "Optimization!\n",
      "Timestep: 253\tReward: 6.586219887227514\n",
      "Optimization!\n",
      "Timestep: 254\tReward: 4.80058499062276\n",
      "Optimization!\n",
      "Timestep: 255\tReward: 4.802398518559742\n",
      "Optimization!\n",
      "Timestep: 256\tReward: 5.433643938056285\n",
      "Optimization!\n",
      "Timestep: 257\tReward: 4.865103589840021\n",
      "Optimization!\n",
      "Timestep: 258\tReward: 3.624463697291264\n",
      "Optimization!\n",
      "Timestep: 259\tReward: 4.062936809765047\n",
      "Optimization!\n",
      "Timestep: 260\tReward: 5.960119029149669\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 261\tReward: 7.40675002797411\n",
      "Optimization!\n",
      "Timestep: 262\tReward: 6.596574096557766\n",
      "Optimization!\n",
      "Timestep: 263\tReward: 5.532606366518625\n",
      "Optimization!\n",
      "Timestep: 264\tReward: 3.2156785776782177\n",
      "Optimization!\n",
      "Timestep: 265\tReward: 6.441950067098036\n",
      "Optimization!\n",
      "Timestep: 266\tReward: 4.160534571178494\n",
      "Optimization!\n",
      "Timestep: 267\tReward: 4.613028568924129\n",
      "Optimization!\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Timestep: 268\tReward: 7.280840159661188\n",
      "Optimization!\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Timestep: 269\tReward: 6.738837469800427\n",
      "Optimization!\n",
      "Timestep: 270\tReward: 3.9923932328995044\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 271\tReward: 4.97751248546888\n",
      "Optimization!\n",
      "Timestep: 272\tReward: 7.154219048206979\n",
      "Optimization!\n",
      "Timestep: 273\tReward: 6.854586523533365\n",
      "Optimization!\n",
      "Timestep: 274\tReward: 6.8721340970912665\n",
      "Optimization!\n",
      "Timestep: 275\tReward: 6.747827515211459\n",
      "Optimization!\n",
      "Timestep: 276\tReward: 7.37184999977209\n",
      "Optimization!\n",
      "Timestep: 277\tReward: 6.290287427801837\n",
      "Optimization!\n",
      "Timestep: 278\tReward: 7.146938188704833\n",
      "Optimization!\n",
      "Timestep: 279\tReward: 6.261715115676019\n",
      "Optimization!\n",
      "Timestep: 280\tReward: 5.95323539240757\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 281\tReward: 5.381633071171082\n",
      "Optimization!\n",
      "Timestep: 282\tReward: 3.9176021503130065\n",
      "Optimization!\n",
      "Timestep: 283\tReward: 5.829706745404726\n",
      "Optimization!\n",
      "Timestep: 284\tReward: 3.9664199416806323\n",
      "Optimization!\n",
      "Timestep: 285\tReward: 5.707172209787004\n",
      "Optimization!\n",
      "Timestep: 286\tReward: 5.433762051678743\n",
      "Optimization!\n",
      "Timestep: 287\tReward: 6.8203940417661215\n",
      "Optimization!\n",
      "Timestep: 288\tReward: 7.5945228179413355\n",
      "Optimization!\n",
      "Timestep: 289\tReward: 4.774494812169842\n",
      "Optimization!\n",
      "Timestep: 290\tReward: 4.1881253959927625\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 291\tReward: 5.125682990139048\n",
      "Optimization!\n",
      "Timestep: 292\tReward: 5.484366595235066\n",
      "Optimization!\n",
      "Timestep: 293\tReward: 4.424842306702798\n",
      "Optimization!\n",
      "Timestep: 294\tReward: 6.5128918868342\n",
      "Optimization!\n",
      "Timestep: 295\tReward: 6.972268465287979\n",
      "Optimization!\n",
      "Timestep: 296\tReward: 4.933747878851636\n",
      "Optimization!\n",
      "Timestep: 297\tReward: 4.158588500342872\n",
      "Optimization!\n",
      "Timestep: 298\tReward: 3.298747099950493\n",
      "Optimization!\n",
      "Timestep: 299\tReward: 5.08604001700965\n",
      "Optimization!\n",
      "Timestep: 300\tReward: 4.877728888877217\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 301\tReward: 7.087846085104021\n",
      "Optimization!\n",
      "Timestep: 302\tReward: 3.5943418594330208\n",
      "Optimization!\n",
      "Timestep: 303\tReward: 8.251054919008194\n",
      "Optimization!\n",
      "Timestep: 304\tReward: 7.357879154444973\n",
      "Optimization!\n",
      "Timestep: 305\tReward: 6.408809625202491\n",
      "Optimization!\n",
      "Timestep: 306\tReward: 3.5410838008275376\n",
      "Optimization!\n",
      "Timestep: 307\tReward: 5.744979643160011\n",
      "Optimization!\n",
      "Timestep: 308\tReward: 4.554188105859194\n",
      "Optimization!\n",
      "Timestep: 309\tReward: 5.562914531791202\n",
      "Optimization!\n",
      "Timestep: 310\tReward: 6.854286001823465\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 311\tReward: 7.590450968535302\n",
      "Optimization!\n",
      "Timestep: 312\tReward: 6.777297251914063\n",
      "Optimization!\n",
      "Timestep: 313\tReward: 2.014695842099234\n",
      "Optimization!\n",
      "Timestep: 314\tReward: 2.3106558346046366\n",
      "Optimization!\n",
      "Timestep: 315\tReward: 6.152384460836628\n",
      "Optimization!\n",
      "Timestep: 316\tReward: 3.4858910899446913\n",
      "Optimization!\n",
      "Timestep: 317\tReward: 5.132500215750403\n",
      "Optimization!\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Timestep: 318\tReward: 4.75733602395567\n",
      "Optimization!\n",
      "Timestep: 319\tReward: 3.342184817453504\n",
      "Optimization!\n",
      "Timestep: 320\tReward: 5.248315039235185\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 321\tReward: 4.888113241710801\n",
      "Optimization!\n",
      "Timestep: 322\tReward: 2.967464703118858\n",
      "Optimization!\n",
      "Timestep: 323\tReward: 3.1679057011614344\n",
      "Optimization!\n",
      "Timestep: 324\tReward: 4.052079002231238\n",
      "Optimization!\n",
      "Timestep: 325\tReward: 3.327854088528261\n",
      "Optimization!\n",
      "Timestep: 326\tReward: 1.4767177865492322\n",
      "Optimization!\n",
      "Timestep: 327\tReward: 5.317672500790626\n",
      "Optimization!\n",
      "Timestep: 328\tReward: 6.066264750218531\n",
      "Optimization!\n",
      "Timestep: 329\tReward: 11.705284603899353\n",
      "Optimization!\n",
      "Timestep: 330\tReward: 5.421135776680793\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 331\tReward: 6.318664513013299\n",
      "Optimization!\n",
      "Timestep: 332\tReward: 5.961307986022384\n",
      "Optimization!\n",
      "Timestep: 333\tReward: 5.046145884720335\n",
      "Optimization!\n",
      "Timestep: 334\tReward: 4.964998999370074\n",
      "Optimization!\n",
      "Timestep: 335\tReward: 5.726075673855876\n",
      "Optimization!\n",
      "Timestep: 336\tReward: 5.376828166370058\n",
      "Optimization!\n",
      "Timestep: 337\tReward: 5.162694557114356\n",
      "Optimization!\n",
      "Timestep: 338\tReward: 2.775041069559451\n",
      "Optimization!\n",
      "Timestep: 339\tReward: 11.981188846849443\n",
      "Optimization!\n",
      "Timestep: 340\tReward: 2.059701234178688\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 341\tReward: 11.82456080141444\n",
      "Optimization!\n",
      "Timestep: 342\tReward: 2.1850006080830062\n",
      "Optimization!\n",
      "Timestep: 343\tReward: 5.635814012377291\n",
      "Optimization!\n",
      "Timestep: 344\tReward: 5.240030790088652\n",
      "Optimization!\n",
      "Timestep: 345\tReward: 6.098837992951031\n",
      "Optimization!\n",
      "Timestep: 346\tReward: 3.546525181399272\n",
      "Optimization!\n",
      "Timestep: 347\tReward: 4.9665962572096625\n",
      "Optimization!\n",
      "Timestep: 348\tReward: 4.796838882705756\n",
      "Optimization!\n",
      "Timestep: 349\tReward: 7.123005324283615\n",
      "Optimization!\n",
      "Timestep: 350\tReward: 2.46998319397449\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 351\tReward: 6.114791350610237\n",
      "Optimization!\n",
      "Timestep: 352\tReward: 4.107239420700779\n",
      "Optimization!\n",
      "Timestep: 353\tReward: 3.4015283653192467\n",
      "Optimization!\n",
      "Timestep: 354\tReward: 3.440179563715467\n",
      "Optimization!\n",
      "Timestep: 355\tReward: 4.649229561459611\n",
      "Optimization!\n",
      "Timestep: 356\tReward: 5.615204090172685\n",
      "Optimization!\n",
      "Timestep: 357\tReward: 6.307728780427232\n",
      "Optimization!\n",
      "Timestep: 358\tReward: 6.117703206167567\n",
      "Optimization!\n",
      "Timestep: 359\tReward: 5.237207105858977\n",
      "Optimization!\n",
      "Timestep: 360\tReward: 4.240574982819141\n",
      "Optimization!\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Timestep: 361\tReward: 5.921308431580224\n",
      "Optimization!\n",
      "Timestep: 362\tReward: 5.699465877585975\n",
      "Optimization!\n",
      "Timestep: 363\tReward: 6.043153337570775\n",
      "Optimization!\n",
      "Timestep: 364\tReward: 5.955576519860137\n",
      "Optimization!\n",
      "Timestep: 365\tReward: 4.286150146537537\n",
      "Optimization!\n",
      "Timestep: 366\tReward: 7.349899669277115\n",
      "Optimization!\n",
      "Timestep: 367\tReward: 8.13846032711375\n",
      "Optimization!\n",
      "Timestep: 368\tReward: 5.438745888631704\n",
      "Optimization!\n",
      "Timestep: 369\tReward: 5.93202335096528\n",
      "Optimization!\n",
      "Timestep: 370\tReward: 4.377055554681904\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 371\tReward: 4.461462778610507\n",
      "Optimization!\n",
      "Timestep: 372\tReward: 4.064574388500675\n",
      "Optimization!\n",
      "Timestep: 373\tReward: 3.3629921680403543\n",
      "Optimization!\n",
      "Timestep: 374\tReward: 1.4482523741017395\n",
      "Optimization!\n",
      "Timestep: 375\tReward: 4.778412392622868\n",
      "Optimization!\n",
      "Timestep: 376\tReward: 1.8239426816378341\n",
      "Optimization!\n",
      "Timestep: 377\tReward: 4.851796382882624\n",
      "Optimization!\n",
      "Timestep: 378\tReward: 5.780810243731587\n",
      "Optimization!\n",
      "Timestep: 379\tReward: 4.413145166971023\n",
      "Optimization!\n",
      "Timestep: 380\tReward: 5.363269808142638\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 381\tReward: 5.207789080544157\n",
      "Optimization!\n",
      "Timestep: 382\tReward: 5.081145149505891\n",
      "Optimization!\n",
      "Timestep: 383\tReward: 6.3152833387256555\n",
      "Optimization!\n",
      "Timestep: 384\tReward: 5.94211719192229\n",
      "Optimization!\n",
      "Timestep: 385\tReward: 6.536905610593639\n",
      "Optimization!\n",
      "Timestep: 386\tReward: 5.17366097014961\n",
      "Optimization!\n",
      "Timestep: 387\tReward: 5.368538351756179\n",
      "Optimization!\n",
      "Timestep: 388\tReward: 3.2849756623255466\n",
      "Optimization!\n",
      "Timestep: 389\tReward: 4.320911437908859\n",
      "Optimization!\n",
      "Timestep: 390\tReward: 5.39688669801905\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 391\tReward: 6.608113339334966\n",
      "Optimization!\n",
      "Timestep: 392\tReward: 3.553313242491746\n",
      "Optimization!\n",
      "Timestep: 393\tReward: 7.143581732180215\n",
      "Optimization!\n",
      "Timestep: 394\tReward: 5.491472860117535\n",
      "Optimization!\n",
      "Timestep: 395\tReward: 6.032042174780975\n",
      "Optimization!\n",
      "Timestep: 396\tReward: 4.9494755700060935\n",
      "Optimization!\n",
      "Timestep: 397\tReward: 6.131496138826812\n",
      "Optimization!\n",
      "Timestep: 398\tReward: 3.474400647795755\n",
      "Optimization!\n",
      "Timestep: 399\tReward: 3.81815857039019\n",
      "Optimization!\n",
      "Timestep: 400\tReward: 3.272035140670627\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 401\tReward: 5.359922002426707\n",
      "Optimization!\n",
      "Timestep: 402\tReward: 1.536032428668539\n",
      "Optimization!\n",
      "Timestep: 403\tReward: 3.681288718525621\n",
      "Optimization!\n",
      "Timestep: 404\tReward: 7.602325213580593\n",
      "Optimization!\n",
      "Timestep: 405\tReward: 6.115507865340906\n",
      "Optimization!\n",
      "Timestep: 406\tReward: 3.763567647218075\n",
      "Optimization!\n",
      "Timestep: 407\tReward: 3.3272222205531214\n",
      "Optimization!\n",
      "Timestep: 408\tReward: 2.5802800779766026\n",
      "Optimization!\n",
      "Timestep: 409\tReward: 6.655269519126739\n",
      "Optimization!\n",
      "Timestep: 410\tReward: 7.820179563606617\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 411\tReward: 7.272472467824852\n",
      "Optimization!\n",
      "Timestep: 412\tReward: 3.357783697003881\n",
      "Optimization!\n",
      "Timestep: 413\tReward: 3.9520502398788326\n",
      "Optimization!\n",
      "Timestep: 414\tReward: 0.7382839619941692\n",
      "Optimization!\n",
      "Timestep: 415\tReward: 5.743299621366204\n",
      "Optimization!\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Timestep: 416\tReward: 3.923498802250762\n",
      "Optimization!\n",
      "Timestep: 417\tReward: 6.639610493577088\n",
      "Optimization!\n",
      "Timestep: 418\tReward: 3.2627584975890582\n",
      "Optimization!\n",
      "Timestep: 419\tReward: 6.031066145585743\n",
      "Optimization!\n",
      "Timestep: 420\tReward: 4.574794889508917\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 421\tReward: 6.745922572414194\n",
      "Optimization!\n",
      "Timestep: 422\tReward: 2.5046985552199144\n",
      "Optimization!\n",
      "Timestep: 423\tReward: 7.016600014954257\n",
      "Optimization!\n",
      "Timestep: 424\tReward: 4.797246853594381\n",
      "Optimization!\n",
      "Timestep: 425\tReward: 3.955541624385046\n",
      "Optimization!\n",
      "Timestep: 426\tReward: 5.226164367929787\n",
      "Optimization!\n",
      "Timestep: 427\tReward: 7.076450716613233\n",
      "Optimization!\n",
      "Timestep: 428\tReward: 4.310792507074317\n",
      "Optimization!\n",
      "Timestep: 429\tReward: 5.255576803224844\n",
      "Optimization!\n",
      "Timestep: 430\tReward: 5.363876951477106\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 431\tReward: 7.864930941401828\n",
      "Optimization!\n",
      "Timestep: 432\tReward: 7.447368910451341\n",
      "Optimization!\n",
      "Timestep: 433\tReward: 4.463038603423556\n",
      "Optimization!\n",
      "Timestep: 434\tReward: 5.657068971476439\n",
      "Optimization!\n",
      "Timestep: 435\tReward: 5.282978201919244\n",
      "Optimization!\n",
      "Timestep: 436\tReward: 6.0859390829350515\n",
      "Optimization!\n",
      "Timestep: 437\tReward: 2.5979619624752353\n",
      "Optimization!\n",
      "Timestep: 438\tReward: 6.426536817963812\n",
      "Optimization!\n",
      "Timestep: 439\tReward: 8.137078623207227\n",
      "Optimization!\n",
      "Timestep: 440\tReward: 3.656662349993512\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 441\tReward: 5.118365902095026\n",
      "Optimization!\n",
      "Timestep: 442\tReward: 6.7814175984032055\n",
      "Optimization!\n",
      "Timestep: 443\tReward: 6.823375664583611\n",
      "Optimization!\n",
      "Timestep: 444\tReward: 7.068730391838363\n",
      "Optimization!\n",
      "Timestep: 445\tReward: 4.815936848873739\n",
      "Optimization!\n",
      "Timestep: 446\tReward: 5.240880158167233\n",
      "Optimization!\n",
      "Timestep: 447\tReward: 4.064756772254192\n",
      "Optimization!\n",
      "Timestep: 448\tReward: 5.073926465602141\n",
      "Optimization!\n",
      "Timestep: 449\tReward: 3.4698509058909037\n",
      "Optimization!\n",
      "Timestep: 450\tReward: 6.982797540245185\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 451\tReward: 3.145032048735633\n",
      "Optimization!\n",
      "Timestep: 452\tReward: 4.601481013269024\n",
      "Optimization!\n",
      "Timestep: 453\tReward: 7.552506681824663\n",
      "Optimization!\n",
      "Timestep: 454\tReward: 4.977148101919884\n",
      "Optimization!\n",
      "Timestep: 455\tReward: 7.045968734390175\n",
      "Optimization!\n",
      "Timestep: 456\tReward: 4.636702133227289\n",
      "Optimization!\n",
      "Timestep: 457\tReward: 4.136563132897084\n",
      "Optimization!\n",
      "Timestep: 458\tReward: 4.301560910635768\n",
      "Optimization!\n",
      "Timestep: 459\tReward: 4.684964957856315\n",
      "Optimization!\n",
      "Timestep: 460\tReward: 6.511036251556209\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 461\tReward: 2.151720447477343\n",
      "Optimization!\n",
      "Timestep: 462\tReward: 5.694751516838726\n",
      "Optimization!\n",
      "Timestep: 463\tReward: 1.9230138110619681\n",
      "Optimization!\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Timestep: 464\tReward: 5.680683888845498\n",
      "Optimization!\n",
      "Timestep: 465\tReward: 6.162308952210674\n",
      "Optimization!\n",
      "Timestep: 466\tReward: 5.1637877358028454\n",
      "Optimization!\n",
      "Timestep: 467\tReward: 5.003515784754439\n",
      "Optimization!\n",
      "Timestep: 468\tReward: 7.598568940387134\n",
      "Optimization!\n",
      "Timestep: 469\tReward: 5.569895171001525\n",
      "Optimization!\n",
      "Timestep: 470\tReward: 3.4560311185697943\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 471\tReward: 6.334121397435179\n",
      "Optimization!\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Timestep: 472\tReward: 3.873406790897665\n",
      "Optimization!\n",
      "Timestep: 473\tReward: 2.4206776204347817\n",
      "Optimization!\n",
      "Timestep: 474\tReward: 7.06562877089781\n",
      "Optimization!\n",
      "Timestep: 475\tReward: 5.704879750347962\n",
      "Optimization!\n",
      "Timestep: 476\tReward: 7.377655631662702\n",
      "Optimization!\n",
      "Timestep: 477\tReward: 5.539380678081309\n",
      "Optimization!\n",
      "Timestep: 478\tReward: 6.593649617275894\n",
      "Optimization!\n",
      "Timestep: 479\tReward: 3.0077018455392546\n",
      "Optimization!\n",
      "Timestep: 480\tReward: 6.856606922329829\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 481\tReward: 3.3269917049742976\n",
      "Optimization!\n",
      "Timestep: 482\tReward: 2.76213322146472\n",
      "Optimization!\n",
      "Timestep: 483\tReward: 6.502577487696461\n",
      "Optimization!\n",
      "Timestep: 484\tReward: 7.012207610949263\n",
      "Optimization!\n",
      "Timestep: 485\tReward: 4.254141945059675\n",
      "Optimization!\n",
      "Timestep: 486\tReward: 5.3153383450674845\n",
      "Optimization!\n",
      "Timestep: 487\tReward: 5.616188143705145\n",
      "Optimization!\n",
      "Timestep: 488\tReward: 6.495401845637928\n",
      "Optimization!\n",
      "Timestep: 489\tReward: 6.356174337973176\n",
      "Optimization!\n",
      "Timestep: 490\tReward: 2.7617598180314116\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 491\tReward: 5.70594991288641\n",
      "Optimization!\n",
      "Timestep: 492\tReward: 5.621442828658389\n",
      "Optimization!\n",
      "Timestep: 493\tReward: 2.729137801378581\n",
      "Optimization!\n",
      "Timestep: 494\tReward: 5.002469844176628\n",
      "Optimization!\n",
      "Timestep: 495\tReward: 3.6955024738851\n",
      "Optimization!\n",
      "Timestep: 496\tReward: 4.0399186847077715\n",
      "Optimization!\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Timestep: 497\tReward: 5.2722051889253665\n",
      "Optimization!\n",
      "Timestep: 498\tReward: 6.174022849798405\n",
      "Optimization!\n",
      "Timestep: 499\tReward: 3.8436284459549968\n",
      "Optimization!\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Timestep: 500\tReward: 9.04669982540488\n",
      "Optimization!\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Timestep: 501\tReward: 7.253205751085757\n",
      "Optimization!\n",
      "Timestep: 502\tReward: 6.345482177064755\n",
      "Optimization!\n",
      "Timestep: 503\tReward: 4.916213303365601\n",
      "Optimization!\n",
      "Timestep: 504\tReward: 6.335148691207501\n",
      "Optimization!\n",
      "Timestep: 505\tReward: 5.254333136518118\n",
      "Optimization!\n",
      "Timestep: 506\tReward: 2.710288197849485\n",
      "Optimization!\n",
      "Timestep: 507\tReward: 4.425313993002849\n",
      "Optimization!\n",
      "Timestep: 508\tReward: 7.543701382115437\n",
      "Optimization!\n",
      "Timestep: 509\tReward: 6.371320437605872\n",
      "Optimization!\n",
      "Timestep: 510\tReward: 5.60749567112275\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 511\tReward: 6.25626611951691\n",
      "Optimization!\n",
      "Timestep: 512\tReward: 7.726424474184522\n",
      "Optimization!\n",
      "Timestep: 513\tReward: 4.707137017880302\n",
      "Optimization!\n",
      "Timestep: 514\tReward: 4.724779137123244\n",
      "Optimization!\n",
      "Timestep: 515\tReward: 5.42728696127068\n",
      "Optimization!\n",
      "Timestep: 516\tReward: 6.06415051658487\n",
      "Optimization!\n",
      "Timestep: 517\tReward: 6.044153196383781\n",
      "Optimization!\n",
      "Timestep: 518\tReward: 5.70942303944609\n",
      "Optimization!\n",
      "Timestep: 519\tReward: 6.8285034030619105\n",
      "Optimization!\n",
      "Timestep: 520\tReward: 2.540042331065798\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 521\tReward: 5.005516602171067\n",
      "Optimization!\n",
      "Timestep: 522\tReward: 6.718573060564846\n",
      "Optimization!\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Timestep: 523\tReward: 4.626426654632701\n",
      "Optimization!\n",
      "Timestep: 524\tReward: 5.259947925803128\n",
      "Optimization!\n",
      "Timestep: 525\tReward: 7.10112743824564\n",
      "Optimization!\n",
      "Timestep: 526\tReward: 3.5244579552000923\n",
      "Optimization!\n",
      "Timestep: 527\tReward: 4.8261744428136755\n",
      "Optimization!\n",
      "Timestep: 528\tReward: 4.844296611981775\n",
      "Optimization!\n",
      "Timestep: 529\tReward: 4.814920715814264\n",
      "Optimization!\n",
      "Timestep: 530\tReward: 3.891948234887415\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 531\tReward: 7.4882151157074\n",
      "Optimization!\n",
      "Timestep: 532\tReward: 3.7806336688511766\n",
      "Optimization!\n",
      "Timestep: 533\tReward: 3.854658879082767\n",
      "Optimization!\n",
      "Timestep: 534\tReward: 3.9488913821830085\n",
      "Optimization!\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Timestep: 535\tReward: 5.474698364268138\n",
      "Optimization!\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Timestep: 536\tReward: 3.2360400883992013\n",
      "Optimization!\n",
      "Timestep: 537\tReward: 5.481770618334397\n",
      "Optimization!\n",
      "Timestep: 538\tReward: 4.384729602026656\n",
      "Optimization!\n",
      "Timestep: 539\tReward: 5.317152071694511\n",
      "Optimization!\n",
      "Timestep: 540\tReward: 7.74520141573057\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 541\tReward: 4.7839626432386435\n",
      "Optimization!\n",
      "Timestep: 542\tReward: 2.2388176960059534\n",
      "Optimization!\n",
      "Timestep: 543\tReward: 4.805635761710864\n",
      "Optimization!\n",
      "Timestep: 544\tReward: 5.299618114095773\n",
      "Optimization!\n",
      "Timestep: 545\tReward: 4.241292584618613\n",
      "Optimization!\n",
      "Timestep: 546\tReward: 5.947344004086748\n",
      "Optimization!\n",
      "Timestep: 547\tReward: 5.000619728539032\n",
      "Optimization!\n",
      "Timestep: 548\tReward: 4.031462931586241\n",
      "Optimization!\n",
      "Timestep: 549\tReward: 3.1329323297456146\n",
      "Optimization!\n",
      "Timestep: 550\tReward: 4.715422533342313\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 551\tReward: 3.3604404055111745\n",
      "Optimization!\n",
      "Timestep: 552\tReward: 4.185865490854532\n",
      "Optimization!\n",
      "Timestep: 553\tReward: 7.741209229353226\n",
      "Optimization!\n",
      "Timestep: 554\tReward: 5.202891570087408\n",
      "Optimization!\n",
      "Timestep: 555\tReward: 5.952344638650676\n",
      "Optimization!\n",
      "Timestep: 556\tReward: 2.092781720005635\n",
      "Optimization!\n",
      "Timestep: 557\tReward: 7.287923175271008\n",
      "Optimization!\n",
      "Timestep: 558\tReward: 3.4878665067930354\n",
      "Optimization!\n",
      "Timestep: 559\tReward: 6.70684563743541\n",
      "Optimization!\n",
      "Timestep: 560\tReward: 6.511521018737526\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 561\tReward: 4.053737463222154\n",
      "Optimization!\n",
      "Timestep: 562\tReward: 5.4889967973928115\n",
      "Optimization!\n",
      "Timestep: 563\tReward: 5.164373222148625\n",
      "Optimization!\n",
      "Timestep: 564\tReward: 6.826992735710891\n",
      "Optimization!\n",
      "Timestep: 565\tReward: 5.099563846131719\n",
      "Optimization!\n",
      "Timestep: 566\tReward: 3.0873643204463908\n",
      "Optimization!\n",
      "Timestep: 567\tReward: 6.487916928552619\n",
      "Optimization!\n",
      "Timestep: 568\tReward: 4.032808999682069\n",
      "Optimization!\n",
      "Timestep: 569\tReward: 2.6587889212775\n",
      "Optimization!\n",
      "Timestep: 570\tReward: 2.867126948766008\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 571\tReward: 5.144284672506526\n",
      "Optimization!\n",
      "Timestep: 572\tReward: 4.831291630088162\n",
      "Optimization!\n",
      "Timestep: 573\tReward: 5.8058505730909555\n",
      "Optimization!\n",
      "Timestep: 574\tReward: 4.349640806564602\n",
      "Optimization!\n",
      "Timestep: 575\tReward: 3.465326466375014\n",
      "Optimization!\n",
      "Timestep: 576\tReward: 6.7375558667437385\n",
      "Optimization!\n",
      "Timestep: 577\tReward: 6.297449701394407\n",
      "Optimization!\n",
      "Timestep: 578\tReward: 6.237414481036054\n",
      "Optimization!\n",
      "Timestep: 579\tReward: 3.7194293417910855\n",
      "Optimization!\n",
      "Timestep: 580\tReward: 3.620538827642994\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 581\tReward: 1.9250741226187702\n",
      "Optimization!\n",
      "Timestep: 582\tReward: 6.457978256589598\n",
      "Optimization!\n",
      "Timestep: 583\tReward: 3.1788012661422282\n",
      "Optimization!\n",
      "Timestep: 584\tReward: 3.0155211926620473\n",
      "Optimization!\n",
      "Timestep: 585\tReward: 4.558484674181486\n",
      "Optimization!\n",
      "Timestep: 586\tReward: 5.363591740726969\n",
      "Optimization!\n",
      "Timestep: 587\tReward: 5.705087721734512\n",
      "Optimization!\n",
      "Timestep: 588\tReward: 2.443423676214947\n",
      "Optimization!\n",
      "Timestep: 589\tReward: 6.2103566666700605\n",
      "Optimization!\n",
      "Timestep: 590\tReward: 7.204658143625848\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 591\tReward: 5.503742007043701\n",
      "Optimization!\n",
      "Timestep: 592\tReward: 4.698484111032146\n",
      "Optimization!\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Timestep: 593\tReward: 3.9311590691889347\n",
      "Optimization!\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Timestep: 594\tReward: 5.176601324093389\n",
      "Optimization!\n",
      "Timestep: 595\tReward: 5.266144165121474\n",
      "Optimization!\n",
      "Timestep: 596\tReward: 6.795605686214466\n",
      "Optimization!\n",
      "Timestep: 597\tReward: 6.299537806574489\n",
      "Optimization!\n",
      "Timestep: 598\tReward: 1.9839475617014213\n",
      "Optimization!\n",
      "Timestep: 599\tReward: 5.287944439190285\n",
      "Optimization!\n",
      "Timestep: 600\tReward: 5.534508503230614\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 601\tReward: 8.449540281960807\n",
      "Optimization!\n",
      "Timestep: 602\tReward: 4.3010809063968685\n",
      "Optimization!\n",
      "Timestep: 603\tReward: 6.176814325970475\n",
      "Optimization!\n",
      "Timestep: 604\tReward: 4.06181617188695\n",
      "Optimization!\n",
      "Timestep: 605\tReward: 6.683861658753647\n",
      "Optimization!\n",
      "Timestep: 606\tReward: 2.8386587376173167\n",
      "Optimization!\n",
      "Timestep: 607\tReward: 2.265810592621107\n",
      "Optimization!\n",
      "Timestep: 608\tReward: 3.7633137995826456\n",
      "Optimization!\n",
      "Timestep: 609\tReward: 5.5017915816215845\n",
      "Optimization!\n",
      "Timestep: 610\tReward: 5.553398552843399\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 611\tReward: 3.8478120774114926\n",
      "Optimization!\n",
      "Timestep: 612\tReward: 7.088962173536293\n",
      "Optimization!\n",
      "Timestep: 613\tReward: 7.841267724202319\n",
      "Optimization!\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Timestep: 614\tReward: 1.7570126844738423\n",
      "Optimization!\n",
      "Timestep: 615\tReward: 3.0742402511105453\n",
      "Optimization!\n",
      "Timestep: 616\tReward: 3.72236781134863\n",
      "Optimization!\n",
      "Timestep: 617\tReward: 4.666134602889131\n",
      "Optimization!\n",
      "Timestep: 618\tReward: 6.922397688690559\n",
      "Optimization!\n",
      "Timestep: 619\tReward: 5.434083479409981\n",
      "Optimization!\n",
      "Timestep: 620\tReward: 7.031121640557416\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 621\tReward: 8.377628352390191\n",
      "Optimization!\n",
      "Timestep: 622\tReward: 5.699290902194718\n",
      "Optimization!\n",
      "Timestep: 623\tReward: 5.4295723091381305\n",
      "Optimization!\n",
      "Timestep: 624\tReward: 6.6900492918207535\n",
      "Optimization!\n",
      "Timestep: 625\tReward: 5.0576737549398425\n",
      "Optimization!\n",
      "Timestep: 626\tReward: 5.628080950460766\n",
      "Optimization!\n",
      "Timestep: 627\tReward: 4.069313331681384\n",
      "Optimization!\n",
      "Timestep: 628\tReward: 5.212264082265424\n",
      "Optimization!\n",
      "Timestep: 629\tReward: 5.779275784753853\n",
      "Optimization!\n",
      "Timestep: 630\tReward: 5.396349658255192\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 631\tReward: 4.619149823357533\n",
      "Optimization!\n",
      "Timestep: 632\tReward: 6.011955200743334\n",
      "Optimization!\n",
      "Timestep: 633\tReward: 4.347352362149681\n",
      "Optimization!\n",
      "Timestep: 634\tReward: 5.742512990489692\n",
      "Optimization!\n",
      "Timestep: 635\tReward: 6.614215051927488\n",
      "Optimization!\n",
      "Timestep: 636\tReward: 2.724918008114593\n",
      "Optimization!\n",
      "Timestep: 637\tReward: 6.072179008653621\n",
      "Optimization!\n",
      "Timestep: 638\tReward: 5.00115433533183\n",
      "Optimization!\n",
      "Timestep: 639\tReward: 5.468613897461303\n",
      "Optimization!\n",
      "Timestep: 640\tReward: 4.408685297641333\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 641\tReward: 5.736035575322567\n",
      "Optimization!\n",
      "Timestep: 642\tReward: 1.7617036744688965\n",
      "Optimization!\n",
      "Timestep: 643\tReward: 4.974409196582945\n",
      "Optimization!\n",
      "Timestep: 644\tReward: 5.813731777861731\n",
      "Optimization!\n",
      "Timestep: 645\tReward: 6.879528456614614\n",
      "Optimization!\n",
      "Timestep: 646\tReward: 4.535726642503348\n",
      "Optimization!\n",
      "Timestep: 647\tReward: 7.590518720389388\n",
      "Optimization!\n",
      "Timestep: 648\tReward: 4.058488095690038\n",
      "Optimization!\n",
      "Timestep: 649\tReward: 6.22813256023876\n",
      "Optimization!\n",
      "Timestep: 650\tReward: 5.736331977095139\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 651\tReward: 4.711880476814876\n",
      "Optimization!\n",
      "Timestep: 652\tReward: 4.529824831310756\n",
      "Optimization!\n",
      "Timestep: 653\tReward: 3.643337628036982\n",
      "Optimization!\n",
      "Timestep: 654\tReward: 8.85868553982501\n",
      "Optimization!\n",
      "Timestep: 655\tReward: 6.620473274243519\n",
      "Optimization!\n",
      "Timestep: 656\tReward: 6.360955345936281\n",
      "Optimization!\n",
      "Timestep: 657\tReward: 6.951569984950876\n",
      "Optimization!\n",
      "Timestep: 658\tReward: 5.13359730750783\n",
      "Optimization!\n",
      "Timestep: 659\tReward: 8.538723551896686\n",
      "Optimization!\n",
      "Timestep: 660\tReward: 7.87219472625204\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 661\tReward: 4.433682639033616\n",
      "Optimization!\n",
      "Timestep: 662\tReward: 4.227861029643543\n",
      "Optimization!\n",
      "Timestep: 663\tReward: 6.631075990297282\n",
      "Optimization!\n",
      "Timestep: 664\tReward: 4.503290229097964\n",
      "Optimization!\n",
      "Timestep: 665\tReward: 5.464345022309577\n",
      "Optimization!\n",
      "Timestep: 666\tReward: 7.608270550079425\n",
      "Optimization!\n",
      "Timestep: 667\tReward: 3.7583860191484786\n",
      "Optimization!\n",
      "Timestep: 668\tReward: 5.7696400226801465\n",
      "Optimization!\n",
      "Timestep: 669\tReward: 7.365518522237387\n",
      "Optimization!\n",
      "Timestep: 670\tReward: 6.941743204418782\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 671\tReward: 5.769588418470638\n",
      "Optimization!\n",
      "Timestep: 672\tReward: 3.96978820361637\n",
      "Optimization!\n",
      "Timestep: 673\tReward: 4.676227350560023\n",
      "Optimization!\n",
      "Timestep: 674\tReward: 2.1042665885876644\n",
      "Optimization!\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Timestep: 675\tReward: 6.270155630297991\n",
      "Optimization!\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Timestep: 676\tReward: 1.380769827917389\n",
      "Optimization!\n",
      "Timestep: 677\tReward: 4.676390665296951\n",
      "Optimization!\n",
      "Timestep: 678\tReward: 5.473669095777781\n",
      "Optimization!\n",
      "Timestep: 679\tReward: 7.581015798863771\n",
      "Optimization!\n",
      "Timestep: 680\tReward: 7.2684796071213835\n",
      "Optimization!\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Timestep: 681\tReward: 5.316541895741207\n",
      "Optimization!\n",
      "Timestep: 682\tReward: 6.86590915831995\n",
      "Optimization!\n",
      "Timestep: 683\tReward: 5.271264669203725\n",
      "Optimization!\n",
      "Timestep: 684\tReward: 6.094171943454525\n",
      "Optimization!\n",
      "Timestep: 685\tReward: 6.482661781363046\n",
      "Optimization!\n",
      "Timestep: 686\tReward: 6.252485730150035\n",
      "Optimization!\n",
      "Timestep: 687\tReward: 5.095876207477687\n",
      "Optimization!\n",
      "Timestep: 688\tReward: 7.0380824020622565\n",
      "Optimization!\n",
      "Timestep: 689\tReward: 4.20255826704751\n",
      "Optimization!\n",
      "Timestep: 690\tReward: 5.3438801219576035\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 691\tReward: 4.515633126114759\n",
      "Optimization!\n",
      "Timestep: 692\tReward: 1.928355806026239\n",
      "Optimization!\n",
      "Timestep: 693\tReward: 2.3840154567706624\n",
      "Optimization!\n",
      "Timestep: 694\tReward: 3.8226366401646663\n",
      "Optimization!\n",
      "Timestep: 695\tReward: 4.630937272074047\n",
      "Optimization!\n",
      "Timestep: 696\tReward: 3.0974230626397428\n",
      "Optimization!\n",
      "Timestep: 697\tReward: 5.707709857608583\n",
      "Optimization!\n",
      "Timestep: 698\tReward: 4.562606362545686\n",
      "Optimization!\n",
      "Timestep: 699\tReward: 5.189401821082036\n",
      "Optimization!\n",
      "Timestep: 700\tReward: 7.013521130690453\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 701\tReward: 3.616975104620447\n",
      "Optimization!\n",
      "Timestep: 702\tReward: 4.639786621452332\n",
      "Optimization!\n",
      "Timestep: 703\tReward: 5.629517551864038\n",
      "Optimization!\n",
      "Timestep: 704\tReward: 4.518658334808176\n",
      "Optimization!\n",
      "Timestep: 705\tReward: 5.546089192787455\n",
      "Optimization!\n",
      "Timestep: 706\tReward: 6.1520932890208355\n",
      "Optimization!\n",
      "Timestep: 707\tReward: 4.510252495307613\n",
      "Optimization!\n",
      "Timestep: 708\tReward: 8.016699944188792\n",
      "Optimization!\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Timestep: 709\tReward: 6.788808121751204\n",
      "Optimization!\n",
      "Timestep: 710\tReward: 6.664590561066781\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 711\tReward: 2.005356910250136\n",
      "Optimization!\n",
      "Timestep: 712\tReward: 3.152025035904198\n",
      "Optimization!\n",
      "Timestep: 713\tReward: 4.819060992452695\n",
      "Optimization!\n",
      "Timestep: 714\tReward: 5.790311490366077\n",
      "Optimization!\n",
      "Timestep: 715\tReward: 3.7950245248695085\n",
      "Optimization!\n",
      "Timestep: 716\tReward: 5.0020268718551675\n",
      "Optimization!\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Timestep: 717\tReward: 4.998580675546189\n",
      "Optimization!\n",
      "Timestep: 718\tReward: 4.8158419567109965\n",
      "Optimization!\n",
      "Timestep: 719\tReward: 5.716342080210207\n",
      "Optimization!\n",
      "Timestep: 720\tReward: 5.326275776071591\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 721\tReward: 2.052375060376607\n",
      "Optimization!\n",
      "Timestep: 722\tReward: 5.208979693136846\n",
      "Optimization!\n",
      "Timestep: 723\tReward: 4.771359674384238\n",
      "Optimization!\n",
      "Timestep: 724\tReward: 4.000543269759682\n",
      "Optimization!\n",
      "Timestep: 725\tReward: 4.109996215047002\n",
      "Optimization!\n",
      "Timestep: 726\tReward: 6.310074801799459\n",
      "Optimization!\n",
      "Timestep: 727\tReward: 4.499565942148002\n",
      "Optimization!\n",
      "Timestep: 728\tReward: 3.419931277187535\n",
      "Optimization!\n",
      "Timestep: 729\tReward: 7.242817120514802\n",
      "Optimization!\n",
      "Timestep: 730\tReward: 3.68515619453748\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 731\tReward: 4.147410055048911\n",
      "Optimization!\n",
      "Timestep: 732\tReward: 3.9528993754501744\n",
      "Optimization!\n",
      "Timestep: 733\tReward: 7.755539458162089\n",
      "Optimization!\n",
      "Timestep: 734\tReward: 5.910308878479139\n",
      "Optimization!\n",
      "Timestep: 735\tReward: 4.5744857584966745\n",
      "Optimization!\n",
      "Timestep: 736\tReward: 1.799926599018913\n",
      "Optimization!\n",
      "Timestep: 737\tReward: 6.481721912853189\n",
      "Optimization!\n",
      "Timestep: 738\tReward: 7.627639036084527\n",
      "Optimization!\n",
      "Timestep: 739\tReward: 6.138594404580612\n",
      "Optimization!\n",
      "Timestep: 740\tReward: 4.714987492647276\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 741\tReward: 7.805943398867863\n",
      "Optimization!\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Timestep: 742\tReward: 2.997175130350055\n",
      "Optimization!\n",
      "Timestep: 743\tReward: 8.043258723341017\n",
      "Optimization!\n",
      "Timestep: 744\tReward: 2.5569481882356797\n",
      "Optimization!\n",
      "Timestep: 745\tReward: 5.868050111245225\n",
      "Optimization!\n",
      "Timestep: 746\tReward: 5.540413240566112\n",
      "Optimization!\n",
      "Timestep: 747\tReward: 6.6400302964016\n",
      "Optimization!\n",
      "Timestep: 748\tReward: 5.497937835160738\n",
      "Optimization!\n",
      "Timestep: 749\tReward: 2.667265049741712\n",
      "Optimization!\n",
      "Timestep: 750\tReward: 6.58075056513313\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 751\tReward: 5.852497242093883\n",
      "Optimization!\n",
      "Timestep: 752\tReward: 5.082005038687487\n",
      "Optimization!\n",
      "Timestep: 753\tReward: 7.796079139073502\n",
      "Optimization!\n",
      "Timestep: 754\tReward: 3.2655694676575373\n",
      "Optimization!\n",
      "Timestep: 755\tReward: 6.2764210031174175\n",
      "Optimization!\n",
      "Timestep: 756\tReward: 2.9758548138739425\n",
      "Optimization!\n",
      "Timestep: 757\tReward: 4.01030945812558\n",
      "Optimization!\n",
      "Timestep: 758\tReward: 4.959083921202143\n",
      "Optimization!\n",
      "Timestep: 759\tReward: 7.099966677611693\n",
      "Optimization!\n",
      "Timestep: 760\tReward: 6.07933951142933\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 761\tReward: 5.41646920149088\n",
      "Optimization!\n",
      "Timestep: 762\tReward: 5.8644785121511545\n",
      "Optimization!\n",
      "Timestep: 763\tReward: 1.4559155872136076\n",
      "Optimization!\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Timestep: 764\tReward: 4.533403742356952\n",
      "Optimization!\n",
      "Timestep: 765\tReward: 4.841999295409496\n",
      "Optimization!\n",
      "Timestep: 766\tReward: 5.714025238392991\n",
      "Optimization!\n",
      "Timestep: 767\tReward: 7.394394018136874\n",
      "Optimization!\n",
      "Timestep: 768\tReward: 2.8882721391911095\n",
      "Optimization!\n",
      "Timestep: 769\tReward: 6.208775579693283\n",
      "Optimization!\n",
      "Timestep: 770\tReward: 6.697361645314775\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 771\tReward: 6.244145751307763\n",
      "Optimization!\n",
      "Timestep: 772\tReward: 4.61635171741434\n",
      "Optimization!\n",
      "Timestep: 773\tReward: 4.508052556801655\n",
      "Optimization!\n",
      "Timestep: 774\tReward: 2.642039197293171\n",
      "Optimization!\n",
      "Timestep: 775\tReward: 4.4488460309427635\n",
      "Optimization!\n",
      "Timestep: 776\tReward: 5.156036901480888\n",
      "Optimization!\n",
      "Timestep: 777\tReward: 4.230466565400458\n",
      "Optimization!\n",
      "Timestep: 778\tReward: 7.103604240604283\n",
      "Optimization!\n",
      "Timestep: 779\tReward: 5.939018228123744\n",
      "Optimization!\n",
      "Timestep: 780\tReward: 4.436927376046575\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 781\tReward: 6.111119304785068\n",
      "Optimization!\n",
      "Timestep: 782\tReward: 7.234530873645059\n",
      "Optimization!\n",
      "Timestep: 783\tReward: 5.031019473977972\n",
      "Optimization!\n",
      "Timestep: 784\tReward: 5.122040125995333\n",
      "Optimization!\n",
      "Timestep: 785\tReward: 5.5338759764159775\n",
      "Optimization!\n",
      "Timestep: 786\tReward: 7.840339157086923\n",
      "Optimization!\n",
      "Timestep: 787\tReward: 5.305571897966055\n",
      "Optimization!\n",
      "Timestep: 788\tReward: 5.028602994874567\n",
      "Optimization!\n",
      "Timestep: 789\tReward: 2.7149607469576096\n",
      "Optimization!\n",
      "Timestep: 790\tReward: 4.987109059779891\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 791\tReward: 4.027163916681035\n",
      "Optimization!\n",
      "Timestep: 792\tReward: 4.411276134316669\n",
      "Optimization!\n",
      "Timestep: 793\tReward: 6.248532057679274\n",
      "Optimization!\n",
      "Timestep: 794\tReward: 1.3456640943083344\n",
      "Optimization!\n",
      "Timestep: 795\tReward: 5.383739149198632\n",
      "Optimization!\n",
      "Timestep: 796\tReward: 4.062495148152571\n",
      "Optimization!\n",
      "Timestep: 797\tReward: 4.819961775086348\n",
      "Optimization!\n",
      "Timestep: 798\tReward: 2.62286305413732\n",
      "Optimization!\n",
      "Timestep: 799\tReward: 4.122175730522324\n",
      "Optimization!\n",
      "Timestep: 800\tReward: 7.160128710502258\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 801\tReward: 6.963344838468858\n",
      "Optimization!\n",
      "Timestep: 802\tReward: 5.462530472218177\n",
      "Optimization!\n",
      "Timestep: 803\tReward: 4.971115600640666\n",
      "Optimization!\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Timestep: 804\tReward: 3.924542326227153\n",
      "Optimization!\n",
      "Timestep: 805\tReward: 6.516225594385211\n",
      "Optimization!\n",
      "Timestep: 806\tReward: 6.804511148103718\n",
      "Optimization!\n",
      "Timestep: 807\tReward: 7.611954374424255\n",
      "Optimization!\n",
      "Timestep: 808\tReward: 4.61313565068751\n",
      "Optimization!\n",
      "Timestep: 809\tReward: 7.786246510663535\n",
      "Optimization!\n",
      "Timestep: 810\tReward: 2.1408572873991782\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 811\tReward: 6.465313383966223\n",
      "Optimization!\n",
      "Timestep: 812\tReward: 6.159379040712757\n",
      "Optimization!\n",
      "Timestep: 813\tReward: 3.5834272675941934\n",
      "Optimization!\n",
      "Timestep: 814\tReward: 5.375916864928908\n",
      "Optimization!\n",
      "Timestep: 815\tReward: 6.953781121488911\n",
      "Optimization!\n",
      "Timestep: 816\tReward: 7.435073020554048\n",
      "Optimization!\n",
      "Timestep: 817\tReward: 5.443975648695219\n",
      "Optimization!\n",
      "Timestep: 818\tReward: 4.502586591814405\n",
      "Optimization!\n",
      "Timestep: 819\tReward: 5.759736304603532\n",
      "Optimization!\n",
      "Timestep: 820\tReward: 8.823430610853276\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 821\tReward: 1.5493199151570947\n",
      "Optimization!\n",
      "Timestep: 822\tReward: 4.243891892960906\n",
      "Optimization!\n",
      "Timestep: 823\tReward: 5.280519940150749\n",
      "Optimization!\n",
      "Timestep: 824\tReward: 6.379685433391978\n",
      "Optimization!\n",
      "Timestep: 825\tReward: 13.343857062750338\n",
      "Optimization!\n",
      "Timestep: 826\tReward: 5.352326808227571\n",
      "Optimization!\n",
      "Timestep: 827\tReward: 5.248399342198776\n",
      "Optimization!\n",
      "Timestep: 828\tReward: 6.618547268594472\n",
      "Optimization!\n",
      "Timestep: 829\tReward: 7.0113428399767574\n",
      "Optimization!\n",
      "Timestep: 830\tReward: 8.972235208733467\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 831\tReward: 2.0378633219128908\n",
      "Optimization!\n",
      "Timestep: 832\tReward: 5.9606810778789425\n",
      "Optimization!\n",
      "Timestep: 833\tReward: 7.680049378077197\n",
      "Optimization!\n",
      "Timestep: 834\tReward: 5.865971491499396\n",
      "Optimization!\n",
      "Timestep: 835\tReward: 2.9789467137453065\n",
      "Optimization!\n",
      "Timestep: 836\tReward: 7.246545742921461\n",
      "Optimization!\n",
      "Timestep: 837\tReward: 3.1369247463592704\n",
      "Optimization!\n",
      "Timestep: 838\tReward: 7.939068924108053\n",
      "Optimization!\n",
      "Timestep: 839\tReward: 6.17636144074691\n",
      "Optimization!\n",
      "Timestep: 840\tReward: 4.8019220742609585\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 841\tReward: 6.0563651241542855\n",
      "Optimization!\n",
      "Timestep: 842\tReward: 7.813040612640054\n",
      "Optimization!\n",
      "Timestep: 843\tReward: 1.3468946377227213\n",
      "Optimization!\n",
      "Timestep: 844\tReward: 4.733625680229007\n",
      "Optimization!\n",
      "Timestep: 845\tReward: 6.104767936743866\n",
      "Optimization!\n",
      "Timestep: 846\tReward: 4.114773703126352\n",
      "Optimization!\n",
      "Timestep: 847\tReward: 4.421499329027593\n",
      "Optimization!\n",
      "Timestep: 848\tReward: 2.007657777987264\n",
      "Optimization!\n",
      "Timestep: 849\tReward: 6.405428259084169\n",
      "Optimization!\n",
      "Timestep: 850\tReward: 2.3765442656007956\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 851\tReward: 5.466681190758733\n",
      "Optimization!\n",
      "Timestep: 852\tReward: 5.229930779791334\n",
      "Optimization!\n",
      "Timestep: 853\tReward: 2.5534416287299337\n",
      "Optimization!\n",
      "Timestep: 854\tReward: 5.355862230755793\n",
      "Optimization!\n",
      "Timestep: 855\tReward: 2.8384027440341533\n",
      "Optimization!\n",
      "Timestep: 856\tReward: 6.583043830531104\n",
      "Optimization!\n",
      "Timestep: 857\tReward: 5.157689810666131\n",
      "Optimization!\n",
      "Timestep: 858\tReward: 3.418551554260164\n",
      "Optimization!\n",
      "Timestep: 859\tReward: 4.841944625799716\n",
      "Optimization!\n",
      "Timestep: 860\tReward: 3.0477795546488196\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 861\tReward: 6.03727656826235\n",
      "Optimization!\n",
      "Timestep: 862\tReward: 4.152906159953693\n",
      "Optimization!\n",
      "Timestep: 863\tReward: 3.9198226759332044\n",
      "Optimization!\n",
      "Timestep: 864\tReward: 6.523428397261202\n",
      "Optimization!\n",
      "Timestep: 865\tReward: 6.4321512488646455\n",
      "Optimization!\n",
      "Timestep: 866\tReward: 6.52767341751319\n",
      "Optimization!\n",
      "Timestep: 867\tReward: 4.408701506624849\n",
      "Optimization!\n",
      "Timestep: 868\tReward: 6.520195929906553\n",
      "Optimization!\n",
      "Timestep: 869\tReward: 5.372474282114543\n",
      "Optimization!\n",
      "Timestep: 870\tReward: 5.933280216557817\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 871\tReward: 7.129657325180823\n",
      "Optimization!\n",
      "Timestep: 872\tReward: 7.599191952517489\n",
      "Optimization!\n",
      "Timestep: 873\tReward: 3.804147784292688\n",
      "Optimization!\n",
      "Timestep: 874\tReward: 5.085970491838443\n",
      "Optimization!\n",
      "Timestep: 875\tReward: 4.653134699325396\n",
      "Optimization!\n",
      "Timestep: 876\tReward: 6.057672868325841\n",
      "Optimization!\n",
      "Timestep: 877\tReward: 6.767413320719955\n",
      "Optimization!\n",
      "Timestep: 878\tReward: 4.101898587119202\n",
      "Optimization!\n",
      "Timestep: 879\tReward: 7.312170330302033\n",
      "Optimization!\n",
      "Timestep: 880\tReward: 1.392682106953603\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 881\tReward: 6.168794329718875\n",
      "Optimization!\n",
      "Timestep: 882\tReward: 2.572488232363572\n",
      "Optimization!\n",
      "Timestep: 883\tReward: 4.319299375526008\n",
      "Optimization!\n",
      "Timestep: 884\tReward: 4.211104604092528\n",
      "Optimization!\n",
      "Timestep: 885\tReward: 7.349984991594245\n",
      "Optimization!\n",
      "Timestep: 886\tReward: 3.8453292309626224\n",
      "Optimization!\n",
      "Timestep: 887\tReward: 5.34163868048818\n",
      "Optimization!\n",
      "Timestep: 888\tReward: 5.91571652918964\n",
      "Optimization!\n",
      "Timestep: 889\tReward: 5.816316961484432\n",
      "Optimization!\n",
      "Timestep: 890\tReward: 4.003873796069065\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 891\tReward: 6.966723831801806\n",
      "Optimization!\n",
      "Timestep: 892\tReward: 7.619984950437003\n",
      "Optimization!\n",
      "Timestep: 893\tReward: 5.620457323425134\n",
      "Optimization!\n",
      "Timestep: 894\tReward: 2.7879736784014573\n",
      "Optimization!\n",
      "Timestep: 895\tReward: 4.115302289445959\n",
      "Optimization!\n",
      "Timestep: 896\tReward: 4.714186912584247\n",
      "Optimization!\n",
      "Timestep: 897\tReward: 4.989350816358036\n",
      "Optimization!\n",
      "Timestep: 898\tReward: 6.5866166373232575\n",
      "Optimization!\n",
      "Timestep: 899\tReward: 1.4347417111578094\n",
      "Optimization!\n",
      "Timestep: 900\tReward: 6.086877693337545\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 901\tReward: 6.821690154166722\n",
      "Optimization!\n",
      "Timestep: 902\tReward: 4.749148547940452\n",
      "Optimization!\n",
      "Timestep: 903\tReward: 4.475030297224558\n",
      "Optimization!\n",
      "Timestep: 904\tReward: 5.266429778396333\n",
      "Optimization!\n",
      "Timestep: 905\tReward: 5.511274608788745\n",
      "Optimization!\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Timestep: 906\tReward: 6.254093795156797\n",
      "Optimization!\n",
      "Timestep: 907\tReward: 6.336915140872212\n",
      "Optimization!\n",
      "Timestep: 908\tReward: 3.7713221471558005\n",
      "Optimization!\n",
      "Timestep: 909\tReward: 5.2440787579505965\n",
      "Optimization!\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Timestep: 910\tReward: 4.2372678385252565\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 911\tReward: 4.753637377234672\n",
      "Optimization!\n",
      "Timestep: 912\tReward: 5.880396132076057\n",
      "Optimization!\n",
      "Timestep: 913\tReward: 6.859036164423769\n",
      "Optimization!\n",
      "Timestep: 914\tReward: 5.294929730957351\n",
      "Optimization!\n",
      "Timestep: 915\tReward: 4.434822514602441\n",
      "Optimization!\n",
      "Timestep: 916\tReward: 5.872996145197516\n",
      "Optimization!\n",
      "Timestep: 917\tReward: 2.413962547658844\n",
      "Optimization!\n",
      "Timestep: 918\tReward: 5.4121536461547874\n",
      "Optimization!\n",
      "Timestep: 919\tReward: 2.631252276576408\n",
      "Optimization!\n",
      "Timestep: 920\tReward: 6.808723239856391\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 921\tReward: 5.7196146876904805\n",
      "Optimization!\n",
      "Timestep: 922\tReward: 5.935019412505971\n",
      "Optimization!\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Timestep: 923\tReward: 6.2626184952941015\n",
      "Optimization!\n",
      "Loading a new scene! ---------------------------------------- : True\n",
      "Timestep: 924\tReward: 8.865678259645389\n",
      "Optimization!\n",
      "Timestep: 925\tReward: 2.989912107233322\n",
      "Optimization!\n",
      "Timestep: 926\tReward: 1.1280863031034043\n",
      "Optimization!\n",
      "Timestep: 927\tReward: 4.114195003014195\n",
      "Optimization!\n",
      "Timestep: 928\tReward: 1.7506818816596632\n",
      "Optimization!\n",
      "Timestep: 929\tReward: 6.3081580171696405\n",
      "Optimization!\n",
      "Timestep: 930\tReward: 5.196618353264608\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 931\tReward: 4.3131158032278325\n",
      "Optimization!\n",
      "Timestep: 932\tReward: 6.118440120109158\n",
      "Optimization!\n",
      "Timestep: 933\tReward: 5.132281315945155\n",
      "Optimization!\n",
      "Timestep: 934\tReward: 7.165579135777236\n",
      "Optimization!\n",
      "Timestep: 935\tReward: 5.5884639904460105\n",
      "Optimization!\n",
      "Timestep: 936\tReward: 3.2726103040254184\n",
      "Optimization!\n",
      "Timestep: 937\tReward: 4.814622240867942\n",
      "Optimization!\n",
      "Timestep: 938\tReward: 3.783982353263055\n",
      "Optimization!\n",
      "Timestep: 939\tReward: 8.305948940181944\n",
      "Optimization!\n",
      "Timestep: 940\tReward: 4.337448060462804\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 941\tReward: 4.755746821735513\n",
      "Optimization!\n",
      "Timestep: 942\tReward: 6.352588371870976\n",
      "Optimization!\n",
      "Timestep: 943\tReward: 1.530862412661684\n",
      "Optimization!\n",
      "Timestep: 944\tReward: 3.8719444318073344\n",
      "Optimization!\n",
      "Timestep: 945\tReward: 4.214238455471024\n",
      "Optimization!\n",
      "Timestep: 946\tReward: 3.46399086456406\n",
      "Optimization!\n",
      "Timestep: 947\tReward: 5.076538031675804\n",
      "Optimization!\n",
      "Timestep: 948\tReward: 2.633414840270948\n",
      "Optimization!\n",
      "Timestep: 949\tReward: 2.7624989629320003\n",
      "Optimization!\n",
      "Timestep: 950\tReward: 7.297836323835052\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 951\tReward: 5.778637583870776\n",
      "Optimization!\n",
      "Timestep: 952\tReward: 6.862423895490795\n",
      "Optimization!\n",
      "Timestep: 953\tReward: 6.358355180675884\n",
      "Optimization!\n",
      "Timestep: 954\tReward: 4.43132574953939\n",
      "Optimization!\n",
      "Timestep: 955\tReward: 4.995389351391598\n",
      "Optimization!\n",
      "Timestep: 956\tReward: 6.576710552445009\n",
      "Optimization!\n",
      "Timestep: 957\tReward: 5.577275265817809\n",
      "Optimization!\n",
      "Timestep: 958\tReward: 6.553053262880647\n",
      "Optimization!\n",
      "Timestep: 959\tReward: 5.188001684372605\n",
      "Optimization!\n",
      "Timestep: 960\tReward: 6.585184446130514\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 961\tReward: 3.4618543995589754\n",
      "Optimization!\n",
      "Timestep: 962\tReward: 5.05193858688692\n",
      "Optimization!\n",
      "Timestep: 963\tReward: 4.4056124498361635\n",
      "Optimization!\n",
      "Timestep: 964\tReward: 5.889376831916047\n",
      "Optimization!\n",
      "Timestep: 965\tReward: 4.9920437253742085\n",
      "Optimization!\n",
      "Timestep: 966\tReward: 6.389326832089166\n",
      "Optimization!\n",
      "Timestep: 967\tReward: 7.562630938733928\n",
      "Optimization!\n",
      "Timestep: 968\tReward: 5.195356000136974\n",
      "Optimization!\n",
      "Timestep: 969\tReward: 3.8325624028193395\n",
      "Optimization!\n",
      "Timestep: 970\tReward: 4.186453573287086\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 971\tReward: 7.21097995016103\n",
      "Optimization!\n",
      "Timestep: 972\tReward: 4.9593263504256155\n",
      "Optimization!\n",
      "Timestep: 973\tReward: 5.273267932297899\n",
      "Optimization!\n",
      "Timestep: 974\tReward: 5.215772646158732\n",
      "Optimization!\n",
      "Timestep: 975\tReward: 4.628448768621617\n",
      "Optimization!\n",
      "Timestep: 976\tReward: 5.244248452061611\n",
      "Optimization!\n",
      "Timestep: 977\tReward: 4.66324557336145\n",
      "Optimization!\n",
      "Timestep: 978\tReward: 5.155885899026278\n",
      "Optimization!\n",
      "Timestep: 979\tReward: 4.28828656043858\n",
      "Optimization!\n",
      "Timestep: 980\tReward: 3.5716487183121814\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 981\tReward: 2.830810197300326\n",
      "Optimization!\n",
      "Timestep: 982\tReward: 3.3616363159508538\n",
      "Optimization!\n",
      "Timestep: 983\tReward: 5.334485296169708\n",
      "Optimization!\n",
      "Timestep: 984\tReward: 8.221227371089057\n",
      "Optimization!\n",
      "Timestep: 985\tReward: 6.121930423705835\n",
      "Optimization!\n",
      "Timestep: 986\tReward: 11.844807058808295\n",
      "Optimization!\n",
      "Timestep: 987\tReward: 6.3273181884232015\n",
      "Optimization!\n",
      "Timestep: 988\tReward: 5.386014069365387\n",
      "Optimization!\n",
      "Timestep: 989\tReward: 8.084240617399274\n",
      "Optimization!\n",
      "Timestep: 990\tReward: 2.3458549668035427\n",
      "Optimization!\n",
      "Target updated\n",
      "Timestep: 991\tReward: 6.662560498430877\n",
      "Optimization!\n",
      "Timestep: 992\tReward: 11.995674004819323\n",
      "Optimization!\n",
      "Timestep: 993\tReward: 9.476469297955118\n",
      "Optimization!\n",
      "Timestep: 994\tReward: 4.125348398516916\n",
      "Optimization!\n",
      "Timestep: 995\tReward: 6.089769382066823\n",
      "Optimization!\n",
      "Timestep: 996\tReward: 2.8193915559084024\n",
      "Optimization!\n",
      "Timestep: 997\tReward: 4.825985322481739\n",
      "Optimization!\n",
      "Timestep: 998\tReward: 3.1265108294284314\n",
      "Optimization!\n",
      "Timestep: 999\tReward: 6.4436938126804195\n",
      "Optimization!\n",
      "Timestep: 1000\tReward: 5.7440246235303665\n",
      "Optimization!\n",
      "Target updated\n",
      "Saved\n",
      "Timestep: 1001\tReward: 4.244527693061471\n",
      "Optimization!\n",
      "Timestep: 1002\tReward: 5.286024822956426\n",
      "Optimization!\n",
      "Timestep: 1003\tReward: 5.7899282468589215\n",
      "Optimization!\n",
      "Complete\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from itertools import count\n",
    "\n",
    "from Config.constants import MIN_GRASP_THRESHOLDS\n",
    "\n",
    "is_viz = False\n",
    "\n",
    "# env = Environment()\n",
    "env = Environment(gui=True)\n",
    "num_of_envs = 10\n",
    "max_num_of_actions = 15\n",
    "is_viz = False\n",
    "max_extent_threshold = 1 # Max extent threshold of the target object in pixel units\n",
    "push_directions = [0, np.pi/8, np.pi/4, 3*np.pi/8, \n",
    "                    np.pi/2, 5*np.pi/8, 3*np.pi/4, 7*np.pi/8, \n",
    "                    np.pi, 9*np.pi/8, 5*np.pi/4, 11*np.pi/8,  \n",
    "                    3*np.pi/2, 13*np.pi/8, 7*np.pi/4, 15*np.pi/8] # 16 standard directions\n",
    "# num_episodes = 50 # 10\n",
    "max_timesteps = 1000\n",
    "timestep = 0\n",
    "\n",
    "# wandb.config.update({\n",
    "#     'epochs': num_episodes,\n",
    "#     'batch_size': BATCH_SIZE,\n",
    "#     'optimizer': 'Adam',\n",
    "#     'learning_rate': 'default',\n",
    "#     'replay_memory': REPLAY_MEMORY_SIZE, # 10000\n",
    "#     'n_actions': n_actions,\n",
    "#     'action_types': 'Only push in 16 different directions'\n",
    "# })\n",
    "\n",
    "while timestep < max_timesteps:\n",
    "    # Initialize the environment and state\n",
    "    env.reset()\n",
    "    testcase1 = TestCase1(env)\n",
    "    body_ids, success = testcase1.sample_test_case(bottom_obj='random') #'random') # testcase1.create_standard()\n",
    "    color_image, depth_image, _ = env_utils.get_true_heightmap(env)\n",
    "    depth_image = np.stack((depth_image, )*3, axis=-1)\n",
    "    # print(\"Returned body ids: {}, success: {}\".format(body_ids, success))\n",
    "    # last_screen = get_screen()\n",
    "    # current_screen = get_screen()\n",
    "    # state = current_screen - last_screen\n",
    "    target_pos, target_orn = p.getBasePositionAndOrientation(body_ids[1])\n",
    "    euler_orn = p.getEulerFromQuaternion(target_orn)\n",
    "    cur_target_st = np.array([target_pos[0], target_pos[1], euler_orn[2]], dtype=np.float64)\n",
    "    cur_target_goal = cur_target_st + np.random.uniform(low=[-5, -5, -2*np.pi], high=[5, 5, 2*np.pi], size=(3,))\n",
    "    cur_state = np.hstack((cur_target_st, cur_target_goal))\n",
    "    state = {\n",
    "        'cur_state': torch.tensor(cur_state, dtype=torch.float, device=device).unsqueeze(0),\n",
    "        # 'rgb': torch.tensor(np.array([np.transpose(color_image, (2, 0, 1))]), dtype=torch.float, device=device), # transpose used in order to convert (224, 224, 3) to (3, 224, 224)\n",
    "        # 'height_map': torch.tensor(np.array([np.transpose(depth_image, (2, 0, 1))]), dtype=torch.float, device=device) # torch.tensor([np.transpose(depth_image, (2, 0, 1))], device=device) # transpose used in order to convert (224, 224, 3) to (3, 224, 224)\n",
    "    }\n",
    "    done = False\n",
    "\n",
    "    for t in count():\n",
    "        # Select and perform an action\n",
    "        timestep += 1\n",
    "        action = select_action(state['cur_state']) # select_action(state['rgb'], state['height_map'])\n",
    "        color_image, depth_image, _ = env_utils.get_true_heightmap(env)\n",
    "        if action.item() in range(0, 16): # push action\n",
    "            temp = cv2.cvtColor(color_image, cv2.COLOR_RGB2HSV)\n",
    "            target_mask = cv2.inRange(temp, TARGET_LOWER, TARGET_UPPER)\n",
    "            push_dir = push_directions[action.item()] # Sample push directions\n",
    "            push_start, push_end = get_push_start(push_dir, target_mask, body_ids[1])\n",
    "            env.push(push_start, push_end) # Action performed \n",
    "\n",
    "            # color_image, depth_image, _ = env_utils.get_true_heightmap(env) # Evaluating the new state for calculating the reward\n",
    "            # temp = cv2.cvtColor(color_image, cv2.COLOR_RGB2HSV)\n",
    "            # target_mask = cv2.inRange(temp, TARGET_LOWER, TARGET_UPPER)\n",
    "            # bottom_mask = cv2.inRange(temp, orange_lower, orange_upper)\n",
    "            # depth_image = np.stack((depth_image, )*3, axis=-1)\n",
    "            # target_mask = cv2.inRange(temp, TARGET_LOWER, TARGET_UPPER)\n",
    "            # max_extents = get_max_extent_of_target_from_bottom(target_mask=target_mask, bottom_mask=bottom_mask, \n",
    "            #                             bottom_obj_body_id=body_ids[0], \n",
    "            #                             current_bottom_obj_size=testcase1.current_bottom_size, \n",
    "            #                             is_viz=False)\n",
    "            \n",
    "            target_pos, target_orn = p.getBasePositionAndOrientation(body_ids[1])\n",
    "            euler_orn = p.getEulerFromQuaternion(target_orn)\n",
    "\n",
    "            new_target_st = np.array([target_pos[0], target_pos[1], euler_orn[2]], dtype=np.float)\n",
    "            new_target_goal = new_target_st + np.random.uniform(low=[-5, -5, -2*np.pi], high=[5, 5, 2*np.pi], size=(3,))\n",
    "            new_state = np.hstack((new_target_st, new_target_goal))\n",
    "            reward = get_reward(current_state=new_state, prev_state=state['cur_state'].squeeze().cpu().numpy())\n",
    "            # print(f\"Current state: {state['cur_state'].squeeze().cpu().numpy()}\\t Action: {action.item()}\\nNew State: {new_state}\\tReward: {reward}\")\n",
    "            print(f\"Timestep: {timestep}\\tReward: {reward}\")\n",
    "            # reward = get_reward(action='push', max_extents=max_extents, MIN_GRASP_EXTENT_THRESH=MIN_GRASP_THRESHOLDS) # get_reward(action, max_extents, MAX_EXTENT_THRESH, MIN_GRASP_EXTENT_THRESH)\n",
    "            # belman_update_val = get_belman_update_value()\n",
    "        elif action.item()==16:\n",
    "            print(\"Invalid Action!!!!!\")\n",
    "            exit()\n",
    "            # Check if the state is graspable and reward the agent\n",
    "            # temp = cv2.cvtColor(color_image, cv2.COLOR_RGB2HSV)\n",
    "            # target_mask = cv2.inRange(temp, TARGET_LOWER, TARGET_UPPER)\n",
    "            # bottom_mask = cv2.inRange(temp, orange_lower, orange_upper)\n",
    "            # max_extents = get_max_extent_of_target_from_bottom(target_mask=target_mask, bottom_mask=bottom_mask, \n",
    "            #                             bottom_obj_body_id=body_ids[0], \n",
    "            #                             current_bottom_obj_size=testcase1.current_bottom_size, \n",
    "            #                             is_viz=False)\n",
    "            \n",
    "            # reward = get_reward(action='grasp', max_extents=max_extents, MIN_GRASP_EXTENT_THRESH=MIN_GRASP_THRESHOLDS)\n",
    "            # if reward==1:\n",
    "            #     done = True\n",
    "            # done = True\n",
    "        targetPos, _ = p.getBasePositionAndOrientation(body_ids[1])\n",
    "        bottomPos, _ = p.getBasePositionAndOrientation(body_ids[0])\n",
    "        if targetPos[2] < bottomPos[2] + testcase1.current_bottom_size[2]/2 + testcase1.current_target_size[2]/2 - 0.01:\n",
    "            # reward = -0.75\n",
    "            done = True\n",
    "        # _, reward, done, _, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], dtype=torch.float, device=device)\n",
    "        if reward == 1:\n",
    "            done=True\n",
    "\n",
    "        # Observe new state\n",
    "        # last_screen = current_screen\n",
    "        # current_screen = get_screen()\n",
    "        if not done:\n",
    "            target_pos, target_orn = p.getBasePositionAndOrientation(body_ids[1])\n",
    "            euler_orn = p.getEulerFromQuaternion(target_orn)\n",
    "            \n",
    "            new_target_st = np.array([target_pos[0], target_pos[1], euler_orn[2]], dtype=float)\n",
    "            new_target_goal = new_target_st + np.random.uniform(low=[-5, -5, -2*np.pi], high=[5, 5, 2*np.pi], size=(3,))\n",
    "            new_state = np.hstack((new_target_st, new_target_goal))\n",
    "            next_state = {\n",
    "                'cur_state': torch.tensor(new_state, dtype=torch.float, device=device).unsqueeze(0),\n",
    "                # 'rgb': torch.tensor(np.array([np.transpose(color_image, (2, 0, 1))]), dtype=torch.float, device=device), # transpose used in order to convert (224, 224, 3) to (3, 224, 224)\n",
    "                # 'height_map': torch.tensor(np.array([np.transpose(depth_image, (2, 0, 1))]), dtype=torch.float, device=device) # transpose used in order to convert (224, 224, 3) to (3, 224, 224)\n",
    "            }\n",
    "        else:\n",
    "            next_state = {\n",
    "                'cur_state': None,\n",
    "                # 'rgb': None,\n",
    "                # 'height_map': None\n",
    "            }\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state['cur_state'], action, next_state['cur_state'], reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        optimize_model(timestep=timestep, batch_num=t)\n",
    "        if done:\n",
    "            # episode_durations.append(t + 1)\n",
    "            # plot_durations()\n",
    "            break\n",
    "\n",
    "        if t>=100:\n",
    "            done = True\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "        if timestep % TARGET_UPDATE == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "            print(\"Target updated\")\n",
    "            \n",
    "\n",
    "        # if i_episode % TARGET_SAVE == 0 or i_episode==10:\n",
    "        if timestep in TARGET_SAVE_CHECKPOINTS:\n",
    "            print(\"Saved\")\n",
    "            SAVE_PATH = './V2_next_best_action/models/model_checkpoints/{}.pt'.format(timestep)\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "            torch.save(policy_net.state_dict(), SAVE_PATH)\n",
    "\n",
    "print('Complete')\n",
    "# env.render()\n",
    "# env.close()\n",
    "# plt.ioff()\n",
    "# plt.savefig('durations_count.png')\n",
    "# plt.show()\n",
    "# run.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vft2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8654fed7540277ddf6b9b5ab1dc4e73f02ee108c349421d9d519d216622133b6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
